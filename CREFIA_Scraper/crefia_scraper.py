"""
ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ ìŠ¤í¬ë˜í¼ (Option 1 ì ìš©, FileExtractor í†µí•©)
"""
import sys
from pathlib import Path
import os
import time
from typing import List, Dict, Optional
from urllib.parse import urljoin
import re
import json
import csv

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (common ëª¨ë“ˆ import ìœ„í•´)
def find_project_root():
    try:
        current = Path(__file__).resolve().parent
    except NameError:
        current = Path.cwd()
    
    while current != current.parent:
        if (current / 'common').exists() and (current / 'common' / 'base_scraper.py').exists():
            return current
        current = current.parent
    
    return Path.cwd()

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from common.base_scraper import BaseScraper
from common.file_extractor import FileExtractor  # FileExtractor import

# ---------------- Selenium ë‹¤ìš´ë¡œë“œ ìœ í‹¸ ----------------
def init_selenium(download_dir: str) -> webdriver.Chrome:
    os.makedirs(download_dir, exist_ok=True)
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # í•„ìš” ì‹œ í™œì„±í™”
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--lang=ko-KR")
    prefs = {
        "download.default_directory": os.path.abspath(download_dir),
        "download.prompt_for_download": False,
        "directory_upgrade": True,
        "safebrowsing.enabled": True
    }
    chrome_options.add_experimental_option("prefs", prefs)
    driver = webdriver.Chrome(options=chrome_options)
    return driver


# ---------------- ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ ----------------
class CrefiaScraper(BaseScraper):
    """ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ ìŠ¤í¬ë˜í¼"""
    
    BASE_URL = "https://www.crefia.or.kr"
    LIST_URL = "https://www.crefia.or.kr/portal/infocenter/regulation/selfRegulation.xx"
    
    #--------------------------------------
    def __init__(self, delay: float = 1.0):
        super().__init__(delay)
        self.download_dir = os.path.join("output", "downloads")
        os.makedirs(self.download_dir, exist_ok=True)
        self.file_extractor = FileExtractor(download_dir=self.download_dir)
        print("ë‹¤ìš´ë¡œë“œ í´ë” ë‚´ìš©:", os.listdir(self.download_dir))
    
    # ---------------- ëª©ë¡ ì¶”ì¶œ ----------------
    def extract_list_items(self, soup: BeautifulSoup, driver: webdriver.Chrome) -> List[Dict]:
        results: List[Dict] = []
        if soup is None:
            return results
        
        self.save_debug_html(soup, filename="debug_crefia_list.html")
        
        category_containers = soup.select("#contents > div.cont_box_wrap > div")
        print(f"ì¹´í…Œê³ ë¦¬ ì»¨í…Œì´ë„ˆ ìˆ˜: {len(category_containers)}ê°œ")
        
        item_count = 0
        category_idx = 0
        
        for container in category_containers:
            left_right_containers = container.select("div.left, div.right")
            
            for lr_container in left_right_containers:
                category_title_elem = lr_container.select_one("div > div.title.dia_bul > h4") \
                                    or lr_container.select_one("div.title.dia_bul > h4, h4")
                
                if not category_title_elem:
                    continue
                
                category_idx += 1
                category_title = category_title_elem.get_text(strip=True)
                print(f"\n[{category_idx}] ì¹´í…Œê³ ë¦¬: {category_title}")

                if category_title in ["í‘œì¤€ì•½ê´€", "ë¦¬ìŠ¤Â·í• ë¶€Â·ì‹ ê¸°ìˆ ", "ê³µì‹œ", "ì‹ ìš©ì¹´ë“œ", "ëª¨ì§‘ì¸ ê´€ë ¨", "ê´‘ê³ ì‹¬ì˜ ë° ì‚¬í›„ë³´ê³ ì•½ê´€ ì‹¬ì‚¬"]:
                    print(f"  âš  '{category_title}' ì¹´í…Œê³ ë¦¬ëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                    continue
                
                list_box = lr_container.select_one("div > div.list_box") or lr_container.select_one("div.list_box")
                if not list_box:
                    print(f"  âš  ëª©ë¡ ë°•ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # ---- BeautifulSoupì—ì„œ ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----
                links = list_box.select("ul > li > a")
                link_texts = []
                for link in links:
                    title_elem = link.select_one("p")
                    text = title_elem.get_text(strip=True) if title_elem else link.get_text(strip=True)
                    if text:
                        link_texts.append(text)

                print(f"  ë§í¬ ìˆ˜: {len(link_texts)}ê°œ")
                
                # ---- Seleniumìœ¼ë¡œ ì‹¤ì œ í´ë¦­ ----
                for link_idx, text in enumerate(link_texts, 1):
                    try:
                        selenium_link = driver.find_element(By.LINK_TEXT, text)
                    except:
                        print(f"  âš  Seleniumì—ì„œ '{text}' ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                        continue

                    file_name = ""
                    download_url = ""
                    content = ""

                    # ë‹¤ìš´ë¡œë“œ ê°ì§€
                    before = set(os.listdir(self.download_dir))
                    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {text}")
                    driver.execute_script("arguments[0].click();", selenium_link)

                    downloaded_file = None
                    timeout = 40
                    start_time = time.time()

                    while time.time() - start_time < timeout:
                        after = set(os.listdir(self.download_dir))
                        new_files = after - before

                        if new_files:
                            downloaded_file = list(new_files)[0]
                            if not downloaded_file.endswith(".crdownload"):
                                break
                        time.sleep(1)

                    if downloaded_file is None:
                        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì‹œê°„ ì´ˆê³¼: {text}")
                        continue

                    filepath = os.path.join(self.download_dir, downloaded_file)
                    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")

                    # FileExtractorë¡œ ë‚´ìš© ì¶”ì¶œ
                    try:
                        content = self.file_extractor.extract_hwp_content(filepath)
                        content = content[:50]
                    except Exception as e:
                        content = f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"

                    print(f"\nğŸ“„ {text} íŒŒì¼ ë‚´ìš© ì¼ë¶€:\n{content}\n")
                    
                    item: Dict[str, str] = {
                        "no": str(item_count + 1),
                        "title": text,
                        "regulation_name": text,
                        "organization": "ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ",
                        "category": category_title,
                        "detail_link": download_url,
                        "file_download_link": download_url,
                        "file_name": file_name if file_name else text,
                        "content": content,
                        "enactment_date": "",
                        "revision_date": "",
                        "department": "",
                    }
                    
                    results.append(item)
                    item_count += 1
                    
                    if link_idx <= 3:
                        print(f"    [{link_idx}] {text[:50]}... -> {file_name[:60] if file_name else 'íŒŒì¼ëª… ì—†ìŒ'}")
        
        print(f"\nì´ {len(results)}ê°œ í•­ëª© ì¶”ì¶œ ì™„ë£Œ")
        return results
    
    # ---------------- í¬ë¡¤ë§ ----------------
    def crawl_self_regulation_status(self, limit: int = 0) -> List[Dict]:
        driver: Optional[webdriver.Chrome] = None
        try:
            driver = init_selenium(self.download_dir)
            print("Selenium ë“œë¼ì´ë²„ ìƒì„± ì™„ë£Œ")
        except Exception as exc:
            print(f"âš  Selenium ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {exc}")
            return []
        
        try:
            print(f"\ní˜ì´ì§€ ì ‘ì†: {self.LIST_URL}")
            driver.get(self.LIST_URL)
            time.sleep(3)
            
            soup = BeautifulSoup(driver.page_source, "lxml")
            results = self.extract_list_items(soup, driver)
            
            if limit > 0:
                results = results[:limit]
                print(f"limit ì ìš©: {limit}ê°œ í•­ëª©ë§Œ ì²˜ë¦¬ (ì „ì²´: {len(results)}ê°œ)")
            
        finally:
            if driver:
                driver.quit()
        
        return results
    
    def crawl_self_regulation_notice(self) -> List[Dict]:
        """ììœ¨ê·œì œ ì œÂ·ê°œì • ê³µê³  (ë¯¸êµ¬í˜„)"""
        return []

# ---------------- ì €ì¥ í•¨ìˆ˜ ----------------
def save_crefia_results(records: List[Dict]):
    if not records:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    law_results = []
    for item in records:
        law_item = {
            "ë²ˆí˜¸": item.get("no", ""),
            "ê·œì •ëª…": item.get("regulation_name", ""),
            "ê¸°ê´€ëª…": item.get("organization", "ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ"),
            "ë³¸ë¬¸": item.get("content", ""),
            "ì œì •ì¼": item.get("enactment_date", ""),
            "ìµœê·¼ ê°œì •ì¼": item.get("revision_date", ""),
            "ì†Œê´€ë¶€ì„œ": item.get("department", ""),
            "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬": item.get("file_download_link", ""),
            "íŒŒì¼ ì´ë¦„": item.get("file_name", ""),
        }
        law_results.append(law_item)
    
    # JSON ì €ì¥
    json_dir = os.path.join("output", "json")
    os.makedirs(json_dir, exist_ok=True)
    json_path = os.path.join(json_dir, "crefia_scraper.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "url": "https://www.crefia.or.kr/publicdata/reform_info.php",
            "total_count": len(law_results),
            "results": law_results,
        }, f, ensure_ascii=False, indent=2)
    print(f"\nJSON ì €ì¥ ì™„ë£Œ: {json_path}")
    
    # CSV ì €ì¥
    csv_dir = os.path.join("output", "csv")
    os.makedirs(csv_dir, exist_ok=True)
    csv_path = os.path.join(csv_dir, "crefia_scraper.csv")
    headers = ["ë²ˆí˜¸", "ê·œì •ëª…", "ê¸°ê´€ëª…", "ë³¸ë¬¸", "ì œì •ì¼", "ìµœê·¼ ê°œì •ì¼", "ì†Œê´€ë¶€ì„œ", "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬", "íŒŒì¼ ì´ë¦„"]
    
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for law_item in law_results:
            csv_item = law_item.copy()
            csv_item["ë³¸ë¬¸"] = csv_item.get("ë³¸ë¬¸", "").replace("\n", " ").replace("\r", " ")
            writer.writerow(csv_item)
    print(f"CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

# ---------------- ì‹¤í–‰ ----------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ ììœ¨ê·œì œ í˜„í™© ìŠ¤í¬ë˜í¼")
    parser.add_argument("--limit", type=int, default=0, help="ê°€ì ¸ì˜¬ ê°œìˆ˜ ì œí•œ (0=ì „ì²´)")
    args = parser.parse_args()
    
    crawler = CrefiaScraper()
    results = crawler.crawl_self_regulation_status(limit=args.limit)
    print(f"\nì¶”ì¶œëœ ë°ì´í„°: {len(results)}ê°œ")
    save_crefia_results(results)
