"""
ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ ìŠ¤í¬ë˜í¼
"""
import sys
from pathlib import Path
import os
import time
from typing import List, Dict, Optional
import json
import csv
import re
import requests

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (common ëª¨ë“ˆ import ìœ„í•´)
def find_project_root():
    try:
        current = Path(__file__).resolve().parent
    except NameError:
        current = Path.cwd()
    
    while current != current.parent:
        if (current / 'common').exists() and (current / 'common' / 'base_scraper.py').exists():
            return current
        current = current.parent
    
    return Path.cwd()

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from common.base_scraper import BaseScraper
from common.file_extractor import FileExtractor
from data_scraper import extract_data_from_text, extract_dates_from_filename

# ---------------- Selenium ë‹¤ìš´ë¡œë“œ ìœ í‹¸ ----------------
def init_selenium(download_dir: str, headless: bool = False, scraper=None) -> webdriver.Chrome:
    """
    Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”
    
    Args:
        download_dir: ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ë‹¤ìš´ë¡œë“œ ì‹œ False ê¶Œì¥)
        scraper: BaseScraper ì¸ìŠ¤í„´ìŠ¤ (íì‡„ë§ í™˜ê²½ ëŒ€ì‘ì„ ìœ„í•´ _create_webdriver ì‚¬ìš©)
    """
    download_dir_abs = os.path.abspath(download_dir)
    os.makedirs(download_dir_abs, exist_ok=True)
    print(f"ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬: {download_dir_abs}")
    
    chrome_options = Options()
    if headless:
        chrome_options.add_argument("--headless")
        print("âš  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í™œì„±í™” (ë‹¤ìš´ë¡œë“œ ë¬¸ì œ ê°€ëŠ¥)")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--lang=ko-KR")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    prefs = {
        "download.default_directory": download_dir_abs,
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.automatic_downloads": 1
    }
    chrome_options.add_experimental_option("prefs", prefs)
    
    # íì‡„ë§ í™˜ê²½ ëŒ€ì‘: BaseScraperì˜ _create_webdriver ì‚¬ìš© (SeleniumManager ìš°íšŒ)
    if scraper and hasattr(scraper, '_create_webdriver'):
        driver = scraper._create_webdriver(chrome_options)
    else:
        driver = webdriver.Chrome(options=chrome_options)
    return driver


# ---------------- ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ ----------------
class CrefiaScraper(BaseScraper):
    """ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ ìŠ¤í¬ë˜í¼"""
    
    BASE_URL = "https://www.crefia.or.kr"
    LIST_URL = "https://www.crefia.or.kr/portal/infocenter/regulation/selfRegulation.xx"
    
    def __init__(self, delay: float = 1.0, cleanup_downloads: bool = False, clean_downloads: bool = False):
        """
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            cleanup_downloads: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ë‚´ìš© ì¶”ì¶œ í›„ ì‚­ì œí• ì§€ ì—¬ë¶€
            clean_downloads: í¬ë¡¤ë§ ì‹œì‘ ì „ downloads í´ë”ë¥¼ ì •ë¦¬í• ì§€ ì—¬ë¶€
        """
        super().__init__(delay)
        self.download_dir = os.path.join("output", "downloads")
        os.makedirs(self.download_dir, exist_ok=True)
        # BaseScraperì˜ sessionì„ FileExtractorì— ì „ë‹¬
        self.file_extractor = FileExtractor(
            download_dir=self.download_dir,
            session=self.session
        )
        self.cleanup_downloads = cleanup_downloads
        self.clean_downloads = clean_downloads
        
        if self.clean_downloads:
            self._clean_downloads_folder()
        
        print("ë‹¤ìš´ë¡œë“œ í´ë” ë‚´ìš©:", os.listdir(self.download_dir))
    
    def _clean_downloads_folder(self):
        """downloads í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì‚­ì œ"""
        try:
            files = os.listdir(self.download_dir)
            if files:
                print(f"ğŸ—‘ï¸ downloads í´ë” ì •ë¦¬ ì¤‘... ({len(files)}ê°œ íŒŒì¼)")
                for file in files:
                    file_path = os.path.join(self.download_dir, file)
                    if os.path.isfile(file_path):
                        try:
                            os.remove(file_path)
                        except Exception as e:
                            print(f"  âš  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file} - {e}")
                print("âœ… downloads í´ë” ì •ë¦¬ ì™„ë£Œ")
            else:
                print("ğŸ“‚ downloads í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš  downloads í´ë” ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ---------------- ëª©ë¡ ì¶”ì¶œ ----------------
    def extract_list_items(
        self, soup: BeautifulSoup, driver: webdriver.Chrome, limit: int = 0
    ) -> List[Dict]:
        results: List[Dict] = []
        if soup is None:
            return results
        
        self.save_debug_html(soup, filename="debug_crefia_list.html")
        
        category_containers = soup.select("#contents > div.cont_box_wrap > div")
        print(f"ì¹´í…Œê³ ë¦¬ ì»¨í…Œì´ë„ˆ ìˆ˜: {len(category_containers)}ê°œ")
        
        item_count = 0
        category_idx = 0
        
        for container in category_containers:
            left_right_containers = container.select("div.left, div.right")
            
            for lr_container in left_right_containers:
                # div.right ë˜ëŠ” div.left ì•ˆì— ì—¬ëŸ¬ div.cont_boxê°€ ìˆì„ ìˆ˜ ìˆìŒ
                # ê° cont_boxë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
                cont_boxes = lr_container.select("div.cont_box")
                
                for cont_box in cont_boxes:
                    # ì¹´í…Œê³ ë¦¬ ì œëª© ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                    category_title_elem = (
                        cont_box.select_one("div.title.dia_bul > h4") or
                        cont_box.select_one(".title.dia_bul > h4") or
                        cont_box.select_one("div.title > h4") or
                        cont_box.select_one("h4")
                    )
                    
                    if not category_title_elem:
                        continue
                    
                    category_idx += 1
                    category_title = category_title_elem.get_text(strip=True)
                    print(f"\n[{category_idx}] ì¹´í…Œê³ ë¦¬: {category_title}")

                    skip_categories = [
                        "í‘œì¤€ì•½ê´€", 
                    ]
                    if category_title in skip_categories:
                        print(f"  âš  '{category_title}' ì¹´í…Œê³ ë¦¬ëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                        continue
                    
                    # list_box ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                    list_box = (
                        cont_box.select_one("div.list_box") or
                        cont_box.select_one(".list_box")
                    )
                    if not list_box:
                        print("  âš  ëª©ë¡ ë°•ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        continue
                    
                    # ---- BeautifulSoupì—ì„œ ë§í¬ í…ìŠ¤íŠ¸ ë° onclick ì •ë³´ ì¶”ì¶œ ----
                    links = list_box.select("ul > li > a")
                    link_data = []  # (text, filename, file_type) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
                    for link in links:
                        title_elem = link.select_one("p")
                        text = title_elem.get_text(strip=True) if title_elem else link.get_text(strip=True)
                        if text:
                            # onclick ì†ì„±ì—ì„œ íŒŒì¼ëª…ê³¼ íƒ€ì… ì¶”ì¶œ
                            onclick = link.get("onclick", "")
                            filename = ""
                            file_type = "selfRegulation"  # ê¸°ë³¸ê°’
                            if onclick:
                                # fn_downloadFile('íŒŒì¼ëª….hwp', 'selfRegulation') í˜•ì‹ íŒŒì‹±
                                match = re.search(
                                    r"fn_downloadFile\s*\(\s*['\"]([^'\"]+)['\"]\s*,\s*['\"]([^'\"]+)['\"]",
                                    onclick
                                )
                                if match:
                                    filename = match.group(1)
                                    file_type = match.group(2)
                                    print(f"  ğŸ“ onclickì—ì„œ ì¶”ì¶œ: íŒŒì¼ëª…={filename}, íƒ€ì…={file_type}")
                            link_data.append((text, filename, file_type))

                    print(f"  ë§í¬ ìˆ˜: {len(link_data)}ê°œ")
                    
                    # ---- Seleniumìœ¼ë¡œ ì‹¤ì œ í´ë¦­ ----
                    for link_idx, (text, filename, file_type) in enumerate(link_data, 1):
                        # limit ì²´í¬ (0ì´ë©´ ì „ì²´ ì²˜ë¦¬)
                        if limit > 0 and item_count >= limit:
                            print(f"  âš  limit({limit}ê°œ) ë„ë‹¬, ì²˜ë¦¬ ì¤‘ë‹¨")
                            break
                        # ë§í¬ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                        selenium_link = None
                        try:
                            # ë°©ë²• 1: LINK_TEXTë¡œ ì°¾ê¸°
                            selenium_link = driver.find_element(By.LINK_TEXT, text)
                            print(f"  âœ“ LINK_TEXTë¡œ ë§í¬ ì°¾ìŒ: {text}")
                        except Exception:
                            try:
                                # ë°©ë²• 2: ë¶€ë¶„ í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°
                                selenium_link = driver.find_element(
                                    By.PARTIAL_LINK_TEXT, text
                                )
                                print(f"  âœ“ PARTIAL_LINK_TEXTë¡œ ë§í¬ ì°¾ìŒ: {text}")
                            except Exception:
                                try:
                                    # ë°©ë²• 3: XPathë¡œ ì°¾ê¸°
                                    xpath = f"//a[contains(text(), '{text}')]"
                                    selenium_link = driver.find_element(By.XPATH, xpath)
                                    print(f"  âœ“ XPathë¡œ ë§í¬ ì°¾ìŒ: {text}")
                                except Exception as e:
                                    print(f"  âš  ëª¨ë“  ë°©ë²•ìœ¼ë¡œ '{text}' ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨: {e}")
                                    continue

                        # onclickì—ì„œ íŒŒì¼ëª… ì¬í™•ì¸ (Seleniumì—ì„œ)
                        if not filename:
                            try:
                                selenium_onclick = selenium_link.get_attribute("onclick")
                                if selenium_onclick:
                                    match = re.search(
                                        r"fn_downloadFile\s*\(\s*['\"]([^'\"]+)['\"]\s*,\s*['\"]([^'\"]+)['\"]",
                                        selenium_onclick
                                    )
                                    if match:
                                        filename = match.group(1)
                                        file_type = match.group(2)
                                        print(f"  ğŸ“ Seleniumì—ì„œ ì¬ì¶”ì¶œ: íŒŒì¼ëª…={filename}, íƒ€ì…={file_type}")
                            except Exception as e:
                                print(f"  âš  onclick ì¬í™•ì¸ ì‹¤íŒ¨: {e}")

                        # ë‹¤ìš´ë¡œë“œ URL êµ¬ì„±
                        # íŒ¨í„´: /common/downloadFile.do?fileName=<íŒŒì¼ëª…(UTF-8 ì¸ì½”ë”©)>&fileType=selfRegulation&keyNum=&date=&pFileEnc=
                        download_url = ""
                        file_name = filename  # ì €ì¥í•  íŒŒì¼ëª…
                        if filename:
                            try:
                                from urllib.parse import quote
                                # íŒŒì¼ëª…ì„ UTF-8ë¡œ ì¸ì½”ë”©
                                encoded_filename = quote(
                                    filename, encoding='utf-8'
                                )
                                download_url = (
                                    f"{self.BASE_URL}/common/downloadFile.do"
                                    f"?fileName={encoded_filename}"
                                    f"&fileType={file_type}"
                                    f"&keyNum="
                                    f"&date="
                                    f"&pFileEnc="
                                )
                                print(f"  ğŸ“ ë‹¤ìš´ë¡œë“œ URL êµ¬ì„±: {download_url}")
                            except Exception as e:
                                print(f"  âš  URL êµ¬ì„± ì‹¤íŒ¨: {e}")
                        else:
                            print("  âš  íŒŒì¼ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ URL êµ¬ì„± ë¶ˆê°€")

                        content = ""
                        downloaded_file = None
                        filepath = None

                        # ë°©ë²• 1: URLë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„
                        if download_url and filename:
                            filepath = os.path.join(self.download_dir, filename)
                            
                            # ì´ë¯¸ ê°™ì€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                            if os.path.exists(filepath):
                                print(f"  â­ï¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {filename} (ê±´ë„ˆëœ€)")
                                downloaded_file = filename
                                if not file_name:
                                    file_name = filename
                            else:
                                print(f"ğŸ“¥ ë°©ë²• 1: URLë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„: {text}")
                                print(f"  ğŸ“ ë‹¤ìš´ë¡œë“œ URL: {download_url}")
                                
                                try:
                                    # ê°„ë‹¨í•œ GET ìš”ì²­ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
                                    response = requests.get(download_url, timeout=15)
                                    
                                    if response.status_code == 200:
                                        with open(filepath, "wb") as f:
                                            f.write(response.content)
                                        print(f"  âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}")
                                        downloaded_file = filename
                                        if not file_name:
                                            file_name = filename
                                    else:
                                        print(f"  âš  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}, {response.text[:200]}")
                                except Exception as e:
                                    print(f"  âš  URL ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

                        # ë°©ë²• 2: driver í´ë¦­ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ (ë°©ë²• 1 ì‹¤íŒ¨ ì‹œ)
                        if not downloaded_file:
                            print(f"ğŸ“¥ ë°©ë²• 2: driver í´ë¦­ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ: {text}")
                            
                            # ë‹¤ìš´ë¡œë“œ ê°ì§€
                            download_dir_abs = os.path.abspath(self.download_dir)
                            print(f"  ğŸ“‚ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬: {download_dir_abs}")
                            before = set(os.listdir(self.download_dir))
                            print(f"  ğŸ“‹ ë‹¤ìš´ë¡œë“œ ì „ íŒŒì¼ ìˆ˜: {len(before)}ê°œ")
                            
                            # í´ë¦­ ì „ í˜„ì¬ URL ì €ì¥
                            current_url = driver.current_url
                            
                            try:
                                # í´ë¦­ ì‹¤í–‰
                                driver.execute_script("arguments[0].click();", selenium_link)
                                time.sleep(2)  # í´ë¦­ í›„ ì´ˆê¸° ëŒ€ê¸°
                                
                                # í˜ì´ì§€ ì´ë™ í™•ì¸
                                new_url = driver.current_url
                                if new_url != current_url:
                                    print(f"  âš  í˜ì´ì§€ ì´ë™ ë°œìƒ: {current_url} -> {new_url}")
                                    # ì›ë˜ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°
                                    driver.back()
                                    time.sleep(2)
                            except Exception as e:
                                print(f"  âš  í´ë¦­ ì‹¤íŒ¨: {e}")

                            timeout = 40
                            start_time = time.time()
                            crdownload_count = 0

                            while time.time() - start_time < timeout:
                                after = set(os.listdir(self.download_dir))
                                new_files = after - before

                            if new_files:
                                for new_file in new_files:
                                    print(f"  ğŸ” ìƒˆ íŒŒì¼ ë°œê²¬: {new_file}")
                                    if new_file.endswith(".crdownload"):
                                        crdownload_count += 1
                                        print(f"  â³ ë‹¤ìš´ë¡œë“œ ì§„í–‰ ì¤‘... ({crdownload_count}ì´ˆ)")
                                    else:
                                        # ì´ë¯¸ ê°™ì€ íŒŒì¼ëª…ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                                        new_file_path = os.path.join(self.download_dir, new_file)
                                        if filename and os.path.exists(os.path.join(self.download_dir, filename)):
                                            # ê¸°ëŒ€í•œ íŒŒì¼ëª…ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìƒˆë¡œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì€ ìœ ì§€
                                            downloaded_file = new_file
                                            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ íŒŒì¼: {downloaded_file}")
                                        else:
                                            downloaded_file = new_file
                                            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ íŒŒì¼: {downloaded_file}")
                                        break
                                    
                                    if downloaded_file:
                                        break
                                else:
                                    elapsed = int(time.time() - start_time)
                                    if elapsed % 5 == 0:  # 5ì´ˆë§ˆë‹¤ ë¡œê·¸
                                        print(f"  â³ ë‹¤ìš´ë¡œë“œ ëŒ€ê¸° ì¤‘... ({elapsed}ì´ˆ)")
                                
                                time.sleep(1)

                            if downloaded_file is None:
                                print(f"âŒ driver í´ë¦­ ë‹¤ìš´ë¡œë“œë„ ì‹¤íŒ¨: {text}")
                                print(f"  ğŸ“‹ ë‹¤ìš´ë¡œë“œ í›„ íŒŒì¼ ìˆ˜: {len(after) if 'after' in locals() else len(before)}ê°œ")
                                # .crdownload íŒŒì¼ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
                                crdownload_files = [
                                    f for f in os.listdir(self.download_dir)
                                    if f.endswith(".crdownload")
                                ]
                                if crdownload_files:
                                    print(f"  âš  ë¯¸ì™„ë£Œ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ë°œê²¬: {crdownload_files}")
                                continue

                            filepath = os.path.join(self.download_dir, downloaded_file)
                            # onclickì—ì„œ ì¶”ì¶œí•œ íŒŒì¼ëª…ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ëª… ì‚¬ìš©
                            if not file_name:
                                file_name = downloaded_file
                            print(f"  âœ… driver í´ë¦­ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
                            print(f"  ğŸ“ ì €ì¥í•  íŒŒì¼ëª…: {file_name}")

                        # FileExtractorë¡œ ë‚´ìš© ì¶”ì¶œ
                        content = ""
                        enactment_date = ""
                        revision_date = ""
                        department = ""
                        
                        # 1ë‹¨ê³„: íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                        filename_enactment = ""
                        filename_revision = ""
                        if file_name:
                            filename_enactment, filename_revision = extract_dates_from_filename(file_name)
                            if filename_enactment:
                                print(f"  ğŸ“… ì œì •ì¼ ì¶”ì¶œ (íŒŒì¼ëª…): {filename_enactment}")
                            if filename_revision:
                                print(f"  ğŸ“… ìµœê·¼ ê°œì •ì¼ ì¶”ì¶œ (íŒŒì¼ëª…): {filename_revision}")
                        
                        try:
                            full_content = self.file_extractor.extract_hwp_content(filepath)
                            original_length = len(full_content)
                            
                            # 2ë‹¨ê³„: íŒŒì¼ ë‚´ìš©ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                            extract_text = full_content[:1000] if full_content else ""
                            
                            if extract_text:
                                content_enactment, content_revision, content_department = extract_data_from_text(extract_text)
                                
                                # íŒŒì¼ëª… ë°ì´í„°ë¥¼ ìš°ì„  ì‚¬ìš© (íŒŒì¼ëª…ê³¼ ë‹¤ë¥¼ ê²½ìš° íŒŒì¼ëª… ìš°ì„ )
                                if filename_enactment:
                                    if content_enactment and filename_enactment != content_enactment:
                                        print(f"  âš  ì œì •ì¼ ë¶ˆì¼ì¹˜ - íŒŒì¼ëª…: {filename_enactment}, íŒŒì¼ë‚´ìš©: {content_enactment} (íŒŒì¼ëª… ì‚¬ìš©)")
                                    enactment_date = filename_enactment
                                    print(f"  ğŸ“… ì œì •ì¼ ì¶”ì¶œ (íŒŒì¼ëª…): {enactment_date}")
                                elif content_enactment:
                                    enactment_date = content_enactment
                                    print(f"  ğŸ“… ì œì •ì¼ ì¶”ì¶œ (íŒŒì¼ë‚´ìš©, íŒŒì¼ëª… ì—†ìŒ): {enactment_date}")
                                
                                if filename_revision:
                                    if content_revision and filename_revision != content_revision:
                                        print(f"  âš  ê°œì •ì¼ ë¶ˆì¼ì¹˜ - íŒŒì¼ëª…: {filename_revision}, íŒŒì¼ë‚´ìš©: {content_revision} (íŒŒì¼ëª… ì‚¬ìš©)")
                                    revision_date = filename_revision
                                    print(f"  ğŸ“… ìµœê·¼ ê°œì •ì¼ ì¶”ì¶œ (íŒŒì¼ëª…): {revision_date}")
                                elif content_revision:
                                    revision_date = content_revision
                                    print(f"  ğŸ“… ìµœê·¼ ê°œì •ì¼ ì¶”ì¶œ (íŒŒì¼ë‚´ìš©, íŒŒì¼ëª… ì—†ìŒ): {revision_date}")
                                
                                if content_department:
                                    department = content_department
                                    print(f"  ğŸ¢ ì†Œê´€ë¶€ì„œ ì¶”ì¶œ: {department}")
                                
                                if not enactment_date and not revision_date and not department:
                                    print("  âš  íŒŒì¼ëª…ê³¼ íŒŒì¼ë‚´ìš©(500ì) ëª¨ë‘ì—ì„œ ì œì •ì¼/ê°œì •ì¼/ì†Œê´€ë¶€ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                            else:
                                # íŒŒì¼ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œí•œ ê°’ ì‚¬ìš©
                                if filename_enactment:
                                    enactment_date = filename_enactment
                                if filename_revision:
                                    revision_date = filename_revision
                            
                            # contentë¥¼ 1000ìë¡œ ì œí•œ
                            content = full_content[:1000]
                            print(f"\nğŸ“„ {text} íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ "
                                  f"(ì›ë³¸: {original_length}ì, ì €ì¥: {len(content)}ì)")
                        except Exception as e:
                            content = f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"
                            print(f"  âš  íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                            # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œí•œ ê°’ ì‚¬ìš©
                            if filename_enactment:
                                enactment_date = filename_enactment
                            if filename_revision:
                                revision_date = filename_revision

                        # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì •ë¦¬ (ì˜µì…˜)
                        if self.cleanup_downloads:
                            try:
                                os.remove(filepath)
                                print(f"  ğŸ—‘ï¸ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì‚­ì œ: {file_name}")
                            except Exception as e:
                                print(f"  âš  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

                        item: Dict[str, str] = {
                            "no": str(item_count + 1),
                            "title": text,
                            "regulation_name": text,
                            "organization": "ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ",
                            "category": category_title,
                            "detail_link": download_url,
                            "file_download_link": download_url,
                            "file_name": file_name,
                            "content": content,
                            "enactment_date": enactment_date,
                            "revision_date": revision_date,
                            "department": department,
                        }
                        
                        results.append(item)
                        item_count += 1
                        
                        print(f"    [{link_idx}] {text[:50]}... -> {file_name[:60]}")
                        
                        # delay ì ìš© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                        if link_idx < len(link_data):
                            time.sleep(self.delay)
        
        print(f"\nì´ {len(results)}ê°œ í•­ëª© ì¶”ì¶œ ì™„ë£Œ")
        return results
    
    # ---------------- í¬ë¡¤ë§ ----------------
    def crawl_self_regulation_status(self, limit: int = 0, headless: bool = False) -> List[Dict]:
        """
        ììœ¨ê·œì œ í˜„í™© í¬ë¡¤ë§
        
        Args:
            limit: ê°€ì ¸ì˜¬ ê°œìˆ˜ ì œí•œ (0=ì „ì²´)
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ë‹¤ìš´ë¡œë“œ ì‹œ False ê¶Œì¥)
        """
        driver: Optional[webdriver.Chrome] = None
        try:
            driver = init_selenium(self.download_dir, headless=headless, scraper=self)
            print("Selenium ë“œë¼ì´ë²„ ìƒì„± ì™„ë£Œ")
        except Exception as exc:
            print(f"âš  Selenium ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {exc}")
            return []
        
        try:
            print(f"\ní˜ì´ì§€ ì ‘ì†: {self.LIST_URL}")
            driver.get(self.LIST_URL)
            time.sleep(3)
            
            soup = BeautifulSoup(driver.page_source, "lxml")
            results = self.extract_list_items(soup, driver, limit=limit)
            
        finally:
            if driver:
                driver.quit()
        
        return results
    
    def crawl_self_regulation_notice(self) -> List[Dict]:
        """ììœ¨ê·œì œ ì œÂ·ê°œì • ê³µê³  (ë¯¸êµ¬í˜„)"""
        return []

# ---------------- ì €ì¥ í•¨ìˆ˜ ----------------
def save_crefia_results(records: List[Dict]):
    if not records:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‚ ì§œ ì •ê·œí™”ë¥¼ ìœ„í•œ ì„ì‹œ BaseScraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    temp_scraper = CrefiaScraper()
    
    law_results = []
    for item in records:
        law_item = {
            "ë²ˆí˜¸": item.get("no", ""),
            "ê·œì •ëª…": item.get("regulation_name", ""),
            "ê¸°ê´€ëª…": item.get("organization", "ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ"),
            "ë³¸ë¬¸": item.get("content", ""),
            "ì œì •ì¼": temp_scraper.normalize_date_format(item.get("enactment_date", "")),
            "ìµœê·¼ ê°œì •ì¼": temp_scraper.normalize_date_format(item.get("revision_date", "")),
            "ì†Œê´€ë¶€ì„œ": item.get("department", ""),
            "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬": item.get("file_download_link", ""),
            "íŒŒì¼ ì´ë¦„": item.get("file_name", ""),
        }
        law_results.append(law_item)
    
    # JSON ì €ì¥
    json_dir = os.path.join("output", "json")
    os.makedirs(json_dir, exist_ok=True)
    json_path = os.path.join(json_dir, "crefia_scraper.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "url": CrefiaScraper.LIST_URL,
            "total_count": len(law_results),
            "results": law_results,
        }, f, ensure_ascii=False, indent=2)
    print(f"\nJSON ì €ì¥ ì™„ë£Œ: {json_path}")
    
    # CSV ì €ì¥
    csv_dir = os.path.join("output", "csv")
    os.makedirs(csv_dir, exist_ok=True)
    csv_path = os.path.join(csv_dir, "crefia_scraper.csv")
    headers = ["ë²ˆí˜¸", "ê·œì •ëª…", "ê¸°ê´€ëª…", "ë³¸ë¬¸", "ì œì •ì¼", "ìµœê·¼ ê°œì •ì¼", "ì†Œê´€ë¶€ì„œ", "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬", "íŒŒì¼ ì´ë¦„"]
    
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for law_item in law_results:
            csv_item = law_item.copy()
            csv_item["ë³¸ë¬¸"] = csv_item.get("ë³¸ë¬¸", "").replace("\n", " ").replace("\r", " ")
            writer.writerow(csv_item)
    print(f"CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

# ---------------- ì‹¤í–‰ ----------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(
        description="ì—¬ì‹ ê¸ˆìœµí˜‘íšŒ ììœ¨ê·œì œ í˜„í™© ìŠ¤í¬ë˜í¼"
    )

    parser.add_argument("--limit", type=int, default=0, help="ê°€ì ¸ì˜¬ ê°œìˆ˜ ì œí•œ (0=ì „ì²´)")

    parser.add_argument(
        "--cleanup", action="store_true",
        help="ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ë‚´ìš© ì¶”ì¶œ í›„ ì‚­ì œ"
    )
    
    parser.add_argument(
        "--clean-downloads", action="store_true",
        help="í¬ë¡¤ë§ ì‹œì‘ ì „ downloads í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì‚­ì œ"
    )
    
    args = parser.parse_args()
    
    crawler = CrefiaScraper(cleanup_downloads=args.cleanup, clean_downloads=args.clean_downloads)
    results = crawler.crawl_self_regulation_status(limit=args.limit)
    print(f"\nì¶”ì¶œëœ ë°ì´í„°: {len(results)}ê°œ")
    save_crefia_results(results)
