# 검사업무 메뉴얼 스크래퍼

금융감독원(FSS) 검사업무 메뉴얼 게시판에서 검사업무 관련 문서를 자동으로 수집하는 스크래퍼입니다.

## 📋 개요

이 스크래퍼는 금융감독원 검사업무 게시판에서 지정된 검사업무 안내서 및 매뉴얼을 자동으로 수집하고, 각 문서의 메타데이터를 추출하여 구조화된 형태로 저장합니다.

### 스크랩 대상 사이트
- **URL**: https://www.fss.or.kr/fss/bbs/B0000167/list.do?menuNo=200177
- **대상**: 검사업무 메뉴얼 게시판

## 🎯 주요 기능

1. **자동 게시글 수집**: 모든 페이지를 순회하며 게시글 목록 수집
2. **대상 필터링**: 지정된 검사업무 문서만 선택적으로 수집
3. **최신 데이터 선택**: 각 대상별로 최신 등록일의 데이터만 수집
4. **메타데이터 추출**: 구분, 제목 등 기본 정보 수집
5. **결과 저장**: JSON 및 CSV 형식으로 결과 저장

## 📦 스크랩 대상 리스트

현재 스크랩 대상은 다음과 같습니다 (코드 내 `SCRAPE_TARGETS` 변수에서 확인 가능):

1. 은행 검사매뉴얼
2. 여신전문금융 검사업무 안내서
3. 금융투자 검사업무 안내서
4. 저축은행 검사업무 안내서
5. 신용정보업 검사업무 안내서
6. IT 검사업무 안내서
7. 금융소비자보호법 검사업무 안내서
8. 자금세탁방지 검사업무 안내서
9. 퇴직연금 검사매뉴얼

> **참고**: 스크랩 대상 리스트는 `fss_scraper.py` 파일의 `SCRAPE_TARGETS` 변수에서 수정할 수 있습니다.

## 🚀 설치 방법

### 필수 패키지

```bash
pip install requests beautifulsoup4 lxml
```

### 선택적 패키지 (첨부파일 처리용)

첨부파일 다운로드 및 텍스트 추출 기능을 사용하려면 다음 패키지가 필요합니다:

```bash
# PDF 파일 처리
pip install pdfplumber

# HWP 파일 처리 (두 가지 방법 중 하나 또는 둘 다)
# 방법 1: olefile 사용
pip install olefile

# 방법 2: pyhwp5 사용
pip install pyhwp5
```

> **참고**: 현재 스크래퍼는 구분과 제목만 수집하므로, 첨부파일 처리는 선택사항입니다.

## 📖 사용 방법

### 기본 사용법

```bash
# 기본 실행 (현재 디렉터리에 결과 저장)
python fss_scraper.py

# 결과 저장 경로 지정
python fss_scraper.py --output-dir ./results

# 게시글 수 제한
python fss_scraper.py --limit 10

# 첨부파일 처리 건너뛰기 (현재는 사용되지 않지만 옵션으로 제공됨)
python fss_scraper.py --skip-attachments
```

### 파이프라인 실행

```bash
# run_pipeline.py를 사용하여 실행
python run_pipeline.py

# 옵션 지정
python run_pipeline.py --output-dir ./results --limit 10
```

### 명령줄 옵션

| 옵션 | 설명 | 기본값 |
|------|------|--------|
| `--output-dir` | 결과 파일 저장 경로 | 현재 디렉터리 |
| `--limit` | 수집할 게시글 수 제한 | 제한 없음 |
| `--skip-attachments` | 첨부파일 다운로드 및 처리 건너뛰기 | False |

## 📊 출력 형식

스크래퍼는 두 가지 형식으로 결과를 저장합니다:

### JSON 형식 (`inspection_results.json`)

```json
[
  {
    "구분": "은행",
    "제목": "은행 검사매뉴얼('25.7월)"
  },
  {
    "구분": "저축은행",
    "제목": "저축은행 검사업무안내서('25.5월)"
  }
]
```

### CSV 형식 (`inspection_results.csv`)

```csv
구분,제목
은행,은행 검사매뉴얼('25.7월)
저축은행,저축은행 검사업무안내서('25.5월)
```

## 🔧 주요 기능 설명

### 1. 게시글 수집 프로세스

스크래퍼는 다음 단계로 데이터를 수집합니다:

1. **전체 게시글 수집**: 모든 페이지를 순회하며 게시글 목록 수집
2. **대상 필터링**: 제목을 기반으로 스크랩 대상에 해당하는 게시글만 선택
3. **최신 데이터 선택**: 각 대상별로 최신 등록일의 게시글만 선택
4. **메타데이터 추출**: 선택된 게시글의 구분, 제목 정보 수집

### 2. 구분 추출 로직

- 제목의 첫 번째 단어를 구분으로 사용
- 예: "은행 검사매뉴얼('25.7월)" → 구분: "은행"

### 3. 제목 매칭 로직

- 공백 제거 및 대소문자 무시하여 매칭
- 대상 문자열이 제목에 포함되거나 그 반대인 경우 매칭

### 4. 최신 데이터 선택

- 각 스크랩 대상별로 그룹화
- 등록일 기준으로 정렬하여 최신 등록일의 모든 게시글 선택
- 같은 날 등록된 여러 게시글이 있으면 모두 포함

## 🗂️ 파일 구조

```
InspectionManual_Scraper/
├── fss_scraper.py          # 메인 스크래퍼 스크립트
├── run_pipeline.py         # 파이프라인 실행 스크립트
├── 검사업무스크랩핑조건.txt  # 스크랩핑 조건 문서
├── inspection_results.json # 결과 파일 (JSON)
└── inspection_results.csv  # 결과 파일 (CSV)
```

## ⚙️ 주요 함수

### `scrape_all()`
- 전체 스크랩핑 프로세스를 실행
- 게시글 수집, 필터링, 최신 데이터 선택, 메타데이터 추출 수행

### `is_target_item(title)`
- 제목이 스크랩 대상인지 확인

### `parse_bd_list(bd_list)`
- 게시판 목록에서 게시글 정보 추출 (bd-list 구조 지원)

### `parse_list_row(tr)`
- 테이블 행에서 게시글 정보 추출 (기존 테이블 구조 지원)

### `save_results(results, output_dir)`
- 수집된 결과를 JSON 및 CSV 파일로 저장

## ⚠️ 주의사항

1. **네트워크 요청**: 스크래퍼는 웹사이트에 여러 요청을 보내므로 네트워크 연결이 필요합니다.
2. **요청 간격**: 서버 부하를 방지하기 위해 각 요청 사이에 0.3초 대기 시간이 있습니다.
3. **타임아웃**: 기본 요청 타임아웃은 30초입니다.
4. **중복 방지**: nttId를 사용하여 중복 게시글 수집을 방지합니다.

## 🔍 디버깅

스크래퍼는 진행 상황을 콘솔에 출력합니다:

```
[정보] 모든 게시글 수집 시작 (대상: 9개)
[정보] 1페이지 수집 완료 - 총 20개 게시글
[정보] 전체 게시글 수집 완료: 150개
[정보] 대상 게시글 필터링 완료: 25개
[정보] 은행 검사매뉴얼: 최신 등록일(2025.07.15)의 게시글 1개 선택
[완료] 게시글 1번 수집 완료: nttId=12345
[저장] 게시글 1번 결과 저장 완료 (총 1개 레코드)
```

## 📝 수정 방법

### 스크랩 대상 리스트 수정

`fss_scraper.py` 파일의 `SCRAPE_TARGETS` 리스트를 수정하세요:

```python
SCRAPE_TARGETS = [
    "은행 검사매뉴얼",
    "여신전문금융 검사업무 안내서",
    # ... 추가 또는 수정
]
```

### 수집 항목 수정

현재는 구분과 제목만 수집하지만, `scrape_all()` 함수의 `record` 딕셔너리와 `save_results()` 함수의 `fieldnames`를 수정하여 추가 항목을 수집할 수 있습니다.

## 📄 라이선스

이 프로젝트는 개인/교육 목적으로 작성되었습니다.

## 🤝 기여

문제가 발생하거나 개선 사항이 있으면 이슈를 등록하거나 풀 리퀘스트를 보내주세요.




