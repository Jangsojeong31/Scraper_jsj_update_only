from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import csv
import os

class KrxPageCrawler:
    BASE_URL = "https://rule.krx.co.kr/out/index.do"

    def __init__(self, delay=1.0):
        self.delay = delay
        self.driver = self._init_driver()

        # í”„ë¡œì íŠ¸ ê¸°ì¤€ ê²½ë¡œ ìë™ íƒì§€
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        CSV_PATH = os.path.join(BASE_DIR, "input", "list.csv")

        self.filter_list = self._load_csv(CSV_PATH)

        self.OUTPUT_FILE = "filtered_data.csv"
        self.csv_file = open(self.OUTPUT_FILE, "w", encoding="utf-8", newline="")
        self.csv_writer = csv.writer(self.csv_file)
        self.csv_writer.writerow(["ë²ˆí˜¸", "ê·œì •ë²ˆí˜¸", "ê·œì •ëª…", "ì œÂ·ê°œì •", "ì œÂ·ê°œì •ì¼", "ì‹œí–‰ì¼", "ìƒì„¸ë‚´ìš©"])

    def _init_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--lang=ko-KR")
        service = Service()
        return webdriver.Chrome(service=service, options=chrome_options)

    def _load_csv(self, file_path):
        filter_items = set()
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            next(reader, None)
            for row in reader:
                filter_items.add(row[-1])
        print(f"CSVì—ì„œ {len(filter_items)}ê°œì˜ í•„í„° í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ")
        return filter_items

    def crawl(self):
        driver = self.driver
        wait = WebDriverWait(driver, 10)

        driver.get(self.BASE_URL)

        # ìµœê·¼ê°œì • ê·œì • ë²„íŠ¼ í´ë¦­
        wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "moreBtn"))).click()
        time.sleep(1)

        # iframe ì§„ì…
        wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe")))

        print("iframe ë‚´ë¶€ ì§„ì…")

        for keyword in self.filter_list:
            print(f"\nâ–¶ ê²€ìƒ‰ì–´: {keyword}")

            # ê²€ìƒ‰ì–´ ì…ë ¥
            search_input = wait.until(EC.presence_of_element_located((By.ID, "Schtxt")))
            search_input.clear()
            search_input.send_keys(keyword)

            driver.find_element(By.ID, "searchBtn").click()
            time.sleep(1.5)

            # ê²€ìƒ‰ ê²°ê³¼ row ë¡œë”© ëŒ€ê¸°
            rows = wait.until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".x-grid3-row"))
            )

            for idx, row_el in enumerate(rows):
                # row í…ìŠ¤íŠ¸ ì¶”ì¶œ
                cells = row_el.find_elements(By.CSS_SELECTOR, ".x-grid3-cell-inner")
                texts = [c.text.strip() for c in cells]

                print(f" - [{idx}] {texts}")

                # row í´ë¦­
                driver.execute_script("arguments[0].scrollIntoView(true);", row_el)
                row_el.click()
                time.sleep(0.5)

                # íŒì—… ê¸°ë‹¤ë¦¬ê³  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                try:
                    popup = wait.until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "#jo"))
                    )
                    content = popup.text.strip()[:200]  # ë„ˆë¬´ ê¸¸ë©´ 200ìë§Œ
                except:
                    content = "ìƒì„¸ ë‚´ìš© ì—†ìŒ"

                # CSV ì €ì¥
                self.csv_writer.writerow(texts + [content])

        # ì¢…ë£Œ
        self.csv_file.close()
        driver.quit()
        print("\nğŸ”¥ í¬ë¡¤ë§ ì™„ë£Œ â†’", os.path.abspath(self.OUTPUT_FILE))


if __name__ == "__main__":
    crawler = KrxPageCrawler(delay=1.0)
    crawler.crawl()
