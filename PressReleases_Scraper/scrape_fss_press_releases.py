"""
ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ ëª©ë¡ì—ì„œ ì²¨ë¶€íŒŒì¼(HWP, PDF ë“±)ì„ ëª¨ë‘ ì¶”ì¶œí•˜ê³ ,
ë³´ë„ì¼ì„ HWPì—ì„œë§Œ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (CSV/Excel/JSON ì €ì¥)
"""
import requests
from bs4 import BeautifulSoup
import re
import io
import time
import olefile
from urllib.parse import urljoin
import pandas as pd
import json
from openpyxl.utils import get_column_letter


# -----------------------------------------------------------
# HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# -----------------------------------------------------------
def extract_text_from_hwp_bytes(hwp_bytes):
    """HWP íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì½ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with olefile.OleFileIO(io.BytesIO(hwp_bytes)) as ole:
            text_content = ""
            possible_paths = ['PrvText', 'BodyText/Section0', 'Section0', 'DocInfo', 'BodyText']
            for path in possible_paths:
                if ole.exists(path):
                    data = ole.openstream(path).read()
                    try:
                        text = data.decode('utf-16-le', errors='ignore')
                        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text)

                        if len(text.strip()) > 10:
                            text_content = text
                            break
                    except Exception:
                        pass
            return text_content

    except Exception as e:
        print(f"    âš ï¸ HWP íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return ""


# -----------------------------------------------------------
# í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
# -----------------------------------------------------------
def extract_first_date(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì²˜ìŒ ë‚˜íƒ€ë‚˜ëŠ” ë‚ ì§œ ì¶”ì¶œ (ë³´ë„ì¼)"""
    if not text:
        return None

    date_patterns = [
        r'(\d{4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'(\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\s*\(?[ê°€-í£]*\)?)',
        r'(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4}/\d{1,2}/\d{1,2})',
        r'(\d{8})',
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()

    return None


# -----------------------------------------------------------
# ë³´ë„ìë£Œ ëª©ë¡ ìŠ¤í¬ë˜í•‘
# -----------------------------------------------------------
def scrape_press_releases(base_url):
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})

    results = []
    print("ğŸ“¢ ë³´ë„ìë£Œ ëª©ë¡ ì²˜ë¦¬ ì¤‘...\n")

    try:
        response = session.get(base_url, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.find('table', class_='board_list') or soup.find('table')

        if not table:
            print("âŒ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return results

        rows = table.find_all('tr')[1:]
        if not rows:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return results

        # ê° ë³´ë„ìë£Œ í–‰ ë°˜ë³µ ì²˜ë¦¬
        for idx, row in enumerate(rows, start=1):
            title_link = row.find('a', href=re.compile(r'view\.do'))
            if not title_link:
                continue

            # ì œëª©, ìƒì„¸ URL
            title = title_link.get_text(strip=True)
            detail_url = urljoin(base_url, title_link['href'])

            # ë‹´ë‹¹ë¶€ì„œ
            tds = row.find_all('td')
            department = tds[2].get_text(strip=True) if len(tds) >= 3 else None

            # ì²¨ë¶€íŒŒì¼ (.hwp, .pdf, ë“±)
            file_links = []
            attach_links = row.find_all('a', href=re.compile(r'fileDown\.do'))

            for link in attach_links:
                href = urljoin(base_url, link['href'])
                file_name = link.get_text(strip=True)

                file_links.append({
                    'ì²¨ë¶€íŒŒì¼ëª…': file_name,
                    'ì²¨ë¶€íŒŒì¼ url': href
                })

            print(f"[{idx}] {title}")
            if not file_links:
                print("    âš ï¸ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")

            # ìƒì„¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            try:
                detail_response = session.get(detail_url, timeout=30)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                content_div = detail_soup.find('div', class_='dbdata')
                content = content_div.get_text(separator='\n', strip=True) if content_div else ''
                content = re.sub(r'\n+', '\n', content.strip())
            except Exception as e:
                print(f"    âš ï¸ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                content = ''

            # HWP íŒŒì¼ì„ í†µí•œ ë³´ë„ì¼ ì¶”ì¶œ
            date = None
            text_preview = None

            hwp_files = [f for f in file_links if f['ì²¨ë¶€íŒŒì¼ëª…'].lower().endswith('.hwp')]

            for f in hwp_files:
                try:
                    print(f"    ğŸ“‚ HWP ë‹¤ìš´ë¡œë“œ ì¤‘: {f['ì²¨ë¶€íŒŒì¼ëª…']}")
                    file_response = session.get(f['ì²¨ë¶€íŒŒì¼ url'], timeout=30)
                    file_response.raise_for_status()

                    text = extract_text_from_hwp_bytes(file_response.content)
                    if text:
                        date = extract_first_date(text)
                        text_preview = text[:200]
                        print(f"    ğŸ“… ë³´ë„ì¼: {date or 'ì¶”ì¶œ ì‹¤íŒ¨'}")
                        break

                except Exception as e:
                    print(f"    âš ï¸ HWP ì²˜ë¦¬ ì‹¤íŒ¨ ({f['ì²¨ë¶€íŒŒì¼ëª…']}): {e}")

            # ê²°ê³¼ ì €ì¥
            results.append({
                'ë²ˆí˜¸': idx,
                'ì œëª©': title,
                'ë‹´ë‹¹ë¶€ì„œ': department,
                'ë³´ë„ì¼': date,
                'ì²¨ë¶€íŒŒì¼': file_links,
                'ì²¨ë¶€íŒŒì¼ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°': text_preview,
                'ìƒì„¸í˜ì´ì§€URL': detail_url,
                'ë‚´ìš©': content
            })

            time.sleep(0.5)

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    return results


# -----------------------------------------------------------
# CSV / Excel / JSON ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------
def save_results(results, csv_file="results.csv", excel_file="results.xlsx", json_file="results.json"):
    if not results:
        print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(results)

    # ì²¨ë¶€íŒŒì¼ ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜
    df['ì²¨ë¶€íŒŒì¼'] = df['ì²¨ë¶€íŒŒì¼'].apply(
        lambda lst: ', '.join([f"{f['ì²¨ë¶€íŒŒì¼ëª…']} ({f['ì²¨ë¶€íŒŒì¼ url']})" for f in lst]) if lst else ''
    )

    df.fillna('', inplace=True)

    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    df = df[['ë²ˆí˜¸', 'ì œëª©', 'ë³´ë„ì¼', 'ìƒì„¸í˜ì´ì§€URL', 'ì²¨ë¶€íŒŒì¼', 'ë‹´ë‹¹ë¶€ì„œ', 'ë‚´ìš©']]

    # CSV ì €ì¥
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ CSV ì €ì¥ ì™„ë£Œ: {csv_file}")

    # Excel ì €ì¥
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ë³´ë„ìë£Œ')
        ws = writer.sheets['ë³´ë„ìë£Œ']

        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
        for i, col in enumerate(df.columns, start=1):
            max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
            ws.column_dimensions[get_column_letter(i)].width = min(max_len, 50)

    print(f"ğŸ“˜ Excel ì €ì¥ ì™„ë£Œ: {excel_file}")

    # JSON ì €ì¥
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ§¾ JSON ì €ì¥ ì™„ë£Œ: {json_file}")


# -----------------------------------------------------------
# ì‹¤í–‰ ë©”ì¸
# -----------------------------------------------------------
def main():
    base_url = "https://www.fss.or.kr/fss/bbs/B0000188/list.do?menuNo=200218&pageIndex=1"
    print("ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    print("=" * 70)

    results = scrape_press_releases(base_url)

    print("=" * 70)
    print(f"ì´ {len(results)}ê°œ ë³´ë„ìë£Œ ì²˜ë¦¬ ì™„ë£Œ")

    success = sum(1 for r in results if r['ë³´ë„ì¼'])
    if results:
        print(f"ë³´ë„ì¼ ì¶”ì¶œ ì„±ê³µë¥ : {success}/{len(results)} ({success/len(results)*100:.1f}%)")

    save_results(results)


if __name__ == "__main__":
    main()
