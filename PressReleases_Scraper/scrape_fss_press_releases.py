"""
ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ ëª©ë¡ì—ì„œ ì²¨ë¶€íŒŒì¼(HWP, PDF ë“±)ì„ ëª¨ë‘ ì¶”ì¶œí•˜ê³ ,
ë³´ë„ì¼ì„ HWPì—ì„œë§Œ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (CSV/Excel/JSON ì €ì¥)
"""
import requests
from bs4 import BeautifulSoup
import re
import io
import time
import olefile
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import pandas as pd
import json
from openpyxl.utils import get_column_letter


# -----------------------------------------------------------
# HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# -----------------------------------------------------------
def extract_text_from_hwp_bytes(hwp_bytes):
    """HWP íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì½ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with olefile.OleFileIO(io.BytesIO(hwp_bytes)) as ole:
            text_content = ""
            possible_paths = ['PrvText', 'BodyText/Section0', 'Section0', 'DocInfo', 'BodyText']
            for path in possible_paths:
                if ole.exists(path):
                    data = ole.openstream(path).read()
                    try:
                        text = data.decode('utf-16-le', errors='ignore')
                        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text)

                        if len(text.strip()) > 10:
                            text_content = text
                            break
                    except Exception:
                        pass
            return text_content

    except Exception as e:
        print(f"    âš ï¸ HWP íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return ""


# -----------------------------------------------------------
# í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
# -----------------------------------------------------------
def extract_first_date(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì²˜ìŒ ë‚˜íƒ€ë‚˜ëŠ” ë‚ ì§œ ì¶”ì¶œ (ë³´ë„ì¼)"""
    if not text:
        return None

    date_patterns = [
        r'(\d{4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'(\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\s*\(?[ê°€-í£]*\)?)',
        r'(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4}/\d{1,2}/\d{1,2})',
        r'(\d{8})',
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()

    return None


# -----------------------------------------------------------
# ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
# -----------------------------------------------------------
def has_next_page(soup, current_page):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°)
        next_texts = ['ë‹¤ìŒ', '>', 'â–¶', 'next', 'Next']
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            text = link.get_text(strip=True)
            if any(next_text in text for next_text in next_texts):
                href = link.get("href", "").strip()
                if href and "pageIndex=" in href:
                    return True
        
        # .next í´ë˜ìŠ¤ ì°¾ê¸°
        next_links = soup.select(".next, .paging .next, .pagination .next")
        if next_links:
            return True
        
        # í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ì—ì„œ í˜„ì¬ í˜ì´ì§€ë³´ë‹¤ í° ë²ˆí˜¸ ì°¾ê¸°
        pagination_selectors = [
            "div.paging",
            "div.pagination",
            "div.pageArea",
            ".paging",
            ".pagination",
            ".pageArea",
        ]
        
        for selector in pagination_selectors:
            pagination = soup.select_one(selector)
            if pagination:
                page_links = pagination.select("a[href]")
                for link in page_links:
                    text = link.get_text(strip=True)
                    if text.isdigit() and int(text) > current_page:
                        return True
                    
                    href = link.get("href", "").strip()
                    if href and "pageIndex=" in href:
                        match = re.search(r'pageIndex=(\d+)', href)
                        if match:
                            page_num = int(match.group(1))
                            if page_num > current_page:
                                return True
        
        # ì „ì²´ ë§í¬ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ ì°¾ê¸°
        for link in all_links:
            href = link.get("href", "").strip()
            if href and "pageIndex=" in href:
                match = re.search(r'pageIndex=(\d+)', href)
                if match:
                    page_num = int(match.group(1))
                    if page_num > current_page:
                        return True
        
        return False
        
    except Exception as e:
        print(f"    âš ï¸ ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


# -----------------------------------------------------------
# ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë³´ë„ìë£Œ ì¶”ì¶œ
# -----------------------------------------------------------
def scrape_single_page(session, page_url, page_num, total_pages, start_idx=1):
    """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë³´ë„ìë£Œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    results = []
    
    try:
        response = session.get(page_url, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.find('table', class_='board_list') or soup.find('table')

        if not table:
            print(f"    âš ï¸ í˜ì´ì§€ {page_num}: í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return results

        rows = table.find_all('tr')[1:]
        if not rows:
            print(f"    âš ï¸ í˜ì´ì§€ {page_num}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return results

        if total_pages > 0:
            print(f"\nğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì¤‘... ({len(rows)}ê°œ í•­ëª©)")
        else:
            print(f"\nğŸ“„ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘... ({len(rows)}ê°œ í•­ëª©)")

        # ê° ë³´ë„ìë£Œ í–‰ ë°˜ë³µ ì²˜ë¦¬
        for row_idx, row in enumerate(rows, start=start_idx):
            title_link = row.find('a', href=re.compile(r'view\.do'))
            if not title_link:
                continue

            # ì œëª©, ìƒì„¸ URL
            title = title_link.get_text(strip=True)
            detail_url = urljoin(page_url, title_link['href'])

            # ë‹´ë‹¹ë¶€ì„œ
            tds = row.find_all('td')
            department = tds[2].get_text(strip=True) if len(tds) >= 3 else None

            # ì²¨ë¶€íŒŒì¼ (.hwp, .pdf, ë“±)
            file_links = []
            attach_links = row.find_all('a', href=re.compile(r'fileDown\.do'))

            for link in attach_links:
                href = urljoin(page_url, link['href'])
                file_name = link.get_text(strip=True)

                file_links.append({
                    'ì²¨ë¶€íŒŒì¼ëª…': file_name,
                    'ì²¨ë¶€íŒŒì¼ url': href
                })

            print(f"  [{row_idx}] {title}")
            if not file_links:
                print("      âš ï¸ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")

            # ìƒì„¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            try:
                detail_response = session.get(detail_url, timeout=30)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                content_div = detail_soup.find('div', class_='dbdata')
                content = content_div.get_text(separator='\n', strip=True) if content_div else ''
                content = re.sub(r'\n+', '\n', content.strip())
            except Exception as e:
                print(f"      âš ï¸ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                content = ''

            # HWP íŒŒì¼ì„ í†µí•œ ë³´ë„ì¼ ì¶”ì¶œ
            date = None
            text_preview = None

            hwp_files = [f for f in file_links if f['ì²¨ë¶€íŒŒì¼ëª…'].lower().endswith('.hwp')]

            for f in hwp_files:
                try:
                    print(f"      ğŸ“‚ HWP ë‹¤ìš´ë¡œë“œ ì¤‘: {f['ì²¨ë¶€íŒŒì¼ëª…']}")
                    file_response = session.get(f['ì²¨ë¶€íŒŒì¼ url'], timeout=30)
                    file_response.raise_for_status()

                    text = extract_text_from_hwp_bytes(file_response.content)
                    if text:
                        date = extract_first_date(text)
                        text_preview = text[:200]
                        print(f"      ğŸ“… ë³´ë„ì¼: {date or 'ì¶”ì¶œ ì‹¤íŒ¨'}")
                        break

                except Exception as e:
                    print(f"      âš ï¸ HWP ì²˜ë¦¬ ì‹¤íŒ¨ ({f['ì²¨ë¶€íŒŒì¼ëª…']}): {e}")

            # ê²°ê³¼ ì €ì¥
            results.append({
                'ë²ˆí˜¸': row_idx,
                'ì œëª©': title,
                'ë‹´ë‹¹ë¶€ì„œ': department,
                'ë³´ë„ì¼': date,
                'ì²¨ë¶€íŒŒì¼': file_links,
                'ì²¨ë¶€íŒŒì¼ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°': text_preview,
                'ìƒì„¸í˜ì´ì§€URL': detail_url,
                'ë‚´ìš©': content
            })

            time.sleep(0.5)

    except Exception as e:
        print(f"    âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    return results


# -----------------------------------------------------------
# ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
# -----------------------------------------------------------
def load_existing_data(json_file="results.json"):
    """ê¸°ì¡´ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(data)}ê±´ ë°œê²¬")
            return data
    except FileNotFoundError:
        print("ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ (ì²˜ìŒë¶€í„° ì‹œì‘)")
        return []
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e} (ì²˜ìŒë¶€í„° ì‹œì‘)")
        return []


# -----------------------------------------------------------
# ë³´ë„ìë£Œ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ëª¨ë“  í˜ì´ì§€)
# -----------------------------------------------------------
def scrape_press_releases(base_url, total_pages=2010, resume=True):
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})

    results = []
    print("ğŸ“¢ ë³´ë„ìë£Œ ëª©ë¡ ì²˜ë¦¬ ì¤‘...\n")
    print(f"ğŸ“Š ì´ ì˜ˆìƒ í˜ì´ì§€ ìˆ˜: {total_pages}í˜ì´ì§€\n")

    try:
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        all_results = []
        item_counter = 1
        start_page = 1
        
        if resume:
            existing_data = load_existing_data("results.json")
            if existing_data:
                all_results = existing_data
                item_counter = len(existing_data) + 1
                # í˜ì´ì§€ë‹¹ ì•½ 10ê±´ ê°€ì •í•˜ì—¬ ì‹œì‘ í˜ì´ì§€ ê³„ì‚°
                # ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ ë§ˆì§€ë§‰ í•­ëª©ì˜ ìƒì„¸í˜ì´ì§€URLì—ì„œ pageIndex ì¶”ì¶œ ì‹œë„
                last_item = existing_data[-1]
                last_url = last_item.get('ìƒì„¸í˜ì´ì§€URL', '')
                if 'pageIndex=' in last_url:
                    match = re.search(r'pageIndex=(\d+)', last_url)
                    if match:
                        start_page = int(match.group(1)) + 1  # ë‹¤ìŒ í˜ì´ì§€ë¶€í„° ì‹œì‘
                else:
                    # URLì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í•­ëª© ìˆ˜ë¡œ ê³„ì‚° (í˜ì´ì§€ë‹¹ 10ê±´ ê°€ì •)
                    start_page = (len(existing_data) // 10) + 1
                
                print(f"ğŸ”„ ì´ì–´ì„œ ì§„í–‰: {len(existing_data)}ê±´ ì €ì¥ë¨, í˜ì´ì§€ {start_page}ë¶€í„° ì‹œì‘\n")
                import sys
                sys.stdout.flush()
        
        # URLì—ì„œ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        parsed = urlparse(base_url)
        params = parse_qs(parsed.query)
        
        page_num = start_page
        save_interval = 50  # 50í˜ì´ì§€ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        
        start_time = time.time()
        
        while page_num <= total_pages:
            # í˜ì´ì§€ URL ìƒì„±
            params['pageIndex'] = [str(page_num)]
            new_query = urlencode(params, doseq=True)
            page_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))
            
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            try:
                response = session.get(page_url, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # í…Œì´ë¸” í™•ì¸
                table = soup.find('table', class_='board_list') or soup.find('table')
                if not table:
                    print(f"  âš ï¸ í˜ì´ì§€ {page_num}: í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                rows = table.find_all('tr')[1:]
                if not rows:
                    print(f"  âš ï¸ í˜ì´ì§€ {page_num}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
                page_results = scrape_single_page(session, page_url, page_num, total_pages, start_idx=item_counter)
                
                if not page_results:
                    print(f"  âš ï¸ í˜ì´ì§€ {page_num}: ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ë²ˆí˜¸ ì—…ë°ì´íŠ¸
                for result in page_results:
                    result['ë²ˆí˜¸'] = item_counter
                    item_counter += 1
                
                all_results.extend(page_results)
                
                # ì§„í–‰ë¥  ê³„ì‚°
                progress = (page_num / total_pages) * 100
                elapsed_time = time.time() - start_time
                avg_time_per_page = elapsed_time / page_num if page_num > 0 else 0
                remaining_pages = total_pages - page_num
                estimated_remaining_time = avg_time_per_page * remaining_pages
                
                # ì‹œê°„ í¬ë§·íŒ…
                def format_time(seconds):
                    hours = int(seconds // 3600)
                    minutes = int((seconds % 3600) // 60)
                    secs = int(seconds % 60)
                    if hours > 0:
                        return f"{hours}ì‹œê°„ {minutes}ë¶„ {secs}ì´ˆ"
                    elif minutes > 0:
                        return f"{minutes}ë¶„ {secs}ì´ˆ"
                    else:
                        return f"{secs}ì´ˆ"
                
                print(f"  âœ… í˜ì´ì§€ {page_num}/{total_pages} ì™„ë£Œ ({progress:.1f}%) | "
                      f"ì¶”ì¶œ: {len(page_results)}ê°œ | ëˆ„ì : {len(all_results)}ê°œ | "
                      f"ê²½ê³¼: {format_time(elapsed_time)} | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {format_time(estimated_remaining_time)}")
                import sys
                sys.stdout.flush()  # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ ë²„í¼ í”ŒëŸ¬ì‹œ
                
                # ì¤‘ê°„ ì €ì¥ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if page_num % save_interval == 0:
                    print(f"\n  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì¤‘... (í˜ì´ì§€ {page_num})")
                    save_results(all_results, 
                               csv_file="results.csv", 
                               excel_file="results.xlsx", 
                               json_file="results.json")
                    print(f"  âœ… ì¤‘ê°„ ì €ì¥ ì™„ë£Œ\n")
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
                if page_num < total_pages and not has_next_page(soup, page_num):
                    print(f"\n  âš ï¸ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì§€ë§Œ, ì•„ì§ {total_pages - page_num}í˜ì´ì§€ê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤.")
                    print(f"  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...\n")
                
                page_num += 1
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ê°„ ì €ì¥
                if all_results:
                    print(f"\n  ğŸ’¾ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¤‘ê°„ ì €ì¥ ì¤‘...")
                    save_results(all_results, 
                               csv_file="results.csv", 
                               excel_file="results.xlsx", 
                               json_file="results.json")
                # ì—°ì† ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¢…ë£Œ
                break
        
        results = all_results
        total_time = time.time() - start_time
        print(f"\nğŸ“Š ì´ {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {format_time(total_time)})")

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì¤‘ê°„ ì €ì¥
        if 'all_results' in locals() and all_results:
            print(f"\n  ğŸ’¾ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¤‘ê°„ ì €ì¥ ì¤‘...")
            save_results(all_results, 
                       csv_file="results.csv", 
                       excel_file="results.xlsx", 
                       json_file="results.json")

    return results


# -----------------------------------------------------------
# CSV / Excel / JSON ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------
def save_results(results, csv_file="results.csv", excel_file="results.xlsx", json_file="results.json"):
    if not results:
        print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(results)

    # ì²¨ë¶€íŒŒì¼ ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜
    df['ì²¨ë¶€íŒŒì¼'] = df['ì²¨ë¶€íŒŒì¼'].apply(
        lambda lst: ', '.join([f"{f['ì²¨ë¶€íŒŒì¼ëª…']} ({f['ì²¨ë¶€íŒŒì¼ url']})" for f in lst]) if lst else ''
    )

    df.fillna('', inplace=True)

    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    df = df[['ë²ˆí˜¸', 'ì œëª©', 'ë³´ë„ì¼', 'ìƒì„¸í˜ì´ì§€URL', 'ì²¨ë¶€íŒŒì¼', 'ë‹´ë‹¹ë¶€ì„œ', 'ë‚´ìš©']]

    # CSV ì €ì¥
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ CSV ì €ì¥ ì™„ë£Œ: {csv_file}")

    # Excel ì €ì¥
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ë³´ë„ìë£Œ')
        ws = writer.sheets['ë³´ë„ìë£Œ']

        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
        for i, col in enumerate(df.columns, start=1):
            max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
            ws.column_dimensions[get_column_letter(i)].width = min(max_len, 50)

    print(f"ğŸ“˜ Excel ì €ì¥ ì™„ë£Œ: {excel_file}")

    # JSON ì €ì¥
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ§¾ JSON ì €ì¥ ì™„ë£Œ: {json_file}")


# -----------------------------------------------------------
# ì‹¤í–‰ ë©”ì¸
# -----------------------------------------------------------
def main():
    import sys
    # ì¶œë ¥ ë²„í¼ë§ ë¹„í™œì„±í™” (ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´)
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    
    base_url = "https://www.fss.or.kr/fss/bbs/B0000188/list.do?menuNo=200218&pageIndex=1"
    total_pages = 2010  # ì´ í˜ì´ì§€ ìˆ˜
    print("ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    print("=" * 70)
    sys.stdout.flush()

    results = scrape_press_releases(base_url, total_pages=total_pages)

    print("=" * 70)
    print(f"ì´ {len(results)}ê°œ ë³´ë„ìë£Œ ì²˜ë¦¬ ì™„ë£Œ")

    success = sum(1 for r in results if r['ë³´ë„ì¼'])
    if results:
        print(f"ë³´ë„ì¼ ì¶”ì¶œ ì„±ê³µë¥ : {success}/{len(results)} ({success/len(results)*100:.1f}%)")

    save_results(results)


if __name__ == "__main__":
    main()
