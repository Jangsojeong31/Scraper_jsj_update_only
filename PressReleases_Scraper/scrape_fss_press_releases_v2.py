"""
ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ ëª©ë¡ì—ì„œ ì²¨ë¶€íŒŒì¼(HWP, PDF ë“±)ì„ ëª¨ë‘ ì¶”ì¶œí•˜ê³ ,
ë³´ë„ì¼ì„ HWPì—ì„œë§Œ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (CSV/Excel/JSON ì €ì¥)
"""
import requests
from bs4 import BeautifulSoup
import re
import io
import time
import olefile
import zipfile
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import pandas as pd
import json
from openpyxl.utils import get_column_letter
from datetime import datetime, timedelta
try:
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("âš ï¸ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDF íŒŒì¼ ì²˜ë¦¬ê°€ ì œí•œë©ë‹ˆë‹¤.")


# -----------------------------------------------------------
# HWPX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ZIP ê¸°ë°˜ XML)
# -----------------------------------------------------------
def extract_text_from_hwpx_bytes(hwpx_bytes):
    """HWPX íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì½ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ZIP ì••ì¶•ëœ XML)"""
    try:
        with zipfile.ZipFile(io.BytesIO(hwpx_bytes)) as zip_file:
            text_content = ""
            
            # HWPX íŒŒì¼ ë‚´ë¶€ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì°¾ê¸°
            # ì¼ë°˜ì ìœ¼ë¡œ Contents/section0.xml, Contents/section1.xml ë“±ì— í…ìŠ¤íŠ¸ê°€ ìˆìŒ
            possible_paths = [
                'Contents/section0.xml',
                'Contents/section1.xml',
                'section0.xml',
                'section1.xml',
                'body.xml',
                'Contents/body.xml'
            ]
            
            for path in possible_paths:
                try:
                    if path in zip_file.namelist():
                        xml_data = zip_file.read(path)
                        # XML íŒŒì‹±
                        root = ET.fromstring(xml_data)
                        
                        # XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        texts = []
                        for elem in root.iter():
                            if elem.text:
                                texts.append(elem.text.strip())
                            if elem.tail:
                                texts.append(elem.tail.strip())
                        
                        text = ' '.join([t for t in texts if t])
                        text = re.sub(r'\s+', ' ', text)
                        
                        if len(text.strip()) > 10:
                            text_content = text
                            break
                except Exception:
                    continue
            
            return text_content
            
    except Exception as e:
        print(f"    âš ï¸ HWPX íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return ""


# -----------------------------------------------------------
# HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OLE2 í˜•ì‹)
# -----------------------------------------------------------
def extract_text_from_hwp_bytes(hwp_bytes):
    """HWP íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì½ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OLE2 í˜•ì‹)"""
    try:
        with olefile.OleFileIO(io.BytesIO(hwp_bytes)) as ole:
            text_content = ""
            possible_paths = ['PrvText', 'BodyText/Section0', 'Section0', 'DocInfo', 'BodyText']
            for path in possible_paths:
                if ole.exists(path):
                    data = ole.openstream(path).read()
                    try:
                        text = data.decode('utf-16-le', errors='ignore')
                        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text)

                        if len(text.strip()) > 10:
                            text_content = text
                            break
                    except Exception:
                        pass
            return text_content

    except Exception as e:
        # OLE2 í˜•ì‹ì´ ì•„ë‹ˆë©´ HWPXì¼ ìˆ˜ ìˆìŒ
        return ""


# -----------------------------------------------------------
# PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# -----------------------------------------------------------
def extract_text_from_pdf_bytes(pdf_bytes):
    """PDF íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì½ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not PDF_AVAILABLE:
        return ""
    
    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            text_content = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"
            return text_content.strip()
    except Exception as e:
        print(f"    âš ï¸ PDF íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return ""


# -----------------------------------------------------------
# í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
# -----------------------------------------------------------
def extract_date_near_keyword(text, keyword, context_range=150):
    """íŠ¹ì • í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ë‚ ì§œë¥¼ ì°¾ìŠµë‹ˆë‹¤ (í‘œ í˜•ì‹ë„ ê³ ë ¤)"""
    if not text or not keyword:
        return None
    
    # í‚¤ì›Œë“œ ì•ë’¤ ì§€ì •ëœ ë²”ìœ„ ë‚´ì—ì„œ ë‚ ì§œ ì°¾ê¸°
    pattern = re.compile(
        rf'.{{0,{context_range}}}{re.escape(keyword)}.{{0,{context_range}}}',
        re.IGNORECASE | re.DOTALL
    )
    matches = pattern.finditer(text)
    
    date_patterns = [
        r'(\'?\d{2,4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'(\'?\d{2,4}\.\s*\d{1,2}\.\s*\d{1,2}\s*\(?[ê°€-í£]*\)?)',
        r'(\'?\d{2,4}-\d{1,2}-\d{1,2})',
        r'(\'?\d{2,4}/\d{1,2}/\d{1,2})',
        r'(\d{8})',  # 8ìë¦¬ ìˆ«ì
        r'(\d{10})',  # 10ìë¦¬ ìˆ«ì (ì˜ˆ: 25032011)
    ]
    
    for match in matches:
        context = match.group(0)
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        for date_pattern in date_patterns:
            date_match = re.search(date_pattern, context)
            if date_match:
                date_str = date_match.group(1).strip()
                
                # 10ìë¦¬ ìˆ«ì í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: 25032011 -> 2025-03-20)
                if len(date_str) == 10 and date_str.isdigit():
                    year = int(date_str[:2])
                    month = int(date_str[2:4])
                    day = int(date_str[4:6])
                    if year >= 50:
                        full_year = 1900 + year
                    else:
                        full_year = 2000 + year
                    date_str = f"{full_year}.{month}.{day}"
                
                # '25 ê°™ì€ í˜•ì‹ì„ 2025ë¡œ ë³€í™˜
                date_str = normalize_year_format(date_str)
                # ë…„ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë…„ë„ ì¶”ê°€
                date_str = add_year_if_missing(date_str)
                return date_str
    
    # í‘œ í˜•ì‹ ì²˜ë¦¬: í‚¤ì›Œë“œ ë‹¤ìŒ ì¤„ì´ë‚˜ ê°™ì€ ì¤„ì— ë‚ ì§œê°€ ìˆì„ ìˆ˜ ìˆìŒ
    # ì˜ˆ: "ë³´ ë„\n2025.3.20" ë˜ëŠ” "ë³´ë„ì¼\t2025.3.20"
    lines = text.split('\n')
    for i, line in enumerate(lines):
        if keyword in line:
            # ê°™ì€ ì¤„ì—ì„œ ì°¾ê¸°
            for date_pattern in date_patterns:
                date_match = re.search(date_pattern, line)
                if date_match:
                    date_str = date_match.group(1).strip()
                    if len(date_str) == 10 and date_str.isdigit():
                        year = int(date_str[:2])
                        month = int(date_str[2:4])
                        day = int(date_str[4:6])
                        if year >= 50:
                            full_year = 1900 + year
                        else:
                            full_year = 2000 + year
                        date_str = f"{full_year}.{month}.{day}"
                    date_str = normalize_year_format(date_str)
                    date_str = add_year_if_missing(date_str)
                    return date_str
            
            # ë‹¤ìŒ ì¤„ì—ì„œ ì°¾ê¸°
            if i + 1 < len(lines):
                next_line = lines[i + 1]
                for date_pattern in date_patterns:
                    date_match = re.search(date_pattern, next_line)
                    if date_match:
                        date_str = date_match.group(1).strip()
                        if len(date_str) == 10 and date_str.isdigit():
                            year = int(date_str[:2])
                            month = int(date_str[2:4])
                            day = int(date_str[4:6])
                            if year >= 50:
                                full_year = 1900 + year
                            else:
                                full_year = 2000 + year
                            date_str = f"{full_year}.{month}.{day}"
                        date_str = normalize_year_format(date_str)
                        date_str = add_year_if_missing(date_str)
                        return date_str
    
    return None


# -----------------------------------------------------------
# í…ìŠ¤íŠ¸ì—ì„œ ë³´ë„ì¼ ì¶”ì¶œ (ë³´ë„ì¼ ìš°ì„ , ì—†ìœ¼ë©´ ë°°í¬ì¼)
# -----------------------------------------------------------
def extract_first_date(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ë³´ë„ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    1. ë³´ë„ì¼ ê´€ë ¨ í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ìš°ì„  ê²€ìƒ‰
    2. ë³´ë„ì¼ì„ ëª» ì°¾ìœ¼ë©´ ë°°í¬ì¼ ê´€ë ¨ í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ê²€ìƒ‰
    3. "ë³´ë„ê°€ ë°°í¬ ì‹œ" ê°™ì€ ë³µí•© íŒ¨í„´ ì²˜ë¦¬
    4. ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰"""
    if not text:
        return None

    # 0ë‹¨ê³„: "ë³´ë„ì‹œì ì€ ë°°í¬ì‹œ", "ë³´ë„ì‹œì : ë°°í¬ì‹œ", "ë³´ ë„" ê°™ì€ ë³µí•© íŒ¨í„´ ì²˜ë¦¬
    # ì´ ê²½ìš° "ë°°í¬ì‹œ" ë‹¤ìŒì— ì˜¤ëŠ” ë‚ ì§œë¥¼ ì°¾ì•„ì•¼ í•¨
    complex_patterns = [
        # "ë³´ ë„" íŒ¨í„´ (ë„ì–´ì“°ê¸° í¬í•¨) - ë¬¸ì„œ ìƒë‹¨ì— ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í˜•ì‹
        r'ë³´\s+ë„\s*[:ï¼š]?\s*(\'?\d{2,4}\.\s*\d{1,2}\.\s*\d{1,2})',
        r'ë³´\s+ë„\s*[:ï¼š]?\s*(\'?\d{2,4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'ë³´\s+ë„\s*[:ï¼š]?\s*(\'?\d{2,4}-\d{1,2}-\d{1,2})',
        r'ë³´\s+ë„\s*[:ï¼š]?\s*(\'?\d{2,4}/\d{1,2}/\d{1,2})',
        r'ë³´\s+ë„\s*[:ï¼š]?\s*(\d{10})',  # 10ìë¦¬ ìˆ«ì (ì˜ˆ: 25032011)
        # "ë³´ë„ì‹œì ì€ ë°°í¬ì‹œ" ë˜ëŠ” "ë³´ë„ì‹œì : ë°°í¬ì‹œ" íŒ¨í„´
        r'ë³´ë„\s*ì‹œì \s*[ì€ëŠ”:ï¼š]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\'?\d{2,4}\.\s*\d{1,2}\.\s*\d{1,2})',
        r'ë³´ë„\s*ì‹œì \s*[ì€ëŠ”:ï¼š]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\'?\d{2,4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'ë³´ë„\s*ì‹œì \s*[ì€ëŠ”:ï¼š]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\'?\d{2,4}-\d{1,2}-\d{1,2})',
        r'ë³´ë„\s*ì‹œì \s*[ì€ëŠ”:ï¼š]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\'?\d{2,4}/\d{1,2}/\d{1,2})',
        r'ë³´ë„\s*ì‹œì \s*[ì€ëŠ”:ï¼š]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\d{10})',  # 10ìë¦¬ ìˆ«ì
        # "ë³´ë„ê°€ ë°°í¬ ì‹œ" íŒ¨í„´
        r'ë³´ë„\s*[ê°€ì™€]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\d{4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'ë³´ë„\s*[ê°€ì™€]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\d{4}\.\s*\d{1,2}\.\s*\d{1,2})',
        r'ë³´ë„\s*[ê°€ì™€]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\d{4}-\d{1,2}-\d{1,2})',
        r'ë³´ë„\s*[ê°€ì™€]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\d{4}/\d{1,2}/\d{1,2})',
        r'ë³´ë„\s*[ê°€ì™€]\s*ë°°í¬\s*ì‹œ\s*[:ï¼š]?\s*(\d{10})',  # 10ìë¦¬ ìˆ«ì
    ]
    for pattern in complex_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            date_str = match.group(1).strip()
            
            # 10ìë¦¬ ìˆ«ì í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: 25032011 -> 2025-03-20)
            if len(date_str) == 10 and date_str.isdigit():
                year = int(date_str[:2])
                month = int(date_str[2:4])
                day = int(date_str[4:6])
                if year >= 50:
                    full_year = 1900 + year
                else:
                    full_year = 2000 + year
                date_str = f"{full_year}.{month}.{day}"
            
            # '25 ê°™ì€ í˜•ì‹ì„ 2025ë¡œ ë³€í™˜
            date_str = normalize_year_format(date_str)
            date_str = add_year_if_missing(date_str)
            print(f"      â„¹ï¸ ë³µí•© íŒ¨í„´ì—ì„œ ë‚ ì§œ ë°œê²¬: {date_str}")
            return date_str

    # 1ë‹¨ê³„: ë³´ë„ì¼ ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ê²€ìƒ‰ (ë„ì–´ì“°ê¸° ë³€í˜• í¬í•¨)
    # "ë³´ë„ì‹œì " í‚¤ì›Œë“œê°€ ìˆì§€ë§Œ "ë°°í¬ì‹œ"ë¡œ ì—°ê²°ëœ ê²½ìš°ëŠ” ì´ë¯¸ 0ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ë¨
    # "ë³´ ë„" (ë„ì–´ì“°ê¸°) íŒ¨í„´ë„ í¬í•¨
    press_keywords = [
        'ë³´ë„ì‹œì ', 'ë³´ë„ ì‹œì ', 'ë³´ë„ì‹œ ì ', 
        'ë³´ë„ì¼', 'ë³´ë„ ì¼', 
        'ë³´ë„ ì‹œ', 'ë³´ë„ê°€', 
        'ë³´ ë„', 'ë³´  ë„',  # ë„ì–´ì“°ê¸° ë³€í˜•
        'ë³´ë„'
    ]
    
    for keyword in press_keywords:
        date = extract_date_near_keyword(text, keyword)
        if date:
            return date
    
    # 2ë‹¨ê³„: ë³´ë„ì¼ì„ ëª» ì°¾ì•˜ìœ¼ë©´ ë°°í¬ì¼ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰ (ë°°í¬ì‹œ í¬í•¨)
    release_keywords = ['ë°°í¬ì‹œ', 'ë°°í¬ ì‹œ', 'ë°°í¬ì¼', 'ë°°í¬ ì¼', 'ë°°í¬ê°€', 'ë°°í¬']
    
    for keyword in release_keywords:
        date = extract_date_near_keyword(text, keyword)
        if date:
            print(f"      â„¹ï¸ ë³´ë„ì¼ì„ ì°¾ì§€ ëª»í•´ ë°°í¬ì¼ì„ ì‚¬ìš©: {date}")
            return date
    
    # 3ë‹¨ê³„: í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ëª» ì°¾ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
    # 3ë‹¨ê³„: í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ëª» ì°¾ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸° (2ìë¦¬ ë…„ë„ í¬í•¨)
    date_patterns = [
        r'(\'?\d{2,4}\s*ë…„\s*\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)',
        r'(\'?\d{2,4}\.\s*\d{1,2}\.\s*\d{1,2}\s*\(?[ê°€-í£]*\)?)',
        r'(\'?\d{2,4}-\d{1,2}-\d{1,2})',
        r'(\'?\d{2,4}/\d{1,2}/\d{1,2})',
        r'(\d{8})',  # 8ìë¦¬ ìˆ«ì (ì˜ˆ: 20250320)
        r'(\d{10})',  # 10ìë¦¬ ìˆ«ì (ì˜ˆ: 25032011 -> 2025-03-20ìœ¼ë¡œ ë³€í™˜)
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            date_str = match.group(1).strip()
            
            # 10ìë¦¬ ìˆ«ì í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: 25032011 -> 2025-03-20)
            if len(date_str) == 10 and date_str.isdigit():
                year = int(date_str[:2])
                month = int(date_str[2:4])
                day = int(date_str[4:6])
                # ì‹œê°„ ë¶€ë¶„ì€ ë¬´ì‹œ
                if year >= 50:
                    full_year = 1900 + year
                else:
                    full_year = 2000 + year
                date_str = f"{full_year}.{month}.{day}"
            
            # '25 ê°™ì€ í˜•ì‹ì„ 2025ë¡œ ë³€í™˜
            date_str = normalize_year_format(date_str)
            # ë…„ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë…„ë„ ì¶”ê°€
            date_str = add_year_if_missing(date_str)
            return date_str

    return None


# -----------------------------------------------------------
# í…ìŠ¤íŠ¸ì—ì„œ ë³´ë„ì‹œì  ì¶”ì¶œ
# -----------------------------------------------------------
def extract_press_time(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ë³´ë„ì‹œì ì„ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    if not text:
        return None
    
    # ë³´ë„ì‹œì  í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ë‚ ì§œ ì°¾ê¸°
    press_time_keywords = ['ë³´ë„ì‹œì ', 'ë³´ë„ ì‹œì ', 'ë³´ë„ì‹œ ì ']
    
    for keyword in press_time_keywords:
        date = extract_date_near_keyword(text, keyword)
        if date:
            return date
    
    return None


# -----------------------------------------------------------
# ë…„ë„ í˜•ì‹ ì •ê·œí™” ('25 -> 2025)
# -----------------------------------------------------------
def normalize_year_format(date_str):
    """'25.9.3 ê°™ì€ 2ìë¦¬ ë…„ë„ë¥¼ 2025.9.3ë¡œ ë³€í™˜"""
    if not date_str:
        return date_str
    
    # '25 ë˜ëŠ” 25ë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´ ì°¾ê¸°
    # '25.9.3 -> 2025.9.3
    # 25.9.3 -> 2025.9.3 (2ìë¦¬ ë…„ë„ì¸ ê²½ìš°)
    pattern = r'^(\'?)(\d{2})(\.\s*\d{1,2}\.\s*\d{1,2})'
    match = re.match(pattern, date_str)
    if match:
        prefix = match.group(1)  # ' ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
        year = int(match.group(2))  # 25
        rest = match.group(3)  # .9.3
        
        # 2ìë¦¬ ë…„ë„ë¥¼ 4ìë¦¬ë¡œ ë³€í™˜ (50 ì´ìƒì´ë©´ 1900ë…„ëŒ€, ë¯¸ë§Œì´ë©´ 2000ë…„ëŒ€)
        if year >= 50:
            full_year = 1900 + year
        else:
            full_year = 2000 + year
        
        return f"{full_year}{rest}"
    
    # '25ë…„ 9ì›” 3ì¼ í˜•ì‹
    pattern = r'^(\'?)(\d{2})\s*ë…„'
    match = re.match(pattern, date_str)
    if match:
        prefix = match.group(1)
        year = int(match.group(2))
        
        if year >= 50:
            full_year = 1900 + year
        else:
            full_year = 2000 + year
        
        return date_str.replace(f"{prefix}{year}ë…„", f"{full_year}ë…„", 1)
    
    return date_str


# -----------------------------------------------------------
# ë‚ ì§œì— ë…„ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë…„ë„ ì¶”ê°€
# -----------------------------------------------------------
def add_year_if_missing(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì— ë…„ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë…„ë„ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤"""
    if not date_str:
        return date_str
    
    current_year = datetime.now().year
    
    # ë…„ë„ê°€ ìˆëŠ”ì§€ í™•ì¸ (4ìë¦¬ ìˆ«ìë¡œ ì‹œì‘)
    if re.match(r'^\d{4}', date_str):
        return date_str
    
    # ë…„ë„ê°€ ì—†ëŠ” ê²½ìš°: "1ì›” 1ì¼", "1.1", "1-1", "1/1" ë“±
    # í˜„ì¬ ë…„ë„ë¥¼ ì•ì— ì¶”ê°€
    if re.match(r'^\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼', date_str):
        return f"{current_year}ë…„ {date_str}"
    elif re.match(r'^\d{1,2}\.\s*\d{1,2}', date_str):
        return f"{current_year}.{date_str}"
    elif re.match(r'^\d{1,2}-\d{1,2}', date_str):
        return f"{current_year}-{date_str}"
    elif re.match(r'^\d{1,2}/\d{1,2}', date_str):
        return f"{current_year}/{date_str}"
    elif re.match(r'^\d{4}$', date_str):  # 4ìë¦¬ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° (ì›”ì¼)
        return f"{current_year}{date_str}"
    
    return date_str


# -----------------------------------------------------------
# ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
# -----------------------------------------------------------
def parse_date_string(date_str):
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    if not date_str:
        return None
    
    date_str = date_str.strip()
    
    # "2024ë…„ 1ì›” 1ì¼" í˜•ì‹
    match = re.match(r'(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', date_str)
    if match:
        try:
            return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except:
            pass
    
    # "2024. 1. 1" ë˜ëŠ” "2024.1.1" í˜•ì‹
    match = re.match(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})', date_str)
    if match:
        try:
            return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except:
            pass
    
    # "2024-01-01" í˜•ì‹
    match = re.match(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_str)
    if match:
        try:
            return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except:
            pass
    
    # "2024/01/01" í˜•ì‹
    match = re.match(r'(\d{4})/(\d{1,2})/(\d{1,2})', date_str)
    if match:
        try:
            return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except:
            pass
    
    # "20240101" í˜•ì‹
    match = re.match(r'(\d{4})(\d{2})(\d{2})', date_str)
    if match:
        try:
            return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except:
            pass
    
    return None


# -----------------------------------------------------------
# ë‚ ì§œê°€ 2025ë…„ ì´í›„ì¸ì§€ í™•ì¸
# -----------------------------------------------------------
def is_after_2025(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì´ 2025ë…„ ì´í›„ì¸ì§€ í™•ì¸"""
    if not date_str:
        return True  # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨ (ë‚˜ì¤‘ì— í•„í„°ë§)
    
    date_obj = parse_date_string(date_str)
    if not date_obj:
        return True  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í¬í•¨
    
    cutoff_date = datetime(2025, 1, 1)
    return date_obj >= cutoff_date


# -----------------------------------------------------------
# ë‚ ì§œê°€ ì§€ì •ëœ ë‚ ì§œ ì´í›„ì¸ì§€ í™•ì¸
# -----------------------------------------------------------
def is_after_date(date_str, cutoff_date):
    """ë‚ ì§œ ë¬¸ìì—´ì´ ì§€ì •ëœ ë‚ ì§œ ì´í›„ì¸ì§€ í™•ì¸"""
    if not date_str:
        return True  # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨ (ë‚˜ì¤‘ì— í•„í„°ë§)
    
    date_obj = parse_date_string(date_str)
    if not date_obj:
        return True  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í¬í•¨
    
    return date_obj >= cutoff_date


# -----------------------------------------------------------
# ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
# -----------------------------------------------------------
def has_next_page(soup, current_page):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°)
        next_texts = ['ë‹¤ìŒ', '>', 'â–¶', 'next', 'Next']
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            text = link.get_text(strip=True)
            if any(next_text in text for next_text in next_texts):
                href = link.get("href", "").strip()
                if href and "pageIndex=" in href:
                    return True
        
        # .next í´ë˜ìŠ¤ ì°¾ê¸°
        next_links = soup.select(".next, .paging .next, .pagination .next")
        if next_links:
            return True
        
        # í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ì—ì„œ í˜„ì¬ í˜ì´ì§€ë³´ë‹¤ í° ë²ˆí˜¸ ì°¾ê¸°
        pagination_selectors = [
            "div.paging",
            "div.pagination",
            "div.pageArea",
            ".paging",
            ".pagination",
            ".pageArea",
        ]
        
        for selector in pagination_selectors:
            pagination = soup.select_one(selector)
            if pagination:
                page_links = pagination.select("a[href]")
                for link in page_links:
                    text = link.get_text(strip=True)
                    if text.isdigit() and int(text) > current_page:
                        return True
                    
                    href = link.get("href", "").strip()
                    if href and "pageIndex=" in href:
                        match = re.search(r'pageIndex=(\d+)', href)
                        if match:
                            page_num = int(match.group(1))
                            if page_num > current_page:
                                return True
        
        # ì „ì²´ ë§í¬ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ ì°¾ê¸°
        for link in all_links:
            href = link.get("href", "").strip()
            if href and "pageIndex=" in href:
                match = re.search(r'pageIndex=(\d+)', href)
                if match:
                    page_num = int(match.group(1))
                    if page_num > current_page:
                        return True
        
        return False
        
    except Exception as e:
        print(f"    âš ï¸ ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


# -----------------------------------------------------------
# ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë³´ë„ìë£Œ ì¶”ì¶œ
# -----------------------------------------------------------
def scrape_single_page(session, page_url, page_num, total_pages, start_idx=1, cutoff_date=None):
    """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë³´ë„ìë£Œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤
    
    Args:
        cutoff_date: ì´ ë‚ ì§œ ì´í›„ì˜ ë³´ë„ìë£Œë§Œ ìˆ˜ì§‘ (Noneì´ë©´ 2025-01-01 ì‚¬ìš©)
    """
    results = []
    has_recent_data = False  # í˜ì´ì§€ ë‚´ì— ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì—¬ë¶€
    missing_dates_count = 0  # ë³´ë„ì¼ì´ ì—†ëŠ” í•­ëª© ê°œìˆ˜
    
    if cutoff_date is None:
        cutoff_date = datetime(2025, 1, 1)
    
    try:
        response = session.get(page_url, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.find('table', class_='board_list') or soup.find('table')

        if not table:
            print(f"    âš ï¸ í˜ì´ì§€ {page_num}: í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return results, False, False  # (results, should_stop, has_recent_data)

        rows = table.find_all('tr')[1:]
        if not rows:
            print(f"    âš ï¸ í˜ì´ì§€ {page_num}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return results, False, False

        print(f"\nğŸ“„ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘... ({len(rows)}ê°œ í•­ëª©)")

        # ê° ë³´ë„ìë£Œ í–‰ ë°˜ë³µ ì²˜ë¦¬
        for row_idx, row in enumerate(rows, start=start_idx):
            title_link = row.find('a', href=re.compile(r'view\.do'))
            if not title_link:
                continue

            # ì œëª©, ìƒì„¸ URL
            title = title_link.get_text(strip=True)
            detail_url = urljoin(page_url, title_link['href'])

            # ë‹´ë‹¹ë¶€ì„œ
            tds = row.find_all('td')
            department = tds[2].get_text(strip=True) if len(tds) >= 3 else None

            # ì²¨ë¶€íŒŒì¼ (.hwp, .pdf, ë“±) - ë³„ì²¨íŒŒì¼ ì œì™¸
            file_links = []
            attach_links = row.find_all('a', href=re.compile(r'fileDown\.do'))

            for link in attach_links:
                href = urljoin(page_url, link['href'])
                file_name = link.get_text(strip=True)
                
                # ë³„ì²¨íŒŒì¼ ì œì™¸
                if 'ë³„ì²¨' in file_name or 'ë³„ ì²¨' in file_name:
                    continue

                file_links.append({
                    'ì²¨ë¶€íŒŒì¼ëª…': file_name,
                    'ì²¨ë¶€íŒŒì¼ url': href
                })

            print(f"  [{row_idx}] {title}")
            if not file_links:
                print("      âš ï¸ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")

            # ì‹ ë…„ì‚¬ ì œì™¸
            if 'ì‹ ë…„ì‚¬' in title:
                print(f"      â¹ï¸ ì‹ ë…„ì‚¬ í•­ëª©ì€ ì œì™¸í•©ë‹ˆë‹¤.")
                continue
            
            # ìƒì„¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ë° ë“±ë¡ì¼ ì¶”ì¶œ
            registration_date = None
            try:
                detail_response = session.get(detail_url, timeout=30)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                content_div = detail_soup.find('div', class_='dbdata')
                content = content_div.get_text(separator='\n', strip=True) if content_div else ''
                content = re.sub(r'\n+', '\n', content.strip())
                
                # ë“±ë¡ì¼ ì¶”ì¶œ (ë³´ë„ìë£Œê°€ ì•„ë‹Œ ê²½ìš° ì‚¬ìš©)
                # ë“±ë¡ì¼ì€ ë³´í†µ ìƒì„¸í˜ì´ì§€ì˜ ë©”íƒ€ ì •ë³´ì— ìˆìŒ
                # ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
                reg_patterns = [
                    r'ë“±ë¡ì¼[:\s]*(\d{4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})',
                    r'ì‘ì„±ì¼[:\s]*(\d{4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})',
                    r'ë“±ë¡[:\s]*(\d{4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})',
                    r'(\d{4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})',  # ì¼ë°˜ì ì¸ ë‚ ì§œ íŒ¨í„´
                ]
                
                # ìƒì„¸í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë“±ë¡ì¼ ì°¾ê¸°
                page_text = detail_soup.get_text()
                for pattern in reg_patterns:
                    date_match = re.search(pattern, page_text)
                    if date_match:
                        registration_date = date_match.group(1)
                        # ìœ íš¨í•œ ë‚ ì§œì¸ì§€ í™•ì¸ (2025ë…„ ì´í›„)
                        date_obj = parse_date_string(registration_date)
                        if date_obj and date_obj >= datetime(2025, 1, 1):
                            break
                        elif date_obj:
                            registration_date = None  # 2025ë…„ ì´ì „ì´ë©´ ë¬´ì‹œ
                
            except Exception as e:
                print(f"      âš ï¸ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                content = ''

            # ë³´ë„ìë£Œ íŒŒì¼ì—ì„œ ë³´ë„ì¼ ì¶”ì¶œ (ë³„ì²¨íŒŒì¼ ì œì™¸)
            date = None
            text_preview = None
            full_text = None  # ì „ì²´ í…ìŠ¤íŠ¸ (ë³´ë„ì‹œì  ì¶”ì¶œìš©)

            # ë³„ì²¨íŒŒì¼ ì œì™¸í•˜ê³  ë³´ë„ìë£Œ íŒŒì¼ë§Œ í•„í„°ë§
            press_files = [
                f for f in file_links 
                if 'ë³„ì²¨' not in f['ì²¨ë¶€íŒŒì¼ëª…'] and 'ë³„ ì²¨' not in f['ì²¨ë¶€íŒŒì¼ëª…']
            ]
            
            # HWP íŒŒì¼ ë¨¼ì € ì‹œë„
            hwp_files = [
                f for f in press_files 
                if f['ì²¨ë¶€íŒŒì¼ëª…'].lower().endswith('.hwp')
            ]
            
            for f in hwp_files:
                try:
                    print(f"      ğŸ“‚ HWP ë‹¤ìš´ë¡œë“œ ì¤‘: {f['ì²¨ë¶€íŒŒì¼ëª…']}")
                    file_response = session.get(f['ì²¨ë¶€íŒŒì¼ url'], timeout=30)
                    file_response.raise_for_status()

                    text = extract_text_from_hwp_bytes(file_response.content)
                    if text:
                        if not full_text:
                            full_text = text
                        if not text_preview:
                            text_preview = text[:200]
                        found_date = extract_first_date(text)
                        print(f"      ğŸ“… ë³´ë„ì¼: {found_date or 'ì¶”ì¶œ ì‹¤íŒ¨'}")
                        
                        if found_date:
                            # ë‚ ì§œê°€ ê¸°ì¤€ì¼ ì´í›„ì¸ì§€ í™•ì¸
                            if is_after_date(found_date, cutoff_date):
                                date = found_date
                                print(f"      âœ… ê¸°ì¤€ì¼ ì´í›„ ë³´ë„ì¼ í™•ì¸: {date}")
                                break  # ê¸°ì¤€ì¼ ì´í›„ ë‚ ì§œë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                            else:
                                print(f"      âš ï¸ ë³´ë„ì¼ì´ ê¸°ì¤€ì¼ ì´ì „ì…ë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                                # ê¸°ì¤€ì¼ ì´ì „ì´ë©´ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•˜ê¸° ìœ„í•´ continue

                except Exception as e:
                    print(f"      âš ï¸ HWP ì²˜ë¦¬ ì‹¤íŒ¨ ({f['ì²¨ë¶€íŒŒì¼ëª…']}): {e}")
            
            # HWPì—ì„œ 1ë…„ ì´ë‚´ ë‚ ì§œë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ HWPX íŒŒì¼ ì‹œë„
            if not date:
                hwpx_files = [
                    f for f in press_files 
                    if f['ì²¨ë¶€íŒŒì¼ëª…'].lower().endswith('.hwpx')
                ]
                
                for f in hwpx_files:
                    try:
                        print(f"      ğŸ“‚ HWPX ë‹¤ìš´ë¡œë“œ ì¤‘: {f['ì²¨ë¶€íŒŒì¼ëª…']}")
                        file_response = session.get(f['ì²¨ë¶€íŒŒì¼ url'], timeout=30)
                        file_response.raise_for_status()

                        text = extract_text_from_hwpx_bytes(file_response.content)
                        if text:
                            if not full_text:
                                full_text = text
                            if not text_preview:
                                text_preview = text[:200]
                            found_date = extract_first_date(text)
                            print(f"      ğŸ“… ë³´ë„ì¼ (HWPX): {found_date or 'ì¶”ì¶œ ì‹¤íŒ¨'}")
                            
                            if found_date:
                                # ë‚ ì§œê°€ ê¸°ì¤€ì¼ ì´í›„ì¸ì§€ í™•ì¸
                                if is_after_date(found_date, cutoff_date):
                                    date = found_date
                                    print(f"      âœ… ê¸°ì¤€ì¼ ì´í›„ ë³´ë„ì¼ í™•ì¸: {date}")
                                    break  # ê¸°ì¤€ì¼ ì´í›„ ë‚ ì§œë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                                else:
                                    print(f"      âš ï¸ ë³´ë„ì¼ì´ ê¸°ì¤€ì¼ ì´ì „ì…ë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                                    # ê¸°ì¤€ì¼ ì´ì „ì´ë©´ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•˜ê¸° ìœ„í•´ continue

                    except Exception as e:
                        print(f"      âš ï¸ HWPX ì²˜ë¦¬ ì‹¤íŒ¨ ({f['ì²¨ë¶€íŒŒì¼ëª…']}): {e}")
            
            # HWP/HWPXì—ì„œ 1ë…„ ì´ë‚´ ë‚ ì§œë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ PDF íŒŒì¼ ì‹œë„ (ë³„ì²¨ ì œì™¸)
            if not date:
                pdf_files = [
                    f for f in press_files 
                    if f['ì²¨ë¶€íŒŒì¼ëª…'].lower().endswith('.pdf')
                ]
                
                for f in pdf_files:
                    try:
                        print(f"      ğŸ“‚ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {f['ì²¨ë¶€íŒŒì¼ëª…']}")
                        file_response = session.get(f['ì²¨ë¶€íŒŒì¼ url'], timeout=30)
                        file_response.raise_for_status()

                        text = extract_text_from_pdf_bytes(file_response.content)
                        if text:
                            if not full_text:
                                full_text = text
                            if not text_preview:
                                text_preview = text[:200]
                            found_date = extract_first_date(text)
                            print(f"      ğŸ“… ë³´ë„ì¼ (PDF): {found_date or 'ì¶”ì¶œ ì‹¤íŒ¨'}")
                            
                            if found_date:
                                # ë‚ ì§œê°€ ê¸°ì¤€ì¼ ì´í›„ì¸ì§€ í™•ì¸
                                if is_after_date(found_date, cutoff_date):
                                    date = found_date
                                    print(f"      âœ… ê¸°ì¤€ì¼ ì´í›„ ë³´ë„ì¼ í™•ì¸: {date}")
                                    break  # ê¸°ì¤€ì¼ ì´í›„ ë‚ ì§œë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                                else:
                                    print(f"      âš ï¸ ë³´ë„ì¼ì´ ê¸°ì¤€ì¼ ì´ì „ì…ë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                                    # ê¸°ì¤€ì¼ ì´ì „ì´ë©´ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•˜ê¸° ìœ„í•´ continue

                    except Exception as e:
                        print(f"      âš ï¸ PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({f['ì²¨ë¶€íŒŒì¼ëª…']}): {e}")

            # ë³´ë„ì‹œì  ì¶”ì¶œ (ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©)
            press_time = None
            if full_text:
                press_time = extract_press_time(full_text)
            elif text_preview:
                press_time = extract_press_time(text_preview)
            
            # ë³´ë„ì¼ì´ ì—†ê³  ë³´ë„ì‹œì ì´ ìˆìœ¼ë©´ ë³´ë„ì‹œì ì„ ë³´ë„ì¼ë¡œ ì‚¬ìš©
            if not date and press_time:
                date = press_time
                print(f"      â„¹ï¸ ë³´ë„ì¼ì´ ì—†ì–´ ë³´ë„ì‹œì ì„ ë³´ë„ì¼ë¡œ ì‚¬ìš©: {date}")
            
            # ë³´ë„ìë£Œê°€ ì•„ë‹Œ ê²½ìš°(ë³´ë„ì°¸ê³ , ë‹¹ë¶€ì‚¬í•­ ë“±) ë“±ë¡ì¼ì„ ë³´ë„ì¼ë¡œ ì‚¬ìš©
            is_press_release = 'ë³´ë„ìë£Œ' in title or 'ë³´ë„' in title
            if not date and registration_date and not is_press_release:
                date = registration_date
                print(f"      â„¹ï¸ ë³´ë„ìë£Œê°€ ì•„ë‹ˆì–´ì„œ ë“±ë¡ì¼ì„ ë³´ë„ì¼ë¡œ ì‚¬ìš©: {date}")
            
            # ë³´ë„ì¼ ê¸°ì¤€ìœ¼ë¡œë§Œ íŒë‹¨ (ë“±ë¡ì¼ì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
            # ë³´ë„ì¼ì´ ìˆê³  ê¸°ì¤€ì¼ ì´ì „ì´ë©´ ìŠ¤í¬ë© ì¤‘ë‹¨
            if date and not is_after_date(date, cutoff_date):
                print(f"      â¹ï¸ ë³´ë„ì¼({date})ì´ ê¸°ì¤€ì¼({cutoff_date.strftime('%Y-%m-%d')}) ì´ì „ì…ë‹ˆë‹¤. ìŠ¤í¬ë©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return results, True, False, missing_dates_count  # (results, should_stop, has_recent_data, missing_dates_count)
            
            # ë³´ë„ì¼ ê¸°ì¤€ìœ¼ë¡œ í¬í•¨ ì—¬ë¶€ ê²°ì •
            should_include = False
            if date and is_after_date(date, cutoff_date):
                should_include = True
            elif not date:
                # ë³´ë„ì¼ì´ ì—†ì–´ë„ ì¼ë‹¨ í¬í•¨ (ë‚˜ì¤‘ì— í•„í„°ë§)
                should_include = True
            
            # ê²°ê³¼ ì €ì¥ (ë³´ë„ì‹œì ê³¼ ë“±ë¡ì¼ì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©, ì €ì¥í•˜ì§€ ì•ŠìŒ)
            if should_include:
                results.append({
                    'ë²ˆí˜¸': row_idx,
                    'ì œëª©': title,
                    'ë‹´ë‹¹ë¶€ì„œ': department,
                    'ë³´ë„ì¼': date,
                    'ì²¨ë¶€íŒŒì¼': file_links,
                    'ì²¨ë¶€íŒŒì¼ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°': text_preview,
                    'ìƒì„¸í˜ì´ì§€URL': detail_url,
                    'ë‚´ìš©': content
                })
                # ë³´ë„ì¼ì´ ì—†ìœ¼ë©´ ì¹´ìš´íŠ¸ ì¦ê°€
                if not date:
                    missing_dates_count += 1
                # ê¸°ì¤€ì¼ ì´í›„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ (ë³´ë„ì¼ ê¸°ì¤€)
                if date and is_after_date(date, cutoff_date):
                    has_recent_data = True

            time.sleep(0.5)

    except Exception as e:
        print(f"    âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # í˜ì´ì§€ ë‚´ì— ê¸°ì¤€ì¼ ì´í›„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨ ì‹ í˜¸ ë°˜í™˜
    should_stop = not has_recent_data and len(results) == 0
    
    return results, should_stop, has_recent_data, missing_dates_count  # (results, should_stop, has_recent_data, missing_dates_count)


# -----------------------------------------------------------
# ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
# -----------------------------------------------------------
def load_existing_data(json_file="results.json"):
    """ê¸°ì¡´ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(data)}ê±´ ë°œê²¬")
            return data
    except FileNotFoundError:
        print("ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ (ì²˜ìŒë¶€í„° ì‹œì‘)")
        return []
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e} (ì²˜ìŒë¶€í„° ì‹œì‘)")
        return []


# -----------------------------------------------------------
# ë³´ë„ìë£Œ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ëª¨ë“  í˜ì´ì§€)
# -----------------------------------------------------------
def scrape_press_releases(base_url, total_pages=2010, resume=True):
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})

    results = []
    
    # ê¸°ì¤€ì¼ ì„¤ì •: 2025ë…„ 1ì›” 1ì¼
    default_cutoff_date = datetime(2025, 1, 1)
    cutoff_date = default_cutoff_date
    
    try:
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° ê¸°ì¤€ì¼ ì„¤ì •
        all_results = []
        item_counter = 1
        start_page = 1
        
        if resume:
            existing_data = load_existing_data("results.json")
            if existing_data:
                all_results = existing_data
                item_counter = len(existing_data) + 1
                
                # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ê°€ì¥ ìµœì‹  ë³´ë„ì¼ ì°¾ê¸°
                latest_date = None
                for item in existing_data:
                    date_str = item.get('ë³´ë„ì¼')
                    if date_str:
                        date_obj = parse_date_string(date_str)
                        if date_obj:
                            if latest_date is None or date_obj > latest_date:
                                latest_date = date_obj
                
                if latest_date:
                    # ê°€ì¥ ìµœì‹  ë³´ë„ì¼ ì´í›„ì˜ ë°ì´í„°ë§Œ ìˆ˜ì§‘
                    cutoff_date = latest_date
                    print(f"ğŸ“… ê¸°ì¡´ ë°ì´í„°ì—ì„œ ê°€ì¥ ìµœì‹  ë³´ë„ì¼: {latest_date.strftime('%Y-%m-%d')}")
                    print(f"ğŸ“… ì´ ë‚ ì§œ ì´í›„ì˜ ì‹ ê·œ ë³´ë„ìë£Œë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
                else:
                    # ë³´ë„ì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê¸°ì¤€ì¼ ì‚¬ìš©
                    cutoff_date = default_cutoff_date
                    print(f"ğŸ“… ê¸°ì¡´ ë°ì´í„°ì— ë³´ë„ì¼ì´ ì—†ì–´ ê¸°ë³¸ ê¸°ì¤€ì¼({default_cutoff_date.strftime('%Y-%m-%d')})ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                print(f"ğŸ”„ ì´ì–´ì„œ ì§„í–‰: {len(existing_data)}ê±´ ì €ì¥ë¨, ì‹ ê·œ ë°ì´í„° ì¶”ê°€ ì¤‘...\n")
                import sys
                sys.stdout.flush()
            else:
                print(f"ğŸ“… ìµœì´ˆ ì‹¤í–‰: {default_cutoff_date.strftime('%Y-%m-%d')} ì´í›„ ë³´ë„ìë£Œë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n")
        else:
            print(f"ğŸ“… ìµœì´ˆ ì‹¤í–‰: {default_cutoff_date.strftime('%Y-%m-%d')} ì´í›„ ë³´ë„ìë£Œë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n")
        
        print("ğŸ“¢ ë³´ë„ìë£Œ ëª©ë¡ ì²˜ë¦¬ ì¤‘...")
        print("=" * 70)
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ì¤€ì¼: {cutoff_date.strftime('%Y-%m-%d')} ì´í›„")
        print("=" * 70)
        
        # URLì—ì„œ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        parsed = urlparse(base_url)
        params = parse_qs(parsed.query)
        
        page_num = start_page
        save_interval = 10  # 10ê°œì”© ì¤‘ê°„ ì €ì¥
        
        start_time = time.time()
        total_missing_dates = 0  # ì „ì²´ ëˆ„ë½ëœ ë³´ë„ì¼ ê°œìˆ˜
        
        # ì‹œê°„ í¬ë§·íŒ… í•¨ìˆ˜
        def format_time(seconds):
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            if hours > 0:
                return f"{hours}ì‹œê°„ {minutes}ë¶„ {secs}ì´ˆ"
            elif minutes > 0:
                return f"{minutes}ë¶„ {secs}ì´ˆ"
            else:
                return f"{secs}ì´ˆ"
        
        while True:  # 1ë…„ ì´ë‚´ ë°ì´í„°ê°€ ì—†ì„ ë•Œê¹Œì§€ ê³„ì†
            # í˜ì´ì§€ URL ìƒì„±
            params['pageIndex'] = [str(page_num)]
            new_query = urlencode(params, doseq=True)
            page_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))
            
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            try:
                response = session.get(page_url, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # í…Œì´ë¸” í™•ì¸
                table = soup.find('table', class_='board_list') or soup.find('table')
                if not table:
                    print(f"  âš ï¸ í˜ì´ì§€ {page_num}: í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                rows = table.find_all('tr')[1:]
                if not rows:
                    print(f"  âš ï¸ í˜ì´ì§€ {page_num}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
                page_results, should_stop, has_recent_data, missing_dates = scrape_single_page(session, page_url, page_num, 0, start_idx=item_counter, cutoff_date=cutoff_date)
                
                if should_stop:
                    print(f"  â¹ï¸ í˜ì´ì§€ {page_num}ì— ê¸°ì¤€ì¼ ì´í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                if not page_results:
                    print(f"  âš ï¸ í˜ì´ì§€ {page_num}: ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ê¸°ì¡´ ë°ì´í„°ì™€ ì¤‘ë³µ í™•ì¸ (ìƒì„¸í˜ì´ì§€URL ê¸°ì¤€)
                existing_urls = {item.get('ìƒì„¸í˜ì´ì§€URL') for item in all_results}
                new_results = []
                for result in page_results:
                    # ì¤‘ë³µì´ ì•„ë‹ˆê³  ê¸°ì¤€ì¼ ì´í›„ì¸ ê²½ìš°ë§Œ ì¶”ê°€
                    if result.get('ìƒì„¸í˜ì´ì§€URL') not in existing_urls:
                        result['ë²ˆí˜¸'] = item_counter
                        item_counter += 1
                        new_results.append(result)
                
                all_results.extend(new_results)
                total_missing_dates += missing_dates
                
                if len(new_results) < len(page_results):
                    print(f"  â„¹ï¸ ì¤‘ë³µ í•­ëª© {len(page_results) - len(new_results)}ê°œ ì œì™¸ë¨")
                
                # ì§„í–‰ë¥  ê³„ì‚°
                elapsed_time = time.time() - start_time
                avg_time_per_item = elapsed_time / len(all_results) if len(all_results) > 0 else 0
                
                # ë³´ë„ì¼ ì¶”ì¶œ ì„±ê³µë¥  ê³„ì‚°
                total_with_dates = len(all_results) - total_missing_dates
                success_rate = (total_with_dates / len(all_results) * 100) if len(all_results) > 0 else 0
                
                print(f"  âœ… í˜ì´ì§€ {page_num} ì™„ë£Œ | "
                      f"ì¶”ì¶œ: {len(page_results)}ê°œ | ì‹ ê·œ: {len(new_results)}ê°œ | ëˆ„ì : {len(all_results)}ê°œ | "
                      f"ë³´ë„ì¼ ëˆ„ë½: {total_missing_dates}ê°œ ({100-success_rate:.1f}%) | "
                      f"ê²½ê³¼: {format_time(elapsed_time)} | "
                      f"í‰ê· : {avg_time_per_item:.1f}ì´ˆ/ê±´")
                import sys
                sys.stdout.flush()  # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ ë²„í¼ í”ŒëŸ¬ì‹œ
                
                # ì¤‘ê°„ ì €ì¥ (10ê°œì”©)
                if len(all_results) % save_interval == 0:
                    print(f"\n  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì¤‘... (ëˆ„ì  {len(all_results)}ê°œ)")
                    save_results(all_results, 
                               csv_file="results.csv", 
                               excel_file="results.xlsx", 
                               json_file="results.json")
                    print(f"  âœ… ì¤‘ê°„ ì €ì¥ ì™„ë£Œ\n")
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
                if not has_next_page(soup, page_num):
                    print(f"\n  âš ï¸ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...\n")
                
                page_num += 1
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ê°„ ì €ì¥
                if all_results:
                    print(f"\n  ğŸ’¾ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¤‘ê°„ ì €ì¥ ì¤‘...")
                    save_results(all_results, 
                               csv_file="results.csv", 
                               excel_file="results.xlsx", 
                               json_file="results.json")
                # ì—°ì† ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¢…ë£Œ
                break
        
        results = all_results
        total_time = time.time() - start_time
        
        # ìµœì¢… í†µê³„
        total_with_dates = len(all_results) - total_missing_dates
        success_rate = (total_with_dates / len(all_results) * 100) if len(all_results) > 0 else 0
        
        print(f"\nğŸ“Š ìŠ¤í¬ë© ì™„ë£Œ")
        print(f"  - ì²˜ë¦¬ í˜ì´ì§€: {page_num}í˜ì´ì§€")
        print(f"  - ì´ ìˆ˜ì§‘ ë°ì´í„°: {len(all_results)}ê°œ")
        print(f"  - ë³´ë„ì¼ ì¶”ì¶œ ì„±ê³µ: {total_with_dates}ê°œ ({success_rate:.1f}%)")
        print(f"  - ë³´ë„ì¼ ëˆ„ë½: {total_missing_dates}ê°œ ({100-success_rate:.1f}%)")
        print(f"  - ì†Œìš” ì‹œê°„: {format_time(total_time)}")
        if len(all_results) > 0:
            print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(all_results):.1f}ì´ˆ/ê±´")

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì¤‘ê°„ ì €ì¥
        if 'all_results' in locals() and all_results:
            print(f"\n  ğŸ’¾ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¤‘ê°„ ì €ì¥ ì¤‘...")
            save_results(all_results, 
                       csv_file="results.csv", 
                       excel_file="results.xlsx", 
                       json_file="results.json")

    return results


# -----------------------------------------------------------
# CSV / Excel / JSON ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------
def save_results(results, csv_file="results.csv", excel_file="results.xlsx", json_file="results.json"):
    if not results:
        print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(results)

    # ì²¨ë¶€íŒŒì¼ ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜
    df['ì²¨ë¶€íŒŒì¼'] = df['ì²¨ë¶€íŒŒì¼'].apply(
        lambda lst: ', '.join([f"{f['ì²¨ë¶€íŒŒì¼ëª…']} ({f['ì²¨ë¶€íŒŒì¼ url']})" for f in lst]) if lst else ''
    )

    df.fillna('', inplace=True)

    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ (ë³´ë„ì‹œì ê³¼ ë“±ë¡ì¼ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ)
    columns = ['ë²ˆí˜¸', 'ì œëª©', 'ë³´ë„ì¼', 'ìƒì„¸í˜ì´ì§€URL', 'ì²¨ë¶€íŒŒì¼', 'ë‹´ë‹¹ë¶€ì„œ', 'ë‚´ìš©']
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_columns = [col for col in columns if col in df.columns]
    df = df[available_columns]

    # CSV ì €ì¥
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ CSV ì €ì¥ ì™„ë£Œ: {csv_file}")

    # Excel ì €ì¥
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ë³´ë„ìë£Œ')
        ws = writer.sheets['ë³´ë„ìë£Œ']

        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
        for i, col in enumerate(df.columns, start=1):
            max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
            ws.column_dimensions[get_column_letter(i)].width = min(max_len, 50)

    print(f"ğŸ“˜ Excel ì €ì¥ ì™„ë£Œ: {excel_file}")

    # JSON ì €ì¥ (ë³´ë„ì‹œì ê³¼ ë“±ë¡ì¼ ì œê±°)
    # ë³´ë„ì‹œì ê³¼ ë“±ë¡ì¼ì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
    results_for_json = []
    for item in results:
        item_copy = {k: v for k, v in item.items() if k not in ['ë³´ë„ì‹œì ', 'ë“±ë¡ì¼']}
        results_for_json.append(item_copy)
    
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(results_for_json, f, ensure_ascii=False, indent=2)

    print(f"ğŸ§¾ JSON ì €ì¥ ì™„ë£Œ: {json_file}")


# -----------------------------------------------------------
# ë¬¸ì œê°€ ìˆëŠ” í•­ëª© ë¦¬ìŠ¤íŠ¸ì—… (ë³´ë„ì¼ ì—†ìŒ ë˜ëŠ” ë³´ë„ì‹œì ê³¼ 1ì£¼ ì´ìƒ ì°¨ì´)
# -----------------------------------------------------------
def list_problematic_items(results):
    """ë³´ë„ì¼ì´ ì—†ê±°ë‚˜ 2025ë…„ 1ì›” 1ì¼ ì´ì „ì¸ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤
    
    ë³´ë„ì¼ì´ 2025ë…„ 1ì›” 1ì¼ ì´í›„ë©´ ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤.
    ë³´ë„ì¼ì´ ê°€ì¥ ë¨¼ì € ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.
    """
    problematic_items = []
    cutoff_date = datetime(2025, 1, 1)
    
    for item in results:
        press_date_str = item.get('ë³´ë„ì¼')
        
        # ë³´ë„ì¼ì´ ì—†ëŠ” ê²½ìš°
        if not press_date_str:
            problematic_items.append({
                **item,
                'ë¬¸ì œìœ í˜•': 'ë³´ë„ì¼ ì—†ìŒ'
            })
            continue
        
        # ë³´ë„ì¼ íŒŒì‹±
        press_date = parse_date_string(press_date_str)
        
        if not press_date:
            # ë³´ë„ì¼ íŒŒì‹± ì‹¤íŒ¨
            problematic_items.append({
                **item,
                'ë¬¸ì œìœ í˜•': 'ë³´ë„ì¼ íŒŒì‹± ì‹¤íŒ¨',
                'ì›ë³¸ë³´ë„ì¼': press_date_str
            })
            continue
        
        # ë³´ë„ì¼ì´ 2025ë…„ 1ì›” 1ì¼ ì´ì „ì¸ ê²½ìš°ë§Œ ë¬¸ì œ í•­ëª©
        if press_date < cutoff_date:
            problematic_items.append({
                **item,
                'ë¬¸ì œìœ í˜•': f'ë³´ë„ì¼ì´ 2025ë…„ ì´ì „ ({press_date.strftime("%Y-%m-%d")})',
                'íŒŒì‹±ëœë³´ë„ì¼': press_date.strftime('%Y-%m-%d')
            })
    
    return problematic_items


# -----------------------------------------------------------
# ì‹¤í–‰ ë©”ì¸
# -----------------------------------------------------------
def main():
    import sys
    # ì¶œë ¥ ë²„í¼ë§ ë¹„í™œì„±í™” (ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´)
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    
    base_url = "https://www.fss.or.kr/fss/bbs/B0000188/list.do?menuNo=200218&pageIndex=1"
    total_pages = 2010  # ì´ í˜ì´ì§€ ìˆ˜
    print("ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (2025ë…„ ì´í›„ ë°ì´í„°)")
    print("=" * 70)
    sys.stdout.flush()

    results = scrape_press_releases(base_url, total_pages=total_pages, resume=True)

    print("=" * 70)
    print(f"ì´ {len(results)}ê°œ ë³´ë„ìë£Œ ì²˜ë¦¬ ì™„ë£Œ")

    success = sum(1 for r in results if r.get('ë³´ë„ì¼'))
    if results:
        print(f"ë³´ë„ì¼ ì¶”ì¶œ ì„±ê³µë¥ : {success}/{len(results)} ({success/len(results)*100:.1f}%)")

    # ìµœì¢… ì €ì¥
    save_results(results)
    
    # ë¬¸ì œê°€ ìˆëŠ” í•­ëª© ë¦¬ìŠ¤íŠ¸ì—…
    print("\n" + "=" * 70)
    print("ğŸ” ë¬¸ì œê°€ ìˆëŠ” í•­ëª© ë¶„ì„ ì¤‘...")
    problematic_items = list_problematic_items(results)
    
    if problematic_items:
        print(f"âš ï¸ ë¬¸ì œê°€ ìˆëŠ” í•­ëª©: {len(problematic_items)}ê°œ ë°œê²¬")
        
        # ë¬¸ì œ í•­ëª© ì €ì¥
        problem_df = pd.DataFrame(problematic_items)
        problem_df['ì²¨ë¶€íŒŒì¼'] = problem_df['ì²¨ë¶€íŒŒì¼'].apply(
            lambda lst: ', '.join([f"{f['ì²¨ë¶€íŒŒì¼ëª…']} ({f['ì²¨ë¶€íŒŒì¼ url']})" for f in lst]) if lst else ''
        )
        problem_df.fillna('', inplace=True)
        
        # CSV ì €ì¥
        problem_df.to_csv('problematic_items.csv', index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ ë¬¸ì œ í•­ëª© CSV ì €ì¥ ì™„ë£Œ: problematic_items.csv")
        
        # Excel ì €ì¥
        with pd.ExcelWriter('problematic_items.xlsx', engine='openpyxl') as writer:
            problem_df.to_excel(writer, index=False, sheet_name='ë¬¸ì œí•­ëª©')
            ws = writer.sheets['ë¬¸ì œí•­ëª©']
            for i, col in enumerate(problem_df.columns, start=1):
                max_len = max(problem_df[col].astype(str).map(len).max(), len(col)) + 2
                ws.column_dimensions[get_column_letter(i)].width = min(max_len, 50)
        print(f"ğŸ“˜ ë¬¸ì œ í•­ëª© Excel ì €ì¥ ì™„ë£Œ: problematic_items.xlsx")
        
        # JSON ì €ì¥
        with open('problematic_items.json', 'w', encoding='utf-8') as f:
            json.dump(problematic_items, f, ensure_ascii=False, indent=2)
        print(f"ğŸ§¾ ë¬¸ì œ í•­ëª© JSON ì €ì¥ ì™„ë£Œ: problematic_items.json")
        
        # ë¬¸ì œ ìœ í˜•ë³„ í†µê³„
        problem_types = {}
        for item in problematic_items:
            ptype = item.get('ë¬¸ì œìœ í˜•', 'ì•Œ ìˆ˜ ì—†ìŒ')
            problem_types[ptype] = problem_types.get(ptype, 0) + 1
        
        print("\në¬¸ì œ ìœ í˜•ë³„ í†µê³„:")
        for ptype, count in problem_types.items():
            print(f"  - {ptype}: {count}ê°œ")
    else:
        print("âœ… ë¬¸ì œê°€ ìˆëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")


# -------------------------------------------------
# Health Check ëª¨ë“œ
# -------------------------------------------------
from typing import Dict
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re

def fss_press_releases_health_check() -> Dict:
    """
    ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ Health Check
    - ëª©ë¡ 1ê±´ ì¶”ì¶œ
    - ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ í™•ì¸
    """

    BASE_URL = "https://www.fss.or.kr"
    LIST_URL = "https://www.fss.or.kr/fss/bbs/B0000188/list.do?menuNo=200218&pageIndex=1"

    result = {
        "org_name": "FSS",
        "target": "ê¸ˆìœµê°ë…ì› > ë³´ë„ìë£Œ",
        "check_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "status": "FAIL",
        "checks": {
            "list_page": {},
            "detail_page": {}
        },
        "error": None
    }

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (HealthCheck)"
    })

    try:
        # ===============================
        # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼
        # ===============================
        resp = session.get(LIST_URL, timeout=15)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        table = soup.find("table", class_="board_list") or soup.find("table")

        if not table:
            result["checks"]["list_page"] = {
                "url": LIST_URL,
                "success": False,
                "count": 0,
                "message": "ëª©ë¡ í…Œì´ë¸” ì—†ìŒ"
            }
            return result

        rows = table.find_all("tr")[1:]
        if not rows:
            result["checks"]["list_page"] = {
                "url": LIST_URL,
                "success": False,
                "count": 0,
                "message": "ëª©ë¡ ë°ì´í„° ì—†ìŒ"
            }
            return result

        # ===============================
        # 2. ëª©ë¡ 1ê±´ ì¶”ì¶œ
        # ===============================
        first_row = rows[0]
        title_link = first_row.find("a", href=re.compile(r"view\.do"))

        if not title_link:
            result["checks"]["list_page"] = {
                "url": LIST_URL,
                "success": False,
                "count": 0,
                "message": "ìƒì„¸ ë§í¬ ì—†ìŒ"
            }
            return result

        title = title_link.get_text(strip=True)
        detail_url = urljoin(LIST_URL, title_link["href"])

        result["checks"]["list_page"] = {
            "url": LIST_URL,
            "success": True,
            "count": 1,
            "title": title
        }

        # ===============================
        # 3. ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
        # ===============================
        detail_resp = session.get(detail_url, timeout=15)
        detail_resp.raise_for_status()

        detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
        content_div = detail_soup.find("div", class_="dbdata")

        content_length = len(content_div.get_text(strip=True)) if content_div else 0

        result["checks"]["detail_page"] = {
            "url": detail_url,
            "success": True,
            "content_length": content_length
        }

        result["status"] = "OK"
        return result

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "FAIL"
        return result

if __name__ == "__main__":
    import json
    import argparse
    import sys

    parser = argparse.ArgumentParser(description='FSS ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼')
    parser.add_argument("--check", action="store_true", help="FSS ë³´ë„ìë£Œ Health Check ì‹¤í–‰")

    args = parser.parse_args()

    # -------------------------------------------------
    # Health Check ëª¨ë“œ
    # python scrape_fss_press_releases_v2.py --check
    # -------------------------------------------------
    if args.check:
        health = fss_press_releases_health_check()
        print(json.dumps(health, ensure_ascii=False, indent=2))
        sys.exit(0)

    main()
