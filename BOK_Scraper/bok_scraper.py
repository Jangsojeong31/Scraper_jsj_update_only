"""
í•œêµ­ì€í–‰ ìŠ¤í¬ë˜í¼
CSV ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ ë²•ê·œ ì •ë³´ ìŠ¤í¬ë˜í•‘
"""
from __future__ import annotations

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (common ëª¨ë“ˆ importë¥¼ ìœ„í•´)
def find_project_root():
    """common ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™"""
    try:
        # __file__ì´ ìˆëŠ” ê²½ìš° (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
        current = Path(__file__).resolve().parent
    except NameError:
        # __file__ì´ ì—†ëŠ” ê²½ìš° (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)
        current = Path.cwd()
    
    # common ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ë¡œ ì´ë™
    while current != current.parent:
        if (current / 'common').exists() and (current / 'common' / 'base_scraper.py').exists():
            return current
        current = current.parent
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ë°˜í™˜
    return Path.cwd()

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import os
import json
import csv
import time
import re
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from common.base_scraper import BaseScraper
from common.file_extractor import FileExtractor
from common.file_comparator import FileComparator


class BokScraper(BaseScraper):
    """í•œêµ­ì€í–‰ ìŠ¤í¬ë˜í¼ - CSV ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ ë²•ê·œ ì •ë³´ ìˆ˜ì§‘"""
    
    BASE_URL = "https://www.bok.or.kr"
    # ê²€ìƒ‰ URL í…œí”Œë¦¿ (ê²€ìƒ‰ì–´ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ)
    SEARCH_URL_TEMPLATE = "https://www.bok.or.kr/portal/search/search/main.do?menuNo=201693&query={query}"
    DEFAULT_CSV_PATH = "BOK_Scraper/input/list.csv"
    
    def __init__(self, delay: float = 1.0, csv_path: Optional[str] = None):
        super().__init__(delay)
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.base_dir = Path(__file__).resolve().parent
        self.output_dir = self.base_dir / "output"
        (self.output_dir / "downloads").mkdir(parents=True, exist_ok=True)
        # previousì™€ current ë””ë ‰í† ë¦¬ ì„¤ì •
        self.previous_dir = self.output_dir / "downloads" / "previous"
        self.current_dir = self.output_dir / "downloads" / "current"
        self.previous_dir.mkdir(parents=True, exist_ok=True)
        self.current_dir.mkdir(parents=True, exist_ok=True)
        # FileExtractor ì´ˆê¸°í™” (current ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        self.file_extractor = FileExtractor(download_dir=str(self.current_dir), session=self.session)
        # íŒŒì¼ ë¹„êµê¸° ì´ˆê¸°í™”
        self.file_comparator = FileComparator(base_dir=str(self.output_dir / "downloads"))
        # CSVì—ì„œ ëŒ€ìƒ ê·œì • ëª©ë¡ ë¡œë“œ
        self.csv_path = csv_path or self.DEFAULT_CSV_PATH
        self.target_laws = self._load_target_laws(self.csv_path)
        if self.target_laws:
            print(f"âœ“ CSVì—ì„œ {len(self.target_laws)}ê°œì˜ ëŒ€ìƒ ê·œì •ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {self.csv_path}")
        else:
            print("âš  ëŒ€ìƒ CSVë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ëª©ë¡ì„ ëŒ€ìƒìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    def _load_target_laws(self, csv_path: str) -> List[Dict]:
        """CSV íŒŒì¼ì—ì„œ ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ê·œì •ëª…ì„ ë¡œë“œí•œë‹¤."""
        if not csv_path:
            return []
        csv_file = Path(csv_path)
        if not csv_file.is_absolute():
            csv_file = find_project_root() / csv_path
        if not csv_file.exists():
            print(f"âš  BOK ëŒ€ìƒ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
            return []

        targets: List[Dict] = []
        try:
            with open(csv_file, "r", encoding="utf-8-sig", newline="") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    name = (row.get("ë²•ë ¹ëª…") or "").strip()
                    category = (row.get("êµ¬ë¶„") or "").strip()
                    if not name:
                        continue
                    targets.append({"law_name": name, "category": category})
        except Exception as exc:
            print(f"âš  BOK ëŒ€ìƒ CSV ë¡œë“œ ì‹¤íŒ¨: {exc}")
            return []
        return targets
    
    def _normalize_title(self, text: Optional[str]) -> str:
        """ë¹„êµë¥¼ ìœ„í•œ ê·œì •ëª… ì •ê·œí™”"""
        if not text:
            return ""
        cleaned = re.sub(r"[\s\W]+", "", text)
        return cleaned.lower()
    
    def _parse_date(self, date_text: str) -> Optional[tuple]:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ (year, month, day) íŠœí”Œë¡œ ë°˜í™˜
        ì˜ˆ: "2024.01.15" -> (2024, 1, 15)
        """
        if not date_text:
            return None
        
        # ê³µë°± ì œê±° ë° ì •ê·œí™”
        cleaned = date_text.strip().replace(" ", "").replace("-", ".")
        
        # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD)
        date_pattern = r'(\d{4})[\.\-](\d{1,2})[\.\-](\d{1,2})'
        match = re.search(date_pattern, cleaned)
        
        if match:
            try:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return (year, month, day)
            except (ValueError, IndexError):
                pass
        
        return None
    
    def _remove_parentheses(self, text: str) -> str:
        """íƒ€ì´í‹€ì—ì„œ ê´„í˜¸ì™€ ê·¸ ë’¤ì˜ í…ìŠ¤íŠ¸ë¥¼ ì œê±°
        ì˜ˆ: "ê·œì •ëª… (ë¶€ì¹™)" -> "ê·œì •ëª…"
        ì˜ˆ: "ê·œì •ëª… [ê°œì •]" -> "ê·œì •ëª…"
        """
        if not text:
            return ""
        
        # ì†Œê´„í˜¸, ëŒ€ê´„í˜¸, ì¤‘ê´„í˜¸, ì „ê° ê´„í˜¸ ì œê±°
        # ê´„í˜¸ë¶€í„° ëê¹Œì§€ ì œê±°
        patterns = [
            r'[\(ï¼ˆ].*?[\)ï¼‰]',  # ì†Œê´„í˜¸ (ì¼ë°˜, ì „ê°)
            r'\[.*?\]',          # ëŒ€ê´„í˜¸
            r'\{.*?\}',          # ì¤‘ê´„í˜¸
        ]
        
        cleaned = text
        for pattern in patterns:
            cleaned = re.sub(pattern, '', cleaned)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        return cleaned.strip()
    
    def is_target_regulation(self, title: str) -> bool:
        """ì œëª©ì´ ëŒ€ìƒ ê·œì •ì¸ì§€ í™•ì¸ (CSV ëª©ë¡ ê¸°ë°˜)"""
        if not title or not self.target_laws:
            return True  # CSVê°€ ì—†ìœ¼ë©´ ëª¨ë“  í•­ëª© í—ˆìš©
        
        title_normalized = self._normalize_title(title)
        for target in self.target_laws:
            target_normalized = self._normalize_title(target["law_name"])
            # ì •ê·œí™”ëœ ì´ë¦„ì´ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì¸ì§€ í™•ì¸
            if target_normalized == title_normalized or target_normalized in title_normalized or title_normalized in target_normalized:
                return True
        return False
    
    def _backup_current_to_previous(self) -> None:
        """ìŠ¤í¬ë˜í¼ ì‹œì‘ ì‹œ current ë””ë ‰í† ë¦¬ë¥¼ previousë¡œ ë°±ì—…
        ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë¹„êµë¥¼ ìœ„í•´ í˜„ì¬ ë²„ì „ì„ ì´ì „ ë²„ì „ìœ¼ë¡œ ë§Œë“¦
        """
        if not self.current_dir.exists():
            return
        
        # current ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        files_in_current = [f for f in self.current_dir.glob("*") if f.is_file()]
        if not files_in_current:
            return
        
        print(f"  â†’ ì´ì „ ë²„ì „ ë°±ì—… ì¤‘... (current â†’ previous)")
        
        # previous ë””ë ‰í† ë¦¬ ë¹„ìš°ê¸°
        import shutil
        if self.previous_dir.exists():
            for item in self.previous_dir.iterdir():
                if item.is_file():
                    item.unlink()
                elif item.is_dir():
                    shutil.rmtree(item)
        
        # currentì˜ íŒŒì¼ë“¤ì„ previousë¡œ ë³µì‚¬
        for file_path in files_in_current:
            shutil.copy2(file_path, self.previous_dir / file_path.name)
        
        # current ë””ë ‰í† ë¦¬ ë¹„ìš°ê¸° (ìƒˆ íŒŒì¼ë§Œ ë‚¨ê¸°ê¸° ìœ„í•´)
        for file_path in files_in_current:
            file_path.unlink()
        
        print(f"  âœ“ ì´ì „ ë²„ì „ ë°±ì—… ì™„ë£Œ ({len(files_in_current)}ê°œ íŒŒì¼)")
    
    def _clear_diffs_directory(self) -> None:
        """ìŠ¤í¬ë˜í¼ ì‹œì‘ ì‹œ diffs ë””ë ‰í† ë¦¬ ë¹„ìš°ê¸°
        ì´ì „ ì‹¤í–‰ì˜ diff íŒŒì¼ì´ ë‚¨ì•„ìˆì–´ í˜¼ë™ì„ ë°©ì§€í•˜ê¸° ìœ„í•´
        """
        diffs_dir = self.output_dir / "downloads" / "diffs"
        if not diffs_dir.exists():
            return
        
        import shutil
        diff_files = list(diffs_dir.glob("*"))
        if not diff_files:
            return
        
        print(f"  â†’ ì´ì „ diff íŒŒì¼ ì •ë¦¬ ì¤‘...")
        for item in diff_files:
            if item.is_file():
                item.unlink()
            elif item.is_dir():
                shutil.rmtree(item)
        
        print(f"  âœ“ diff íŒŒì¼ ì •ë¦¬ ì™„ë£Œ ({len(diff_files)}ê°œ íŒŒì¼)")
    
    def extract_regulation_list(self, soup: BeautifulSoup, search_keyword: str = "") -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ ì²« ë²ˆì§¸ í•­ëª© ì¶”ì¶œ"""
        results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ ì˜ì—­ ì°¾ê¸° (ë©”ë‰´ë‚˜ ì‚¬ì´ë“œë°” ì œì™¸)
        search_result_containers = [
            ".bdLine.type4",  # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ ì˜ì—­
            ".bdLine",  # bdLine í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì˜ì—­
            "div.content .bdLine",  # content ì•ˆì˜ bdLine
            "#searchResult",  # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ
            ".search-result", 
            ".search-results",
            ".result-area",
            ".result-list",
            "main .list",
            "main ul",
            ".content ul",
            "#content ul",
        ]
        
        result_container = None
        for container_selector in search_result_containers:
            container = soup.select_one(container_selector)
            if container:
                result_container = container
                print(f"  âœ“ ê²€ìƒ‰ ê²°ê³¼ ì˜ì—­ ë°œê²¬: {container_selector}")
                break
        
        # ê²€ìƒ‰ ê²°ê³¼ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ì°¾ê¸°
        if not result_container:
            result_container = soup
        
        # í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ë³´í†µ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í‘œì‹œë¨
        selectors = [
            ".bdLine.type4 ul li",  # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            ".bdLine ul li",  # bdLine í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ulì˜ li
            "div.bdLine li",  # bdLine div ì•ˆì˜ li
            "li a[href*='view.do']",  # view.doë¥¼ í¬í•¨í•œ ë§í¬ê°€ ìˆëŠ” li
            "li a[href*='/portal/singl/law/view.do']",  # ë²•ê·œ ìƒì„¸ í˜ì´ì§€ ë§í¬
            "li[class*='result']",  # result í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ li
            "li[class*='item']",  # item í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ li
            ".search-result-list li",  # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            ".result-list li",
            ".search-list li",
            "ul.search-result li",
            "ul.result li",
            "li a[href*='bbs']",  # bbsë¥¼ í¬í•¨í•œ ë§í¬ê°€ ìˆëŠ” li
            "table tbody tr",  # í…Œì´ë¸” í–‰
            ".list-item",
            ".law-item",
            ".regulation-item",
            "div.list li",
        ]
        
        found_items = []
        for selector in selectors:
            items = result_container.select(selector)
            if items and len(items) > 0:
                # ë¹ˆ í•­ëª©ì´ë‚˜ í—¤ë” ì œì™¸í•˜ê³  ì‹¤ì œ ë°ì´í„° í•­ëª©ë§Œ í•„í„°ë§
                valid_items = []
                for item in items:
                    # ë§í¬ê°€ ìˆê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í•­ëª©ë§Œ í¬í•¨
                    link = item.select_one("a")
                    text = item.get_text(strip=True)
                    
                    # í•„í„°ë§ ì¡°ê±´:
                    # 1. ë§í¬ê°€ ìˆì–´ì•¼ í•¨
                    # 2. í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•´ì•¼ í•¨
                    # 3. ê²€ìƒ‰ì–´ì™€ ê´€ë ¨ì´ ìˆì–´ì•¼ í•¨ (ê²€ìƒ‰ì–´ê°€ ì œê³µëœ ê²½ìš°)
                    # 4. view.doë‚˜ bbsë¥¼ í¬í•¨í•œ ë§í¬ì—¬ì•¼ í•¨ (ë²•ê·œ ìƒì„¸ í˜ì´ì§€ë¡œ ê°€ëŠ” ë§í¬)
                    if link:
                        href = link.get("href", "")
                        # ë²•ê·œ ìƒì„¸ í˜ì´ì§€ ë§í¬ì¸ì§€ í™•ì¸
                        # /portal/singl/law/view.do ë˜ëŠ” /portal/bbs/.../view.do í˜•ì‹
                        is_regulation_link = (
                            ("view.do" in href and ("/portal/singl/law/" in href or "/portal/bbs/" in href)) or
                            ("bbs" in href and "view.do" in href) or
                            ("/portal/singl/law/view.do" in href)
                        )
                        
                        # ë©”ë‰´ë‚˜ ì‚¬ì´ë“œë°” ë§í¬ ì œì™¸ (ecos, youtube, facebook ë“±)
                        is_excluded = any(excluded in href.lower() for excluded in [
                            "ecos.bok.or.kr",
                            "youtube.com",
                            "facebook.com",
                            "instagram.com",
                            "twitter.com",
                            "#",
                            "javascript:",
                            "list.do",  # ëª©ë¡ í˜ì´ì§€ ì œì™¸
                        ])
                        
                        if (is_regulation_link and 
                            not is_excluded and 
                            len(text) > 10 and
                            (not search_keyword or search_keyword in text or search_keyword[:5] in text)):
                            valid_items.append(item)
                
                if valid_items:
                    found_items = valid_items
                    print(f"  âœ“ ì„ íƒì '{selector}'ë¡œ {len(valid_items)}ê°œ ìœ íš¨í•œ í•­ëª© ë°œê²¬")
                    # ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œ í•­ëª©ì˜ ë§í¬ ì¶œë ¥
                    for i, item in enumerate(valid_items[:3], 1):
                        link_elem = item.select_one("a[href]")
                        if link_elem:
                            href = link_elem.get("href", "")
                            title = link_elem.get_text(strip=True) or item.get_text(strip=True)[:50]
                            print(f"    [{i}] {title[:30]}... -> {href[:80]}")
                    break
        
        if not found_items:
            # ë””ë²„ê¹…ì„ ìœ„í•´ HTML ì¼ë¶€ ì €ì¥
            self.save_debug_html(soup, filename="debug_bok_list.html")
            print("  âš  ëª©ë¡ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë””ë²„ê·¸ HTML ì €ì¥: output/debug/debug_bok_list.html")
            print("  ğŸ’¡ ë””ë²„ê·¸ HTMLì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.")
            return results
        
        # ì²« ë²ˆì§¸ í•­ëª©ë§Œ ì¶”ì¶œ
        if found_items:
            item = found_items[0]
            try:
                # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
                title = None
                title_elem = (
                    item.select_one("span.col a span.title") or  # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹
                    item.select_one("span.col a") or  # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹
                    item.select_one("a span.title") or
                    item.select_one("a") or
                    item.select_one(".title") or
                    item.select_one(".result-title") or
                    item.select_one("td:first-child") or
                    item.select_one(".name") or
                    item.select_one("strong") or
                    item  # ì „ì²´ í•­ëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                )
                
                if title_elem:
                    # ë§í¬ê°€ ìˆìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸
                    if title_elem.name == "a":
                        title = title_elem.get_text(strip=True)
                    else:
                        # ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‹œë„
                        link_in_elem = title_elem.select_one("a")
                        if link_in_elem:
                            title = link_in_elem.get_text(strip=True)
                        else:
                            title = title_elem.get_text(strip=True)
                
                # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                if not title or len(title) < 5:
                    print(f"  âš  ì²« ë²ˆì§¸ í•­ëª©ì—ì„œ ì œëª©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    return results
                
                print(f"  âœ“ ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ ë°œê²¬: {title}")
                
                # ìƒì„¸ ë§í¬ ì¶”ì¶œ
                detail_link = ""
                # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹: span.col > a
                link_elem = (
                    item.select_one("span.col a[href]") or
                    item.select_one("a[href*='view.do']") or
                    item.select_one("a[href]")
                )
                if link_elem:
                    href = link_elem.get("href", "")
                    print(f"  â†’ ì›ë³¸ href: {href}")
                    if href:
                        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if href.startswith("/"):
                            detail_link = self.BASE_URL + href
                        elif href.startswith("http"):
                            detail_link = href
                        else:
                            detail_link = urljoin(self.BASE_URL, href)
                        print(f"  â†’ ìµœì¢… ìƒì„¸ ë§í¬: {detail_link}")
                        
                        # ì˜¬ë°”ë¥¸ ë§í¬ í˜•ì‹ì¸ì§€ í™•ì¸
                        # /portal/singl/law/view.do ë˜ëŠ” /portal/bbs/.../view.do í˜•ì‹
                        if ("view.do" in detail_link and 
                            ("/portal/singl/law/view.do" in detail_link or 
                             "/portal/bbs/" in detail_link or 
                             "nttId" in detail_link or
                             "lawseq" in detail_link)):
                            print(f"  âœ“ ì˜¬ë°”ë¥¸ ë²•ê·œ ìƒì„¸ í˜ì´ì§€ ë§í¬ í˜•ì‹ í™•ì¸ë¨")
                        else:
                            print(f"  âš  ê²½ê³ : ì˜ˆìƒê³¼ ë‹¤ë¥¸ ë§í¬ í˜•ì‹ì…ë‹ˆë‹¤.")
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ê°œì •ì¼, ë²ˆí˜¸ ë“±)
                regulation_info = {
                    "title": title,
                    "regulation_name": title,
                    "organization": "í•œêµ­ì€í–‰",
                    "detail_link": detail_link,
                    "content": "",
                    "department": "",
                    "file_names": [],
                    "download_links": [],
                    "enactment_date": "",
                    "revision_date": "",
                }
                
                # ê°œì •ì¼ ì¶”ì¶œ ì‹œë„ (ë‹¤ì–‘í•œ íŒ¨í„´)
                date_text = None
                date_elem = (
                    item.select_one("span.fs_date") or  # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹
                    item.select_one("div.col.dataInfo1 span.fs_date") or  # í•œêµ­ì€í–‰ ë²•ê·œ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹
                    item.select_one(".date") or
                    item.select_one(".revision-date") or
                    item.select_one(".result-date") or
                    item.select_one("td:nth-child(3)") or
                    item.select_one("td:nth-child(2)") or
                    item.select_one("span.date") or
                    item.select_one("em.date")
                )
                
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                else:
                    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD í˜•ì‹)
                    import re
                    full_text = item.get_text()
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', full_text)
                    if date_match:
                        date_text = date_match.group(1)
                
                if date_text:
                    regulation_info["revision_date"] = date_text
                    print(f"    ê°œì •ì¼: {date_text}")
                
                results.append(regulation_info)
                
            except Exception as e:
                print(f"  âš  í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
        
        return results
    
    def extract_regulation_detail(self, url: str, regulation_name: str = "") -> Dict:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ê·œì • ë‚´ìš© ì¶”ì¶œ"""
        detail_info = {
            "content": "",  # PDFì—ì„œ ì¶”ì¶œí•œ ë³¸ë¬¸ ë‚´ìš©
            "file_names": [],
            "download_links": [],
            "revision_date": "",
            "enactment_date": "",  # ì œì •ì¼
            "department": "",  # ì†Œê´€ë¶€ì„œ
        }
        
        try:
            print(f"  â†’ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì¤‘: {url}")
            # ì˜¬ë°”ë¥¸ URL í˜•ì‹ì¸ì§€ í™•ì¸
            # /portal/singl/law/view.do ë˜ëŠ” /portal/bbs/.../view.do í˜•ì‹
            if ("view.do" in url and 
                ("/portal/singl/law/view.do" in url or 
                 "/portal/bbs/" in url or 
                 "nttId" in url or
                 "lawseq" in url)):
                print(f"  âœ“ ì˜¬ë°”ë¥¸ ë²•ê·œ ìƒì„¸ í˜ì´ì§€ URL í˜•ì‹ í™•ì¸ë¨")
            else:
                print(f"  âš  ê²½ê³ : ì˜ˆìƒê³¼ ë‹¤ë¥¸ URL í˜•ì‹ì…ë‹ˆë‹¤.")
                print(f"     URL: {url}")
            
            # Selenium driver ìƒì„± (XPathë¡œ ì†Œê´€ë¶€ì„œ ì¶”ì¶œì„ ìœ„í•´)
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.common.exceptions import TimeoutException, NoSuchElementException
            
            chrome_options = self._build_default_chrome_options()
            driver = self._create_webdriver(chrome_options)
            
            try:
                driver.get(url)
                time.sleep(2)
                
                # XPathë¡œ ì†Œê´€ë¶€ì„œ ì¶”ì¶œ
                try:
                    department_xpath = "/html/body/div/div[2]/main/div[1]/form/div/div[1]/dl[3]/dd"
                    wait = WebDriverWait(driver, 10)
                    department_elem = wait.until(EC.presence_of_element_located((By.XPATH, department_xpath)))
                    department_text = department_elem.text.strip()
                    if department_text:
                        # ì²« ë²ˆì§¸ '(' ì•ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì˜ˆ: "êµ­ì œì´ê´„íŒ€(02-759-5748)" â†’ "êµ­ì œì´ê´„íŒ€")
                        if '(' in department_text:
                            department_text = department_text.split('(')[0].strip()
                        detail_info["department"] = department_text
                        print(f"  âœ“ ì†Œê´€ë¶€ì„œ (XPath): {department_text}")
                except (TimeoutException, NoSuchElementException):
                    print(f"  âš  XPathë¡œ ì†Œê´€ë¶€ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {department_xpath}")
                except Exception as e:
                    print(f"  âš  ì†Œê´€ë¶€ì„œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # BeautifulSoupìœ¼ë¡œ ë³€í™˜
                soup = BeautifulSoup(driver.page_source, 'html.parser')
            finally:
                driver.quit()
            
            # ì²¨ë¶€íŒŒì¼ ëª©ë¡ ì°¾ê¸°: ul > li > a êµ¬ì¡°
            # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
            file_list_selectors = [
                "main form div dl dd ul li a",  # ì¼ë°˜ì ì¸ êµ¬ì¡°
                "form div dl dd ul li a",
                "dl dd ul li a",
                "ul li a[href*='download']",
                "ul li a[href*='file']",
            ]
            
            file_links = []
            for selector in file_list_selectors:
                links = soup.select(selector)
                if links:
                    file_links = links
                    print(f"  âœ“ ì²¨ë¶€íŒŒì¼ ëª©ë¡ ë°œê²¬: {len(links)}ê°œ (ì…€ë ‰í„°: {selector})")
                    break
            
            if not file_links:
                # ë””ë²„ê¹…ì„ ìœ„í•´ HTML ì €ì¥
                self.save_debug_html(soup, filename="debug_bok_detail.html")
                print(f"  âš  ì²¨ë¶€íŒŒì¼ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë””ë²„ê·¸ HTML ì €ì¥: output/debug/debug_bok_detail.html")
            
            # PDF íŒŒì¼ ìš°ì„ , ì—†ìœ¼ë©´ HWP íŒŒì¼ ì°¾ê¸°
            selected_file_elem = None
            file_type = None
            
            for link in file_links:
                href = link.get("href", "")
                link_text = link.get_text(strip=True)
                
                # hrefë‚˜ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ í™•ì¥ì í™•ì¸
                if href:
                    href_lower = href.lower()
                    if '.pdf' in href_lower or link_text.lower().endswith('.pdf'):
                        selected_file_elem = link
                        file_type = 'pdf'
                        print(f"  âœ“ PDF íŒŒì¼ ë°œê²¬: {link_text}")
                        break
                    elif '.hwp' in href_lower or link_text.lower().endswith('.hwp'):
                        if not selected_file_elem:  # PDFê°€ ì—†ì„ ë•Œë§Œ HWP ì„ íƒ
                            selected_file_elem = link
                            file_type = 'hwp'
                            print(f"  âœ“ HWP íŒŒì¼ ë°œê²¬: {link_text}")
            
            if selected_file_elem:
                href = selected_file_elem.get("href", "")
                if href:
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith("/"):
                        file_url = self.BASE_URL + href
                    elif href.startswith("http"):
                        file_url = href
                    else:
                        file_url = urljoin(self.BASE_URL, href)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ
                    file_name = None
                    from urllib.parse import urlparse, parse_qs, unquote
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
                    link_text = selected_file_elem.get_text(strip=True)
                    if link_text and ('.pdf' in link_text.lower() or '.hwp' in link_text.lower()):
                        # í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                        import re
                        match = re.search(r'([^/]+\.(pdf|hwp))', link_text, re.IGNORECASE)
                        if match:
                            file_name = match.group(1)
                    
                    # hrefì˜ fileNm íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not file_name:
                        try:
                            parsed_url = urlparse(href)
                            query_params = parse_qs(parsed_url.query)
                            
                            if 'fileNm' in query_params:
                                file_name = query_params['fileNm'][0]
                                file_name = unquote(file_name)
                            elif 'fileNm=' in href:
                                file_nm_part = href.split('fileNm=')[1]
                                if '&' in file_nm_part:
                                    file_name = file_nm_part.split('&')[0]
                                elif '&amp;' in file_nm_part:
                                    file_name = file_nm_part.split('&amp;')[0]
                                else:
                                    file_name = file_nm_part
                                file_name = unquote(file_name)
                        except Exception as e:
                            print(f"  âš  íŒŒì¼ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    # íŒŒì¼ëª…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° fallback
                    if not file_name:
                        if file_type == 'pdf':
                            file_name = "íŒŒì¼.pdf"
                        elif file_type == 'hwp':
                            file_name = "íŒŒì¼.hwp"
                        else:
                            file_name = "íŒŒì¼"
                    
                    detail_info["download_links"].append(file_url)
                    detail_info["file_names"].append(file_name)
                    print(f"  âœ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {file_name}")
                    print(f"    ë§í¬: {file_url}")
                    
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¹„êµ
                    downloaded_file_path = self._download_and_compare_file(file_url, file_name, regulation_name=regulation_name)
                    
                    # PDF ë˜ëŠ” HWP íŒŒì¼ì´ë©´ ë‚´ìš© ì¶”ì¶œ
                    if downloaded_file_path and downloaded_file_path.get('file_path'):
                        file_path = downloaded_file_path['file_path']
                        if file_path.lower().endswith('.pdf'):
                            print(f"  PDF ë‚´ìš© ì¶”ì¶œ ì¤‘...")
                            pdf_content = self.file_extractor.extract_pdf_content(file_path)
                            if pdf_content:
                                detail_info["content"] = pdf_content
                                print(f"  âœ“ PDFì—ì„œ {len(pdf_content)}ì ì¶”ì¶œ ì™„ë£Œ")
                                
                                # PDFì—ì„œ ì œì •ì¼ê³¼ ìµœê·¼ê°œì •ì¼ ì¶”ì¶œ
                                extracted_info = self._extract_info_from_pdf_content(pdf_content)
                                if extracted_info.get("enactment_date"):
                                    detail_info["enactment_date"] = extracted_info["enactment_date"]
                                    print(f"  âœ“ ì œì •ì¼: {extracted_info['enactment_date']}")
                                if extracted_info.get("revision_date"):
                                    detail_info["revision_date"] = extracted_info["revision_date"]
                                    print(f"  âœ“ ìµœê·¼ê°œì •ì¼: {extracted_info['revision_date']}")
                            else:
                                print(f"  âš  PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                        elif file_path.lower().endswith('.hwp'):
                            print(f"  HWP ë‚´ìš© ì¶”ì¶œ ì¤‘...")
                            hwp_content = self.file_extractor.extract_hwp_content(file_path)
                            if hwp_content:
                                detail_info["content"] = hwp_content
                                print(f"  âœ“ HWPì—ì„œ {len(hwp_content)}ì ì¶”ì¶œ ì™„ë£Œ")
                                
                                # HWPì—ì„œ ì œì •ì¼ê³¼ ìµœê·¼ê°œì •ì¼ ì¶”ì¶œ
                                extracted_info = self._extract_info_from_pdf_content(hwp_content)
                                if extracted_info.get("enactment_date"):
                                    detail_info["enactment_date"] = extracted_info["enactment_date"]
                                    print(f"  âœ“ ì œì •ì¼: {extracted_info['enactment_date']}")
                                if extracted_info.get("revision_date"):
                                    detail_info["revision_date"] = extracted_info["revision_date"]
                                    print(f"  âœ“ ìµœê·¼ê°œì •ì¼: {extracted_info['revision_date']}")
                            else:
                                print(f"  âš  HWP ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                    else:
                        print(f"  âš  ì²¨ë¶€íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"  âš  ìƒì„¸ í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return detail_info
    
    def _extract_info_from_pdf_content(self, content: str) -> Dict[str, str]:
        """PDF ë‚´ìš©ì—ì„œ ì†Œê´€ë¶€ì„œ, ì œì •ì¼, ìµœê·¼ê°œì •ì¼ ì¶”ì¶œ"""
        result = {
            "department": "",
            "enactment_date": "",
            "revision_date": "",
        }
        
        if not content:
            return result
        
        # ì œì •ì¼ íŒ¨í„´ ì°¾ê¸° (YYYYë…„ MMì›” DDì¼ ë˜ëŠ” YYYY-MM-DD í˜•ì‹)
        # ì˜ˆ: "2023ë…„ 1ì›” 12ì¼", "2023-01-12", "ì œì •ì¼: 2023.01.12"
        # ì˜ˆ: "<2008. 1.24ì¼ ì œ ì •>", "<2008.1.24ì¼ ì œ ì •>"
        # ì˜ˆ: "ì œì •ê°œì • | 1999. 4. 3.1999. 6. 7.2000. 8. 31." (í‘œ í˜•ì‹, ì²« ë²ˆì§¸ ë‚ ì§œê°€ ì œì •ì¼)
        date_patterns = [
            r'<(\d{4})\.\s*(\d{1,2})\.(\d{1,2})ì¼\s*ì œ\s*ì •>',  # <2008. 1.24ì¼ ì œ ì •> í˜•ì‹
            r'<(\d{4})\.(\d{1,2})\.(\d{1,2})ì¼\s*ì œ\s*ì •>',  # <2008.1.24ì¼ ì œ ì •> í˜•ì‹ (ê³µë°± ì—†ìŒ)
            r'ì œì •ì¼[:\s]*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'ì œì •ì¼[:\s]*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
            r'ì œì •ì¼[:\s]*(\d{4})-(\d{1,2})-(\d{1,2})',
            r'ì œì •[:\s]*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'ì œì •[:\s]*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
            r'ì œì •[:\s]*(\d{4})-(\d{1,2})-(\d{1,2})',
        ]
        
        # í‘œ í˜•ì‹ ì²˜ë¦¬: "ì œì •ê°œì • | 1999. 4. 3.1999. 6. 7.2000. 8. 31." í˜•ì‹
        # "ì œì •ê°œì •" í…ìŠ¤íŠ¸ë¥¼ ì°¾ê³  ê·¸ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ì²« ë²ˆì§¸ ë‚ ì§œë¥¼ ì œì •ì¼ë¡œ ì‚¬ìš©
        if not result.get("enactment_date"):
            enactment_table_match = re.search(r'ì œì •ê°œì •[^\d]*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', content)
            if enactment_table_match:
                year, month, day = enactment_table_match.groups()
                result["enactment_date"] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # ì¼ë°˜ íŒ¨í„´ìœ¼ë¡œ ì œì •ì¼ ì°¾ê¸°
        if not result.get("enactment_date"):
            for pattern in date_patterns:
                match = re.search(pattern, content)
                if match:
                    year, month, day = match.groups()
                    # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    result["enactment_date"] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    break
        
        # ì†Œê´€ë¶€ì„œ íŒ¨í„´ ì°¾ê¸° (ë§ˆì§€ë§‰ ê°œì •ì¼ ë¶€ë¶„ì—ì„œ ì¶”ì¶œ)
        # ì˜ˆ: "ê°œì • 2025. 6. 24. êµ­ì¥ê²°ì¬ êµ­ì œì´ê´„íŒ€- 793"
        # ì˜ˆ: "ê°œì •2023.12.20.ì´ì¬ê²°ì¬ ì™¸í™˜ì •ë³´íŒ€-1028"
        # íŒ¨í„´: ê°œì • + ë‚ ì§œ + ê²°ì¬ + íŒ€ëª… + "-" + ìˆ«ì
        department_patterns = [
            r'ê°œì •\s*\d{4}\.?\s*\d{1,2}\.?\s*\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)\s*-',  # ê³µë°± í¬í•¨
            r'ê°œì •\s*\d{4}\.?\s*\d{1,2}\.?\s*\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)-',  # ê³µë°± ì—†ìŒ
            r'ê°œì •\d{4}\.?\d{1,2}\.?\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)\s*-',  # ê³µë°± ì—†ìŒ (ë‚ ì§œ ë¶€ë¶„)
            r'ê°œì •\d{4}\.?\d{1,2}\.?\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)-',  # ê³µë°± ì—†ìŒ (ì „ì²´)
        ]
        
        # ëª¨ë“  ê°œì •ì¼ íŒ¨í„´ì„ ì°¾ì•„ì„œ ë§ˆì§€ë§‰ ê²ƒì„ ì‚¬ìš©
        all_matches = []
        for pattern in department_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                team_name = match.group(1).strip()
                if team_name:
                    all_matches.append((match.start(), team_name))
        
        # ë§ˆì§€ë§‰ ê°œì •ì¼ì—ì„œ ì¶”ì¶œí•œ íŒ€ëª… ì‚¬ìš©
        if all_matches:
            # ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë§ˆì§€ë§‰ ê²ƒ ì„ íƒ
            all_matches.sort(key=lambda x: x[0])
            result["department"] = all_matches[-1][1]
        
        # ìµœê·¼ê°œì •ì¼ íŒ¨í„´ ì°¾ê¸°
        # ì˜ˆ: "<2025. 2.28ì¼ ì œ9ì°¨ ê°œì •>", "<2025.2.28ì¼ ì œ9ì°¨ ê°œì •>"
        # ì˜ˆ: "ê°œì • 2025. 6. 24.", "ê°œì •2025.6.24."
        # ì˜ˆ: "ê°œì • 2000. 7.26 ì´ì¬ê²°ì¬...", "2002. 3.18 ì´ì¬ê²°ì¬..." (ê°œì • ìƒëµ)
        # ì˜ˆ: "ì œì •ê°œì • | 1999. 4. 3.1999. 6. 7.2000. 8. 31.2002. 1. 5." (í‘œ í˜•ì‹, ëª¨ë“  ë‚ ì§œ ì¶”ì¶œ)
        revision_date_patterns = [
            r'<(\d{4})\.\s*(\d{1,2})\.(\d{1,2})ì¼\s*ì œ\d*ì°¨\s*ê°œì •>',  # <2025. 2.28ì¼ ì œ9ì°¨ ê°œì •> í˜•ì‹
            r'<(\d{4})\.(\d{1,2})\.(\d{1,2})ì¼\s*ì œ\d*ì°¨\s*ê°œì •>',  # <2025.2.28ì¼ ì œ9ì°¨ ê°œì •> í˜•ì‹ (ê³µë°± ì—†ìŒ)
            r'ê°œì •\s*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.',  # ê°œì • 2025. 6. 24. í˜•ì‹
            r'ê°œì •\s*(\d{4})\.(\d{1,2})\.(\d{1,2})\.',  # ê°œì • 2025.6.24. í˜•ì‹
            r'ê°œì •\s*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\s+',  # ê°œì • 2000. 7.26 ì´ì¬ê²°ì¬... í˜•ì‹
            r'ê°œì •\s*(\d{4})\.(\d{1,2})\.(\d{1,2})\s+',  # ê°œì • 2000.7.26 ì´ì¬ê²°ì¬... í˜•ì‹
            r'^(\d{4})\.\s*(\d{1,2})\.(\d{1,2})\s+[ê°€-í£]',  # 2002. 3.18 ì´ì¬ê²°ì¬... í˜•ì‹ (ê°œì • ìƒëµ, ì¤„ ì‹œì‘)
            r'\n(\d{4})\.\s*(\d{1,2})\.(\d{1,2})\s+[ê°€-í£]',  # 2002. 3.18 ì´ì¬ê²°ì¬... í˜•ì‹ (ê°œì • ìƒëµ, ì¤„ë°”ê¿ˆ í›„)
            r'ê°œì •\s*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',  # ê°œì • 2025. 6. 24 í˜•ì‹
            r'ê°œì •\s*(\d{4})\.(\d{1,2})\.(\d{1,2})',  # ê°œì • 2025.6.24 í˜•ì‹
        ]
        
        # í‘œ í˜•ì‹ ì²˜ë¦¬: "ì œì •ê°œì • | 1999. 4. 3.1999. 6. 7.2000. 8. 31.2002. 1. 5.2002. 3. 14.2004. 2. 4.2005. 3. 29.2006. 10. 13."
        # "ì œì •ê°œì •" í…ìŠ¤íŠ¸ë¥¼ ì°¾ê³  ê·¸ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ëª¨ë“  ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ì—¬ ê°€ì¥ ìµœì‹  ê²ƒì„ ì‚¬ìš©
        table_dates = []
        table_match = re.search(r'ì œì •ê°œì •[^\d]*((?:\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)+)', content)
        if table_match:
            dates_str = table_match.group(1)
            # ëª¨ë“  ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (YYYY. M. D. í˜•ì‹)
            date_matches = re.finditer(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', dates_str)
            for match in date_matches:
                year, month, day = match.groups()
                try:
                    from datetime import datetime
                    date_obj = datetime(int(year), int(month), int(day))
                    table_dates.append((match.start(), date_obj, year, month, day))
                except:
                    pass
        
        # ëª¨ë“  ê°œì •ì¼ì„ ì°¾ì•„ì„œ ê°€ì¥ ìµœì‹  ê²ƒ ì‚¬ìš©
        all_revision_dates = []
        
        # í‘œ í˜•ì‹ì—ì„œ ì°¾ì€ ë‚ ì§œë“¤ ì¶”ê°€ (ì²« ë²ˆì§¸ëŠ” ì œì •ì¼ì´ë¯€ë¡œ ì œì™¸)
        if table_dates and len(table_dates) > 1:
            # ì²« ë²ˆì§¸ëŠ” ì œì •ì¼ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ê°œì •ì¼ë¡œ ì‚¬ìš©
            for date_info in table_dates[1:]:
                all_revision_dates.append(date_info)
        
        # ì¼ë°˜ íŒ¨í„´ìœ¼ë¡œ ê°œì •ì¼ ì°¾ê¸°
        for pattern in revision_date_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                year, month, day = match.groups()
                # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
                try:
                    from datetime import datetime
                    date_obj = datetime(int(year), int(month), int(day))
                    all_revision_dates.append((match.start(), date_obj, year, month, day))
                except:
                    pass
        
        # ê°€ì¥ ìµœì‹  ê°œì •ì¼ ì„ íƒ
        if all_revision_dates:
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœì‹  ê²ƒ ì„ íƒ
            all_revision_dates.sort(key=lambda x: x[1], reverse=True)
            year, month, day = all_revision_dates[0][2], all_revision_dates[0][3], all_revision_dates[0][4]
            result["revision_date"] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # ìµœì¢… Fallback: ë¬¸ì„œ ë‚´ ëª¨ë“  ë‚ ì§œë¥¼ ìŠ¤ìº”í•´ ì œì •/ìµœê·¼ê°œì • ë³´ì •
        # ì´ë¯¸ ì¶”ì¶œëœ revision_dateê°€ ìˆì–´ë„ ë” ìµœì‹  ë‚ ì§œê°€ ìˆìœ¼ë©´ ë®ì–´ì“´ë‹¤.
        date_candidates = []
        for match in re.finditer(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.?', content):
            try:
                from datetime import datetime
                y, m, d = match.groups()
                date_obj = datetime(int(y), int(m), int(d))
                date_candidates.append((date_obj, y, m, d))
            except Exception:
                continue
        for match in re.finditer(r'(\d{4})-(\d{1,2})-(\d{1,2})', content):
            try:
                from datetime import datetime
                y, m, d = match.groups()
                date_obj = datetime(int(y), int(m), int(d))
                date_candidates.append((date_obj, y, m, d))
            except Exception:
                continue

        if date_candidates:
            if not result.get("enactment_date"):
                oldest = min(date_candidates, key=lambda x: x[0])
                result["enactment_date"] = f"{oldest[1]}-{oldest[2].zfill(2)}-{oldest[3].zfill(2)}"

            latest = max(date_candidates, key=lambda x: x[0])
            latest_dt, ly, lm, ld = latest

            def _parse_existing(dt_str: str):
                try:
                    from datetime import datetime
                    parts = dt_str.split("-")
                    return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                except Exception:
                    return None

            existing_rev_dt = _parse_existing(result.get("revision_date", ""))
            # ë” ìµœì‹  ë‚ ì§œê°€ ìˆìœ¼ë©´ ìµœê·¼ê°œì •ì¼ ë®ì–´ì“°ê¸°
            if (existing_rev_dt is None) or (latest_dt > existing_rev_dt):
                result["revision_date"] = f"{ly}-{lm.zfill(2)}-{ld.zfill(2)}"
        
        return result
    
    def _get_safe_filename(self, filename: str, regulation_name: str = "") -> str:
        """
        íŒŒì¼ëª…ì„ ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê²½ë¡œì— ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ìë§Œ)
        
        Args:
            filename: ì›ë³¸ íŒŒì¼ëª…
            regulation_name: ê·œì •ëª… (íŒŒì¼ëª… ìƒì„±ìš©)
            
        Returns:
            ì•ˆì „í•œ íŒŒì¼ëª…
        """
        import re
        # ê·œì •ëª…ì´ ìˆìœ¼ë©´ ê·œì •ëª… ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
        if regulation_name:
            # ê·œì •ëª…ì—ì„œ ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ
            safe_reg_name = re.sub(r'[^\w\s-]', '', regulation_name)
            safe_reg_name = safe_reg_name.replace(' ', '_')
            # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
            ext = Path(filename).suffix if filename else '.pdf'
            return f"{safe_reg_name}{ext}"
        else:
            # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ
            safe_name = re.sub(r'[^\w\s.-]', '', filename)
            return safe_name.replace(' ', '_')
    
    def _download_and_compare_file(self, file_url: str, file_name: str, regulation_name: str = "") -> Optional[Dict]:
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì´ì „ íŒŒì¼ê³¼ ë¹„êµ
        
        Args:
            file_url: ë‹¤ìš´ë¡œë“œ URL
            file_name: íŒŒì¼ëª…
            regulation_name: ê·œì •ëª… (ì´ì „ íŒŒì¼ ë§¤ì¹­ìš©)
            
        Returns:
            ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_filename = self._get_safe_filename(file_name, regulation_name)
            
            # ìƒˆ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ (current ë””ë ‰í† ë¦¬)
            new_file_path = self.current_dir / safe_filename
            
            # ì´ì „ íŒŒì¼ ê²½ë¡œ (previous ë””ë ‰í† ë¦¬)
            previous_file_path = self.previous_dir / safe_filename
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            print(f"  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_name}")
            # FileExtractor.download_fileëŠ” (filepath, actual_filename) íŠœí”Œ ë°˜í™˜
            downloaded_result = self.file_extractor.download_file(
                file_url,
                safe_filename,
                use_selenium=False,  # requestsë¡œ ë‹¤ìš´ë¡œë“œ
                driver=None
            )
            
            # íŠœí”Œ ì–¸íŒ¨í‚¹
            if downloaded_result:
                downloaded_path, actual_filename = downloaded_result
            else:
                downloaded_path, actual_filename = None, None
            
            if not downloaded_path or not os.path.exists(downloaded_path):
                print(f"  âš  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return None
            
            # ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ìƒˆ íŒŒì¼ ê²½ë¡œë¡œ ì´ë™/ë³µì‚¬
            if str(downloaded_path) != str(new_file_path):
                import shutil
                if new_file_path.exists():
                    new_file_path.unlink()  # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
                shutil.move(downloaded_path, new_file_path)
                print(f"  âœ“ íŒŒì¼ ì €ì¥: {new_file_path}")
            
            # ì´ì „ íŒŒì¼ê³¼ ë¹„êµ
            comparison_result = None
            if previous_file_path.exists():
                print(f"  â†’ ì´ì „ íŒŒì¼ê³¼ ë¹„êµ ì¤‘... (ì´ì „ íŒŒì¼: {previous_file_path})")
                comparison_result = self.file_comparator.compare_and_report(
                    str(new_file_path),
                    str(previous_file_path),
                    save_diff=True
                )
                
                if comparison_result['changed']:
                    print(f"  âœ“ íŒŒì¼ ë³€ê²½ ê°ì§€: {comparison_result['diff_summary']}")
                    if 'diff_file' in comparison_result:
                        print(f"    Diff íŒŒì¼: {comparison_result['diff_file']}")
                        html_file = Path(comparison_result['diff_file']).with_suffix('.html')
                        if html_file.exists():
                            print(f"    HTML Diff íŒŒì¼: {html_file}")
                else:
                    print(f"  âœ“ íŒŒì¼ ë™ì¼ (ë³€ê²½ ì—†ìŒ)")
            else:
                print(f"  âœ“ ìƒˆ íŒŒì¼ (ì´ì „ íŒŒì¼ ì—†ìŒ)")
            
            return {
                'file_path': str(new_file_path),
                'previous_file_path': str(previous_file_path) if previous_file_path.exists() else None,
                'comparison': comparison_result,
            }
            
        except Exception as e:
            print(f"  âš  íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_regulations(self) -> List[Dict]:
        """
        ë²•ê·œì •ë³´ - ê·œì • ìŠ¤í¬ë˜í•‘
        CSV ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ ê° ê·œì •ëª…ì„ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘
        """
        # ìŠ¤í¬ë˜í¼ ì‹œì‘ ì‹œ currentë¥¼ previousë¡œ ë°±ì—… (ì´ì „ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì´ì „ ë²„ì „ìœ¼ë¡œ)
        self._backup_current_to_previous()
        # ì´ì „ ì‹¤í–‰ì˜ diff íŒŒì¼ ì •ë¦¬
        self._clear_diffs_directory()
        
        print(f"\n=== í•œêµ­ì€í–‰ ë²•ê·œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
        if not self.target_laws:
            print("âš  CSV ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ëŒ€ìƒ ê·œì •: {len(self.target_laws)}ê°œ")
        for i, target in enumerate(self.target_laws, 1):
            print(f"  {i}. {target['law_name']}")
        print()
        
        results = []
        
        try:
            # ê° ê·œì •ëª…ì„ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë° ì¶”ì¶œ
            for idx, target in enumerate(self.target_laws, 1):
                regulation_name = target["law_name"]
                print(f"\n[{idx}/{len(self.target_laws)}] {regulation_name}")
                
                # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”©
                query_encoded = quote(regulation_name)
                search_url = self.SEARCH_URL_TEMPLATE.format(query=query_encoded)
                
                print(f"  ê²€ìƒ‰ URL: {search_url}")
                
                # Seleniumìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì ‘ê·¼
                from selenium.webdriver.common.by import By
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC
                from selenium.common.exceptions import TimeoutException, NoSuchElementException
                
                # Selenium driver ìƒì„±
                chrome_options = self._build_default_chrome_options()
                driver = self._create_webdriver(chrome_options)
                
                detail_link = None
                title = None
                
                try:
                    driver.get(search_url)
                    
                    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                    time.sleep(2)

                    # ë””ë²„ê¹… HTML ì €ì¥ (ì²« ë²ˆì§¸ ê²€ìƒ‰ë§Œ)
                    if idx == 1:
                        soup = BeautifulSoup(driver.page_source, 'html.parser')
                        self.save_debug_html(soup, filename="debug_bok_search.html")
                
                    # ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ ëª¨ë“  í•­ëª© ì°¾ê¸° ë° ë“±ë¡ì¼ ë¹„êµ
                    try:
                        wait = WebDriverWait(driver, 10)
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                        list_items = []
                        list_selectors = [
                            ("CSS", "#frm > div.tsh-main > div.search-main > ul > li"),
                            ("CSS", "div.search-main ul li"),
                            ("CSS", ".search-main ul li"),
                            ("CSS", "ul.search-list li"),
                            ("CSS", ".bdLine.type4 ul li"),
                        ]
                        
                        for method, selector in list_selectors:
                            try:
                                if method == "CSS":
                                    list_items = driver.find_elements(By.CSS_SELECTOR, selector)
                                if list_items:
                                    print(f"  âœ“ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ ë°œê²¬ ({method}): {len(list_items)}ê°œ í•­ëª©")
                                    break
                            except Exception:
                                continue
                        
                        if not list_items:
                            print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                            detail_link = None
                            title = None
                        else:
                            # ê° í•­ëª©ì—ì„œ ë“±ë¡ì¼ ì¶”ì¶œ ë° ë¹„êµ
                            items_with_dates = []
                            
                            normalized_target = self._normalize_title(regulation_name)
                            
                            for item_idx, li_item in enumerate(list_items, 1):
                                try:
                                    # ë§í¬ ìš”ì†Œ ì°¾ê¸°
                                    link_elem = None
                                    try:
                                        link_elem = li_item.find_element(By.TAG_NAME, "a")
                                    except Exception:
                                        # a íƒœê·¸ê°€ li ë‚´ë¶€ì— ìˆì„ ìˆ˜ ìˆìŒ
                                        try:
                                            link_elem = li_item.find_element(By.CSS_SELECTOR, "a")
                                        except Exception:
                                            pass
                                    
                                    if not link_elem:
                                        continue
                                    
                                    # ì œëª© ì¶”ì¶œ (ìš°ì„  ìœ„ì¹˜ ì •ë³´ span.location ì‹œë„)
                                    item_title = ""
                                    title_selectors = [
                                        "span.location",
                                        "span.title",
                                        ".location",
                                    ]
                                    for t_sel in title_selectors:
                                        try:
                                            t_elem = link_elem.find_element(By.CSS_SELECTOR, t_sel)
                                            item_title = t_elem.text.strip()
                                            if item_title:
                                                break
                                        except Exception:
                                            continue
                                    if not item_title:
                                        item_title = link_elem.text.strip()
                                    
                                    # ê´„í˜¸ì™€ ê·¸ ë’¤ì˜ í…ìŠ¤íŠ¸ ì œê±° (ë¹„êµë¥¼ ìœ„í•´)
                                    item_title_cleaned = self._remove_parentheses(item_title)
                                    
                                    # ë“±ë¡ì¼ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                                    date_text = None
                                    date_selectors = [
                                        "span.schDesc span.date",
                                        "span.date",
                                        ".date",
                                        "span.schDesc > span.date",
                                    ]
                                    
                                    for date_selector in date_selectors:
                                        try:
                                            date_elem = li_item.find_element(By.CSS_SELECTOR, date_selector)
                                            date_text = date_elem.text.strip()
                                            if date_text:
                                                break
                                        except Exception:
                                            continue
                                    
                                    # ë§í¬ URL ì¶”ì¶œ
                                    item_link = link_elem.get_attribute("href")
                                    if item_link:
                                        if item_link.startswith("/"):
                                            item_link = self.BASE_URL + item_link
                                        elif not item_link.startswith("http"):
                                            item_link = urljoin(self.BASE_URL, item_link)
                                    
                                    if date_text:
                                        parsed_date = self._parse_date(date_text)
                                        if parsed_date:
                                            items_with_dates.append({
                                                'index': item_idx,
                                                'title': item_title,
                                                'title_cleaned': item_title_cleaned,
                                                'link': item_link,
                                                'date': parsed_date,
                                                'date_text': date_text,
                                                'element': link_elem
                                            })
                                            print(f"  â†’ í•­ëª© {item_idx}: {item_title[:50]}... (ë“±ë¡ì¼: {date_text})")
                                        else:
                                            print(f"  âš  í•­ëª© {item_idx}: ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_text})")
                                    else:
                                        # ë“±ë¡ì¼ì´ ì—†ëŠ” ê²½ìš°ë„ ë§í¬ë§Œ ì €ì¥
                                        if item_link:
                                            items_with_dates.append({
                                                'index': item_idx,
                                                'title': item_title,
                                                'title_cleaned': item_title_cleaned,
                                                'link': item_link,
                                                'date': None,
                                                'date_text': '',
                                                'element': link_elem
                                            })
                                            print(f"  â†’ í•­ëª© {item_idx}: {item_title[:50]}... (ë“±ë¡ì¼ ì—†ìŒ)")
                                
                                except Exception as e:
                                    print(f"  âš  í•­ëª© {item_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                    continue
                            
                            # ê°€ì¥ ìµœê·¼ ë‚ ì§œì´ë©´ì„œ ì œëª©ì´ ê²€ìƒ‰ì–´ì™€ ì¼ì¹˜í•˜ëŠ” í•­ëª© ìš°ì„  ì„ íƒ
                            if items_with_dates:
                                def title_matches(item_title_cleaned: str) -> bool:
                                    norm = self._normalize_title(item_title_cleaned)
                                    return normalized_target and (norm == normalized_target or normalized_target in norm or norm in normalized_target)

                                matching_items = [item for item in items_with_dates if title_matches(item.get('title_cleaned', item.get('title', '')))]

                                if not matching_items:
                                    print(f"  âš  ê²€ìƒ‰ì–´ì™€ ì¼ì¹˜í•˜ëŠ” ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤. ê·œì •ëª…ìœ¼ë¡œë§Œ ë¹ˆ ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                                    detail_link = None
                                    title = None
                                else:
                                    items_with_valid_dates = [item for item in matching_items if item['date'] is not None]
                                    
                                    if items_with_valid_dates:
                                        selected_item = max(items_with_valid_dates, key=lambda x: x['date'])
                                        print(f"  âœ“ ê°€ì¥ ìµœê·¼ ë“±ë¡ì¼(ì œëª© ì¼ì¹˜) í•­ëª© ì„ íƒ: {selected_item['title'][:50]}... (ë“±ë¡ì¼: {selected_item['date_text']})")
                                    else:
                                        selected_item = matching_items[0]
                                        print(f"  âš  ë“±ë¡ì¼ ì •ë³´ê°€ ì—†ì–´ ì²« ë²ˆì§¸ ì¼ì¹˜ í•­ëª© ì„ íƒ: {selected_item['title'][:50]}...")
                                    
                                    title = selected_item['title']
                                    detail_link = selected_item['link']
                                    
                                    # ì„ íƒëœ í•­ëª© í´ë¦­
                                    print(f"  â†’ ì„ íƒëœ ê²€ìƒ‰ ê²°ê³¼ í´ë¦­ ì¤‘...")
                                    selected_item['element'].click()
                                    
                                    # ìƒˆ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                                    time.sleep(2)
                                    
                                    # í˜„ì¬ URL ê°€ì ¸ì˜¤ê¸° (í´ë¦­ í›„ ì´ë™í•œ í˜ì´ì§€)
                                    current_url = driver.current_url
                                    print(f"  â†’ ì´ë™í•œ í˜ì´ì§€ URL: {current_url}")
                                    
                                    # í´ë¦­ í›„ ì´ë™í•œ URLì„ detail_linkë¡œ ì‚¬ìš©
                                    if current_url and current_url != search_url:
                                        detail_link = current_url
                            else:
                                print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                detail_link = None
                                title = None
                    
                    except TimeoutException:
                        print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
                        detail_link = None
                        title = None
                    except Exception as e:
                        print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        import traceback
                        traceback.print_exc()
                        detail_link = None
                        title = None
                    
                    if not detail_link:
                        print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        # ë¹ˆ í•­ëª©ìœ¼ë¡œ ì¶”ê°€
                        empty_item = {
                            "title": regulation_name,
                            "regulation_name": regulation_name,
                            "organization": "í•œêµ­ì€í–‰",
                            "target_name": regulation_name,
                            "target_category": target.get("category", ""),
                            "detail_link": "",
                            "content": "",
                            "department": "",
                            "file_names": [],
                            "download_links": [],
                            "enactment_date": "",
                            "revision_date": "",
                        }
                        results.append(empty_item)
                        continue
                    
                    # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    matched_regulation = {
                        "title": regulation_name,
                        "regulation_name": regulation_name,
                        "organization": "í•œêµ­ì€í–‰",
                        "target_name": regulation_name,
                        "target_category": target.get("category", ""),
                        "detail_link": detail_link,
                        "content": "",
                        "department": "",
                        "file_names": [],
                        "download_links": [],
                        "enactment_date": "",
                        "revision_date": "",
                    }
                    
                    if detail_link:
                        print(f"  ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼: {detail_link}")
                        detail_info = self.extract_regulation_detail(detail_link, regulation_name=regulation_name)
                        matched_regulation.update(detail_info)
                    else:
                        print(f"  âš  ìƒì„¸ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    results.append(matched_regulation)
                    
                finally:
                    # Selenium driver ì¢…ë£Œ
                    try:
                        driver.quit()
                    except Exception as e:
                        print(f"  âš  Driver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
                
                time.sleep(self.delay)
            
        except Exception as e:
            print(f"âœ— ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        return results
    
    def _filter_regulations_by_targets(self, regulation_list: List[Dict]) -> List[Dict]:
        """CSV ëª©ë¡ì— í¬í•¨ëœ ê·œì •ë§Œ ìˆœì„œëŒ€ë¡œ ë°˜í™˜í•œë‹¤."""
        if not self.target_laws:
            return regulation_list

        normalized_tree: Dict[str, List[Dict]] = {}
        for reg in regulation_list:
            reg_name = reg.get("regulation_name") or reg.get("title", "")
            key = self._normalize_title(reg_name)
            if not key:
                continue
            normalized_tree.setdefault(key, []).append(reg)

        selected_regulations: List[Dict] = []
        missing_targets: List[str] = []

        for target in self.target_laws:
            target_name = target["law_name"]
            key = self._normalize_title(target_name)
            matches = normalized_tree.get(key)
            if matches and len(matches) > 0:
                # ê°™ì€ ì´ë¦„ì˜ ê·œì •ì´ ì—¬ëŸ¬ ê°œ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ì‚¬ìš©
                reg = dict(matches[0])  # ë”•ì…”ë„ˆë¦¬ ë³µì‚¬
                reg["target_name"] = target_name
                reg["target_category"] = target.get("category", "")
                if target.get("law_name"):
                    reg["regulation_name"] = target["law_name"]
                selected_regulations.append(reg)
            else:
                missing_targets.append(target_name)

        if missing_targets:
            print(f"  âš  CSVì— ìˆìœ¼ë‚˜ ëª©ë¡ì—ì„œ ì°¾ì§€ ëª»í•œ ê·œì •: {len(missing_targets)}ê°œ")
            for name in missing_targets[:5]:
                print(f"     - {name}")
            if len(missing_targets) > 5:
                print("     ...")
            print(f"     (ì°¾ì§€ ëª»í•œ í•­ëª©ì€ ê²°ê³¼ì— ë¹ˆ ë‚´ìš©ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤)")

        return selected_regulations


def save_bok_results(records: List[Dict], crawler: Optional[BokScraper] = None):
    """JSON ë° CSVë¡œ í•œêµ­ì€í–‰ ë²•ê·œ ë°ì´í„°ë¥¼ ì €ì¥í•œë‹¤.
    
    Args:
        records: ìŠ¤í¬ë˜í•‘ëœ ë²•ê·œì •ë³´ ë¦¬ìŠ¤íŠ¸
        crawler: BokScraper ì¸ìŠ¤í„´ìŠ¤ (CSVì˜ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©)
    """
    # CSVì˜ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ë„ë¡ ì •ë ¬ (CSV ìˆœì„œ ìœ ì§€)
    if crawler and crawler.target_laws:
        # CSV í•­ëª© ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        records_dict = {}
        for item in records:
            reg_name = item.get("target_name") or item.get("regulation_name") or item.get("title", "")
            if reg_name:
                records_dict[reg_name] = item
        
        # CSV ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ê²°ê³¼ ìƒì„±
        ordered_records = []
        missing_count = 0
        for target in crawler.target_laws:
            target_name = target["law_name"]
            if target_name in records_dict:
                ordered_records.append(records_dict[target_name])
            else:
                # CSVì— ìˆì§€ë§Œ ê²°ê³¼ì— ì—†ëŠ” ê²½ìš° ë¹ˆ í•­ëª© ì¶”ê°€
                missing_count += 1
                empty_item: Dict[str, str] = {
                    "title": target_name,
                    "regulation_name": target_name,
                    "organization": "í•œêµ­ì€í–‰",
                    "target_name": target_name,
                    "target_category": target.get("category", ""),
                    "content": "",  # ë¹ˆ ë³¸ë¬¸
                    "department": "",
                    "file_names": [],
                    "download_links": [],
                    "enactment_date": "",
                    "revision_date": "",
                }
                ordered_records.append(empty_item)
                print(f"ë””ë²„ê¹…: ì°¾ì§€ ëª»í•œ í•­ëª© ì¶”ê°€ - {target_name}")
        
        if missing_count > 0:
            print(f"ë””ë²„ê¹…: ì´ {missing_count}ê°œ í•­ëª©ì„ ë¹ˆ ë³¸ë¬¸ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        
        records = ordered_records
    
    if not records:
        print("ì €ì¥í•  ë²•ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    json_dir = os.path.join("output", "json")
    csv_dir = os.path.join("output", "csv")
    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(csv_dir, exist_ok=True)
    
    # ë‚ ì§œ ì •ê·œí™”ë¥¼ ìœ„í•œ scraper ì¸ìŠ¤í„´ìŠ¤
    scraper = crawler if crawler else BokScraper()
    
    # ë²•ê·œ ì •ë³´ ë°ì´í„° ì •ë¦¬ (CSVì™€ ë™ì¼í•œ í•œê¸€ í•„ë“œëª…ìœ¼ë¡œ ì •ë¦¬)
    law_results = []
    for idx, item in enumerate(records, 1):
        # ì—¬ëŸ¬ ì²¨ë¶€íŒŒì¼ì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„
        file_names_str = "; ".join(item.get("file_names", [])) if item.get("file_names") else ""
        download_links_str = "; ".join(item.get("download_links", [])) if item.get("download_links") else ""
        
        # ë³¸ë¬¸ ë‚´ìš© ì²˜ë¦¬ (ê°œí–‰ ìœ ì§€, 4000ì ì œí•œ)
        content = item.get("content", "") or ""
        # \r\nì„ \nìœ¼ë¡œ í†µì¼í•˜ê³ , \rë§Œ ìˆëŠ” ê²½ìš°ë„ \nìœ¼ë¡œ ë³€í™˜
        content = content.replace("\r\n", "\n").replace("\r", "\n")
        if len(content) > 4000:
            content = content[:4000]
        
        law_item = {
            "ë²ˆí˜¸": str(idx),  # ìˆœë²ˆìœ¼ë¡œ ë²ˆí˜¸ ìƒì„±
            "ê·œì •ëª…": item.get("regulation_name", item.get("title", "")),
            "ê¸°ê´€ëª…": item.get("organization", "í•œêµ­ì€í–‰"),
            "ë³¸ë¬¸": content,
            "ì œì •ì¼": scraper.normalize_date_format(item.get("enactment_date", "")),
            "ìµœê·¼ ê°œì •ì¼": scraper.normalize_date_format(item.get("revision_date", "")),
            "ì†Œê´€ë¶€ì„œ": item.get("department", ""),
            "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬": download_links_str,
            "íŒŒì¼ ì´ë¦„": file_names_str,
        }
        law_results.append(law_item)
    
    # JSON ì €ì¥ (í•œê¸€ í•„ë“œëª…ìœ¼ë¡œ)
    json_path = os.path.join(json_dir, "bok_scraper.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "url": BokScraper.SEARCH_URL_TEMPLATE,
                "total_count": len(law_results),
                "results": law_results,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )
    print(f"\nâœ“ JSON ì €ì¥ ì™„ë£Œ: {json_path}")
    
    # CSV ì €ì¥ (ì •ë¦¬ëœ law_results ì‚¬ìš©)
    csv_headers = [
        "ë²ˆí˜¸",
        "ê·œì •ëª…",
        "ê¸°ê´€ëª…",
        "ë³¸ë¬¸",
        "ì œì •ì¼",
        "ìµœê·¼ ê°œì •ì¼",
        "ì†Œê´€ë¶€ì„œ",
        "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬",
        "íŒŒì¼ ì´ë¦„",
    ]
    csv_path = os.path.join(csv_dir, "bok_scraper.csv")
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=csv_headers)
        writer.writeheader()
        for law_item in law_results:
            writer.writerow(law_item)
    print(f"âœ“ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í•œêµ­ì€í–‰ ë²•ê·œì •ë³´ ìŠ¤í¬ë˜í¼ (CSV ëª©ë¡ ê¸°ë°˜)")
    parser.add_argument("--limit", type=int, default=0, help="ê°€ì ¸ì˜¬ ê°œìˆ˜ ì œí•œ (0=ì „ì²´)")
    parser.add_argument(
        "--csv",
        type=str,
        default=None,
        help="ëŒ€ìƒ ê·œì • ëª©ë¡ CSV ê²½ë¡œ (ê¸°ë³¸: BOK_Scraper/input/list.csv)",
    )
    args = parser.parse_args()
    
    scraper = BokScraper(csv_path=args.csv)
    results = scraper.crawl_regulations()
    
    print(f"\nì´ {len(results)}ê°œì˜ ë²•ê·œì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    save_bok_results(results, crawler=scraper)
