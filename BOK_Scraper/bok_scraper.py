"""
í•œêµ­ì€í–‰ ìŠ¤í¬ë˜í¼
íŠ¹ì • ë²•ê·œ í•­ëª©ë§Œ ìŠ¤í¬ë˜í•‘: ì „ìë°©ì‹ ì™¸ìƒë§¤ì¶œì±„ê¶Œë‹´ë³´ëŒ€ì¶œ ê´€ë ¨ ê·œì •
"""
from __future__ import annotations

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (common ëª¨ë“ˆ importë¥¼ ìœ„í•´)
def find_project_root():
    """common ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™"""
    try:
        # __file__ì´ ìˆëŠ” ê²½ìš° (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
        current = Path(__file__).resolve().parent
    except NameError:
        # __file__ì´ ì—†ëŠ” ê²½ìš° (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)
        current = Path.cwd()
    
    # common ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ë¡œ ì´ë™
    while current != current.parent:
        if (current / 'common').exists() and (current / 'common' / 'base_scraper.py').exists():
            return current
        current = current.parent
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ë°˜í™˜
    return Path.cwd()

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import os
import json
import csv
import time
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from common.base_scraper import BaseScraper
from common.file_extractor import FileExtractor
from common.file_comparator import FileComparator


class BokScraper(BaseScraper):
    """í•œêµ­ì€í–‰ ìŠ¤í¬ë˜í¼ - ì „ìë°©ì‹ ì™¸ìƒë§¤ì¶œì±„ê¶Œë‹´ë³´ëŒ€ì¶œ ê´€ë ¨ ê·œì •ë§Œ ìˆ˜ì§‘"""
    
    BASE_URL = "https://www.bok.or.kr"
    # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ URL ì‚¬ìš© (ê¸ˆìœµê¸°ê´€ ì „ìë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ë©´ ëŒ€ìƒ ê·œì • 2ê°œë§Œ ë‚˜ì˜´)
    LIST_URL = "https://www.bok.or.kr/portal/singl/law/listSearch.do?menuNo=200200&parentlawseq=&detaillawseq=&lawseq=&search_text=%EA%B8%88%EC%9C%B5%EA%B8%B0%EA%B4%80+%EC%A0%84%EC%9E%90%EB%B0%A9%EC%8B%9D"
    
    # ìŠ¤í¬ë˜í•‘í•  ëŒ€ìƒ ê·œì •ëª… (ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ëŠ” í•­ëª©)
    TARGET_REGULATIONS = [
        "ê¸ˆìœµê¸°ê´€ ì „ìë°©ì‹ ì™¸ìƒë§¤ì¶œì±„ê¶Œë‹´ë³´ëŒ€ì¶œ ì·¨ê¸‰ì ˆì°¨",
        "ê¸ˆìœµê¸°ê´€ ì „ìë°©ì‹ ì™¸ìƒë§¤ì¶œì±„ê¶Œë‹´ë³´ëŒ€ì¶œ ì·¨ê¸‰ì„¸ì¹™",
    ]
    
    def __init__(self, delay: float = 1.0):
        super().__init__(delay)
        self.download_dir = os.path.join("output", "downloads")
        self.previous_dir = os.path.join("output", "downloads", "previous", "bok")
        os.makedirs(self.download_dir, exist_ok=True)
        os.makedirs(self.previous_dir, exist_ok=True)
        # FileExtractor ì´ˆê¸°í™” (session ì „ë‹¬)
        self.file_extractor = FileExtractor(download_dir=self.download_dir, session=self.session)
        # íŒŒì¼ ë¹„êµê¸° ì´ˆê¸°í™”
        self.file_comparator = FileComparator(base_dir=self.download_dir)
    
    def is_target_regulation(self, title: str) -> bool:
        """ì œëª©ì´ ëŒ€ìƒ ê·œì •ì¸ì§€ í™•ì¸"""
        if not title:
            return False
        
        title_clean = title.strip()
        for target in self.TARGET_REGULATIONS:
            if target in title_clean or title_clean in target:
                return True
        return False
    
    def extract_regulation_list(self, soup: BeautifulSoup) -> List[Dict]:
        """ë²•ê·œ ëª©ë¡ì—ì„œ ëŒ€ìƒ ê·œì •ë§Œ ì¶”ì¶œ"""
        results = []
        
        # í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ë³´í†µ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í‘œì‹œë¨
        selectors = [
            "ul li",  # ë¦¬ìŠ¤íŠ¸ í•­ëª©
            "table tbody tr",  # í…Œì´ë¸” í–‰
            ".list-item",
            ".law-item",
            ".regulation-item",
            "div.list li",
            "li a",  # ë§í¬ê°€ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ í•­ëª©
        ]
        
        found_items = []
        for selector in selectors:
            items = soup.select(selector)
            if items and len(items) > 0:
                # ë¹ˆ í•­ëª©ì´ë‚˜ í—¤ë” ì œì™¸í•˜ê³  ì‹¤ì œ ë°ì´í„° í•­ëª©ë§Œ í•„í„°ë§
                valid_items = []
                for item in items:
                    # ë§í¬ê°€ ìˆê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í•­ëª©ë§Œ í¬í•¨
                    link = item.select_one("a")
                    text = item.get_text(strip=True)
                    if (link or text) and len(text) > 10:  # ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
                        valid_items.append(item)
                
                if valid_items:
                    found_items = valid_items
                    print(f"  âœ“ ì„ íƒì '{selector}'ë¡œ {len(valid_items)}ê°œ í•­ëª© ë°œê²¬")
                    break
        
        if not found_items:
            # ë””ë²„ê¹…ì„ ìœ„í•´ HTML ì¼ë¶€ ì €ì¥
            self.save_debug_html(soup, filename="debug_bok_list.html")
            print("  âš  ëª©ë¡ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë””ë²„ê·¸ HTML ì €ì¥: output/debug/debug_bok_list.html")
            print("  ğŸ’¡ ë””ë²„ê·¸ HTMLì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.")
            return results
        
        for item in found_items:
            try:
                # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
                title = None
                title_elem = (
                    item.select_one("a") or
                    item.select_one(".title") or
                    item.select_one("td:first-child") or
                    item.select_one(".name") or
                    item.select_one("strong") or
                    item  # ì „ì²´ í•­ëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                )
                
                if title_elem:
                    # ë§í¬ê°€ ìˆìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸
                    if title_elem.name == "a":
                        title = title_elem.get_text(strip=True)
                    else:
                        # ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‹œë„
                        link_in_elem = title_elem.select_one("a")
                        if link_in_elem:
                            title = link_in_elem.get_text(strip=True)
                        else:
                            title = title_elem.get_text(strip=True)
                
                # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                if not title or len(title) < 5:
                    continue
                
                # ëŒ€ìƒ ê·œì •ì¸ì§€ í™•ì¸
                if not self.is_target_regulation(title):
                    continue
                
                print(f"  âœ“ ëŒ€ìƒ ê·œì • ë°œê²¬: {title}")
                
                # ìƒì„¸ ë§í¬ ì¶”ì¶œ
                detail_link = ""
                link_elem = item.select_one("a[href]")
                if link_elem:
                    href = link_elem.get("href", "")
                    if href:
                        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if href.startswith("/"):
                            detail_link = self.BASE_URL + href
                        elif href.startswith("http"):
                            detail_link = href
                        else:
                            detail_link = urljoin(self.BASE_URL, href)
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ê°œì •ì¼, ë²ˆí˜¸ ë“±)
                regulation_info = {
                    "title": title,
                    "regulation_name": title,
                    "organization": "í•œêµ­ì€í–‰",
                    "detail_link": detail_link,
                    "content": "",
                    "department": "",
                    "file_names": [],
                    "download_links": [],
                    "enactment_date": "",
                    "revision_date": "",
                }
                
                # ê°œì •ì¼ ì¶”ì¶œ ì‹œë„ (ë‹¤ì–‘í•œ íŒ¨í„´)
                date_text = None
                date_elem = (
                    item.select_one(".date") or
                    item.select_one(".revision-date") or
                    item.select_one("td:nth-child(3)") or
                    item.select_one("td:nth-child(2)") or
                    item.select_one("span.date") or
                    item.select_one("em.date")
                )
                
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                else:
                    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD í˜•ì‹)
                    import re
                    full_text = item.get_text()
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', full_text)
                    if date_match:
                        date_text = date_match.group(1)
                
                if date_text:
                    regulation_info["revision_date"] = date_text
                    print(f"    ê°œì •ì¼: {date_text}")
                
                results.append(regulation_info)
                
            except Exception as e:
                print(f"  âš  í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return results
    
    def extract_regulation_detail(self, url: str, regulation_name: str = "") -> Dict:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ê·œì • ë‚´ìš© ì¶”ì¶œ"""
        detail_info = {
            "content": "",  # ë³¸ë¬¸ ë‚´ìš©ì€ ë¹„ì›Œë‘ 
            "file_names": [],
            "download_links": [],
            "revision_date": "",
        }
        
        try:
            soup = self.fetch_page(url, use_selenium=True)
            
            # íŒŒì¼ë§í¬ì™€ íŒŒì¼ëª… ì¶”ì¶œ: #main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(3) > a
            file_selector = "#main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(3) > a"
            file_elem = soup.select_one(file_selector)
            
            if file_elem:
                href = file_elem.get("href", "")
                if href:
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith("/"):
                        file_url = self.BASE_URL + href
                    elif href.startswith("http"):
                        file_url = href
                    else:
                        file_url = urljoin(self.BASE_URL, href)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ: hrefì˜ fileNm íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
                    file_name = None
                    from urllib.parse import urlparse, parse_qs, unquote
                    
                    try:
                        # URL íŒŒì‹±
                        parsed_url = urlparse(href)
                        query_params = parse_qs(parsed_url.query)
                        
                        # fileNm íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                        if 'fileNm' in query_params:
                            file_name = query_params['fileNm'][0]
                            # URL ë””ì½”ë”© (í•œê¸€ ë“±ì´ ì¸ì½”ë”©ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
                            file_name = unquote(file_name)
                        else:
                            # fileNmì´ ì—†ìœ¼ë©´ hrefì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                            if 'fileNm=' in href:
                                file_nm_part = href.split('fileNm=')[1]
                                # & ë˜ëŠ” &amp; ë˜ëŠ” ëê¹Œì§€
                                if '&' in file_nm_part:
                                    file_name = file_nm_part.split('&')[0]
                                elif '&amp;' in file_nm_part:
                                    file_name = file_nm_part.split('&amp;')[0]
                                else:
                                    file_name = file_nm_part
                                file_name = unquote(file_name)
                    except Exception as e:
                        print(f"  âš  íŒŒì¼ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    # íŒŒì¼ëª…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° fallback
                    if not file_name:
                        # span í…ìŠ¤íŠ¸ëŠ” "ì²¨ë¶€íŒŒì¼ ìˆìŠµë‹ˆë‹¤"ì´ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                        # hrefì—ì„œ íŒŒì¼ í™•ì¥ìë¡œ íŒŒì¼ëª… ì¶”ì •
                        if '.pdf' in href.lower():
                            file_name = "íŒŒì¼.pdf"
                        elif '.hwp' in href.lower():
                            file_name = "íŒŒì¼.hwp"
                        else:
                            file_name = "íŒŒì¼"
                    
                    detail_info["download_links"].append(file_url)
                    detail_info["file_names"].append(file_name)
                    print(f"  âœ“ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {file_name}")
                    print(f"    ë§í¬: {file_url}")
                    
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¹„êµ
                    self._download_and_compare_file(file_url, file_name, regulation_name=regulation_name)
            else:
                print(f"  âš  íŒŒì¼ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì…€ë ‰í„°: {file_selector})")
            
            # ìµœê·¼ê°œì •ì¼ ì¶”ì¶œ: #main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(1) > a
            date_selector = "#main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(1) > a"
            date_elem = soup.select_one(date_selector)
            
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    detail_info["revision_date"] = date_text
                    print(f"  âœ“ ìµœê·¼ê°œì •ì¼: {date_text}")
            else:
                print(f"  âš  ìµœê·¼ê°œì •ì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì…€ë ‰í„°: {date_selector})")
            
        except Exception as e:
            print(f"  âš  ìƒì„¸ í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return detail_info
    
    def _get_safe_filename(self, filename: str, regulation_name: str = "") -> str:
        """
        íŒŒì¼ëª…ì„ ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê²½ë¡œì— ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ìë§Œ)
        
        Args:
            filename: ì›ë³¸ íŒŒì¼ëª…
            regulation_name: ê·œì •ëª… (íŒŒì¼ëª… ìƒì„±ìš©)
            
        Returns:
            ì•ˆì „í•œ íŒŒì¼ëª…
        """
        import re
        # ê·œì •ëª…ì´ ìˆìœ¼ë©´ ê·œì •ëª… ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
        if regulation_name:
            # ê·œì •ëª…ì—ì„œ ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ
            safe_reg_name = re.sub(r'[^\w\s-]', '', regulation_name)
            safe_reg_name = safe_reg_name.replace(' ', '_')
            # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
            ext = Path(filename).suffix if filename else '.pdf'
            return f"{safe_reg_name}{ext}"
        else:
            # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ
            safe_name = re.sub(r'[^\w\s.-]', '', filename)
            return safe_name.replace(' ', '_')
    
    def _download_and_compare_file(self, file_url: str, file_name: str, regulation_name: str = "") -> Optional[Dict]:
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì´ì „ íŒŒì¼ê³¼ ë¹„êµ
        
        Args:
            file_url: ë‹¤ìš´ë¡œë“œ URL
            file_name: íŒŒì¼ëª…
            regulation_name: ê·œì •ëª… (ì´ì „ íŒŒì¼ ë§¤ì¹­ìš©)
            
        Returns:
            ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_filename = self._get_safe_filename(file_name, regulation_name)
            
            # ìƒˆ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
            new_file_path = os.path.join(self.download_dir, safe_filename)
            
            # ì´ì „ íŒŒì¼ ê²½ë¡œ (ê·œì •ëª… ê¸°ë°˜ìœ¼ë¡œ ì°¾ê¸°)
            previous_file_path = os.path.join(self.previous_dir, safe_filename)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            print(f"  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_name}")
            # FileExtractor.download_fileëŠ” (filepath, actual_filename) íŠœí”Œ ë°˜í™˜
            downloaded_result = self.file_extractor.download_file(
                file_url,
                safe_filename,
                use_selenium=False,  # requestsë¡œ ë‹¤ìš´ë¡œë“œ
                driver=None
            )
            
            # íŠœí”Œ ì–¸íŒ¨í‚¹
            if downloaded_result:
                downloaded_path, actual_filename = downloaded_result
            else:
                downloaded_path, actual_filename = None, None
            
            if not downloaded_path or not os.path.exists(downloaded_path):
                print(f"  âš  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return None
            
            # ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ìƒˆ íŒŒì¼ ê²½ë¡œë¡œ ì´ë™/ë³µì‚¬
            if downloaded_path != new_file_path:
                import shutil
                if os.path.exists(new_file_path):
                    os.remove(new_file_path)  # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
                shutil.move(downloaded_path, new_file_path)
                print(f"  âœ“ íŒŒì¼ ì €ì¥: {new_file_path}")
            
            # ì´ì „ íŒŒì¼ê³¼ ë¹„êµ
            if os.path.exists(previous_file_path):
                print(f"  ì´ì „ íŒŒì¼ê³¼ ë¹„êµ ì¤‘...")
                comparison_result = self.file_comparator.compare_and_report(
                    new_file_path,
                    previous_file_path,
                    save_diff=True
                )
                
                if comparison_result['changed']:
                    print(f"  âœ“ íŒŒì¼ ë³€ê²½ ê°ì§€: {comparison_result['diff_summary']}")
                    if 'diff_file' in comparison_result:
                        print(f"    Diff íŒŒì¼: {comparison_result['diff_file']}")
                else:
                    print(f"  âœ“ íŒŒì¼ ë™ì¼ (ë³€ê²½ ì—†ìŒ)")
                
                # ì´ì „ íŒŒì¼ì„ ìƒˆ íŒŒì¼ë¡œ êµì²´ (ë‹¤ìŒ ë¹„êµë¥¼ ìœ„í•´)
                import shutil
                shutil.copy2(new_file_path, previous_file_path)
                print(f"  âœ“ ì´ì „ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                print(f"  âœ“ ìƒˆ íŒŒì¼ (ì´ì „ íŒŒì¼ ì—†ìŒ)")
                # ì´ì „ íŒŒì¼ ë””ë ‰í† ë¦¬ì— ë³µì‚¬ (ë‹¤ìŒ ë¹„êµë¥¼ ìœ„í•´)
                import shutil
                os.makedirs(self.previous_dir, exist_ok=True)
                shutil.copy2(new_file_path, previous_file_path)
                print(f"  âœ“ ì´ì „ íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ")
            
            return {
                'file_path': new_file_path,
                'previous_file_path': previous_file_path if os.path.exists(previous_file_path) else None,
                'comparison': comparison_result if os.path.exists(previous_file_path) else None,
            }
            
        except Exception as e:
            print(f"  âš  íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_regulations(self) -> List[Dict]:
        """
        ë²•ê·œì •ë³´ - ê·œì • ìŠ¤í¬ë˜í•‘
        ëŒ€ìƒ ê·œì •ë§Œ í•„í„°ë§í•˜ì—¬ ìˆ˜ì§‘
        """
        print(f"\n=== í•œêµ­ì€í–‰ ë²•ê·œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
        print(f"ëŒ€ìƒ ê·œì •: {len(self.TARGET_REGULATIONS)}ê°œ")
        for i, reg in enumerate(self.TARGET_REGULATIONS, 1):
            print(f"  {i}. {reg}")
        print()
        
        results = []
        
        try:
            # ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼
            print(f"[1ë‹¨ê³„] ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼: {self.LIST_URL}")
            soup = self.fetch_page(self.LIST_URL, use_selenium=True)
            
            # ë””ë²„ê¹… HTML ì €ì¥
            self.save_debug_html(soup, filename="debug_bok_list.html")
            
            # ëŒ€ìƒ ê·œì • ëª©ë¡ ì¶”ì¶œ
            print(f"[2ë‹¨ê³„] ëŒ€ìƒ ê·œì • ëª©ë¡ ì¶”ì¶œ ì¤‘...")
            regulation_list = self.extract_regulation_list(soup)
            
            if not regulation_list:
                print("  âš  ëŒ€ìƒ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return results
            
            print(f"  âœ“ {len(regulation_list)}ê°œ ê·œì • ë°œê²¬")
            
            # ê° ê·œì •ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            print(f"[3ë‹¨ê³„] ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘...")
            for idx, regulation in enumerate(regulation_list, 1):
                title = regulation.get("title", "")
                detail_link = regulation.get("detail_link", "")
                
                print(f"\n[{idx}/{len(regulation_list)}] {title}")
                
                if detail_link:
                    print(f"  ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼: {detail_link}")
                    regulation_name = regulation.get("regulation_name", regulation.get("title", ""))
                    detail_info = self.extract_regulation_detail(detail_link, regulation_name=regulation_name)
                    regulation.update(detail_info)
                    
                    # ì²¨ë¶€íŒŒì¼ ì •ë³´ëŠ” ì´ë¯¸ detail_infoì— í¬í•¨ë˜ì–´ ìˆìŒ
                    # ë³¸ë¬¸ ë‚´ìš©ì€ ë¹„ì›Œë‘ë¯€ë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì¶”ì¶œì€ ìƒëµ
                else:
                    print(f"  âš  ìƒì„¸ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                results.append(regulation)
                time.sleep(self.delay)
            
        except Exception as e:
            print(f"âœ— ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        return results


def save_bok_results(records: List[Dict]):
    """JSON ë° CSVë¡œ í•œêµ­ì€í–‰ ë²•ê·œ ë°ì´í„°ë¥¼ ì €ì¥í•œë‹¤."""
    if not records:
        print("ì €ì¥í•  ë²•ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    json_dir = os.path.join("output", "json")
    csv_dir = os.path.join("output", "csv")
    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(csv_dir, exist_ok=True)
    
    # ë²•ê·œ ì •ë³´ ë°ì´í„° ì •ë¦¬ (CSVì™€ ë™ì¼í•œ í•œê¸€ í•„ë“œëª…ìœ¼ë¡œ ì •ë¦¬)
    law_results = []
    for idx, item in enumerate(records, 1):
        # ì—¬ëŸ¬ ì²¨ë¶€íŒŒì¼ì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„
        file_names_str = "; ".join(item.get("file_names", [])) if item.get("file_names") else ""
        download_links_str = "; ".join(item.get("download_links", [])) if item.get("download_links") else ""
        
        law_item = {
            "ë²ˆí˜¸": str(idx),  # ìˆœë²ˆìœ¼ë¡œ ë²ˆí˜¸ ìƒì„±
            "ê·œì •ëª…": item.get("regulation_name", item.get("title", "")),
            "ê¸°ê´€ëª…": item.get("organization", "í•œêµ­ì€í–‰"),
            "ë³¸ë¬¸": (item.get("content", "") or "").replace("\n", " ").replace("\r", " "),
            "ì œì •ì¼": item.get("enactment_date", ""),
            "ìµœê·¼ ê°œì •ì¼": item.get("revision_date", ""),
            "ì†Œê´€ë¶€ì„œ": item.get("department", ""),
            "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬": download_links_str,
            "íŒŒì¼ ì´ë¦„": file_names_str,
        }
        law_results.append(law_item)
    
    # JSON ì €ì¥ (í•œê¸€ í•„ë“œëª…ìœ¼ë¡œ)
    json_path = os.path.join(json_dir, "bok_regulations.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "url": BokScraper.LIST_URL,
                "total_count": len(law_results),
                "target_regulations": BokScraper.TARGET_REGULATIONS,
                "results": law_results,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )
    print(f"\nâœ“ JSON ì €ì¥ ì™„ë£Œ: {json_path}")
    
    # CSV ì €ì¥ (ì •ë¦¬ëœ law_results ì‚¬ìš©)
    csv_headers = [
        "ë²ˆí˜¸",
        "ê·œì •ëª…",
        "ê¸°ê´€ëª…",
        "ë³¸ë¬¸",
        "ì œì •ì¼",
        "ìµœê·¼ ê°œì •ì¼",
        "ì†Œê´€ë¶€ì„œ",
        "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬",
        "íŒŒì¼ ì´ë¦„",
    ]
    csv_path = os.path.join(csv_dir, "bok_regulations.csv")
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=csv_headers)
        writer.writeheader()
        for law_item in law_results:
            writer.writerow(law_item)
    print(f"âœ“ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í•œêµ­ì€í–‰ ë²•ê·œì •ë³´ ìŠ¤í¬ë˜í¼ (ì „ìë°©ì‹ ì™¸ìƒë§¤ì¶œì±„ê¶Œë‹´ë³´ëŒ€ì¶œ ê´€ë ¨ ê·œì •)")
    parser.add_argument("--limit", type=int, default=0, help="ê°€ì ¸ì˜¬ ê°œìˆ˜ ì œí•œ (0=ì „ì²´, ê¸°ë³¸ê°’: ëŒ€ìƒ ê·œì •ë§Œ)")
    args = parser.parse_args()
    
    scraper = BokScraper()
    results = scraper.crawl_regulations()
    
    print(f"\nì´ {len(results)}ê°œì˜ ë²•ê·œì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    save_bok_results(results)

