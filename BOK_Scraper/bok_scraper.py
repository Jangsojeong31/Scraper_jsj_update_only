"""
í•œêµ­ì€í–‰ ìŠ¤í¬ë˜í¼
CSV ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ ë²•ê·œ ì •ë³´ ìŠ¤í¬ë˜í•‘
"""
from __future__ import annotations

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (common ëª¨ë“ˆ importë¥¼ ìœ„í•´)
def find_project_root():
    """common ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™"""
    try:
        # __file__ì´ ìˆëŠ” ê²½ìš° (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
        current = Path(__file__).resolve().parent
    except NameError:
        # __file__ì´ ì—†ëŠ” ê²½ìš° (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)
        current = Path.cwd()
    
    # common ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ë¡œ ì´ë™
    while current != current.parent:
        if (current / 'common').exists() and (current / 'common' / 'base_scraper.py').exists():
            return current
        current = current.parent
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ë°˜í™˜
    return Path.cwd()

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import os
import json
import csv
import time
import re
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from common.base_scraper import BaseScraper
from common.file_extractor import FileExtractor
from common.file_comparator import FileComparator


class BokScraper(BaseScraper):
    """í•œêµ­ì€í–‰ ìŠ¤í¬ë˜í¼ - CSV ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ ë²•ê·œ ì •ë³´ ìˆ˜ì§‘"""
    
    BASE_URL = "https://www.bok.or.kr"
    # ê²€ìƒ‰ URL í…œí”Œë¦¿ (ê²€ìƒ‰ì–´ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ)
    SEARCH_URL_TEMPLATE = "https://www.bok.or.kr/portal/singl/law/listSearch.do?menuNo=200200&parentlawseq=&detaillawseq=&lawseq=&search_text={search_text}"
    DEFAULT_CSV_PATH = "BOK_Scraper/input/list.csv"
    
    def __init__(self, delay: float = 1.0, csv_path: Optional[str] = None):
        super().__init__(delay)
        self.download_dir = os.path.join("output", "downloads")
        self.previous_dir = os.path.join("output", "downloads", "previous", "bok")
        os.makedirs(self.download_dir, exist_ok=True)
        os.makedirs(self.previous_dir, exist_ok=True)
        # FileExtractor ì´ˆê¸°í™” (session ì „ë‹¬)
        self.file_extractor = FileExtractor(download_dir=self.download_dir, session=self.session)
        # íŒŒì¼ ë¹„êµê¸° ì´ˆê¸°í™”
        self.file_comparator = FileComparator(base_dir=self.download_dir)
        # CSVì—ì„œ ëŒ€ìƒ ê·œì • ëª©ë¡ ë¡œë“œ
        self.csv_path = csv_path or self.DEFAULT_CSV_PATH
        self.target_laws = self._load_target_laws(self.csv_path)
        if self.target_laws:
            print(f"âœ“ CSVì—ì„œ {len(self.target_laws)}ê°œì˜ ëŒ€ìƒ ê·œì •ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {self.csv_path}")
        else:
            print("âš  ëŒ€ìƒ CSVë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ëª©ë¡ì„ ëŒ€ìƒìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    def _load_target_laws(self, csv_path: str) -> List[Dict]:
        """CSV íŒŒì¼ì—ì„œ ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ê·œì •ëª…ì„ ë¡œë“œí•œë‹¤."""
        if not csv_path:
            return []
        csv_file = Path(csv_path)
        if not csv_file.is_absolute():
            csv_file = find_project_root() / csv_path
        if not csv_file.exists():
            print(f"âš  BOK ëŒ€ìƒ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
            return []

        targets: List[Dict] = []
        try:
            with open(csv_file, "r", encoding="utf-8-sig", newline="") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    name = (row.get("ë²•ë ¹ëª…") or "").strip()
                    category = (row.get("êµ¬ë¶„") or "").strip()
                    if not name:
                        continue
                    targets.append({"law_name": name, "category": category})
        except Exception as exc:
            print(f"âš  BOK ëŒ€ìƒ CSV ë¡œë“œ ì‹¤íŒ¨: {exc}")
            return []
        return targets
    
    def _normalize_title(self, text: Optional[str]) -> str:
        """ë¹„êµë¥¼ ìœ„í•œ ê·œì •ëª… ì •ê·œí™”"""
        if not text:
            return ""
        cleaned = re.sub(r"[\s\W]+", "", text)
        return cleaned.lower()
    
    def is_target_regulation(self, title: str) -> bool:
        """ì œëª©ì´ ëŒ€ìƒ ê·œì •ì¸ì§€ í™•ì¸ (CSV ëª©ë¡ ê¸°ë°˜)"""
        if not title or not self.target_laws:
            return True  # CSVê°€ ì—†ìœ¼ë©´ ëª¨ë“  í•­ëª© í—ˆìš©
        
        title_normalized = self._normalize_title(title)
        for target in self.target_laws:
            target_normalized = self._normalize_title(target["law_name"])
            # ì •ê·œí™”ëœ ì´ë¦„ì´ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì¸ì§€ í™•ì¸
            if target_normalized == title_normalized or target_normalized in title_normalized or title_normalized in target_normalized:
                return True
        return False
    
    def extract_regulation_list(self, soup: BeautifulSoup) -> List[Dict]:
        """ë²•ê·œ ëª©ë¡ì—ì„œ ëŒ€ìƒ ê·œì •ë§Œ ì¶”ì¶œ"""
        results = []
        
        # í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ë³´í†µ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í‘œì‹œë¨
        selectors = [
            "ul li",  # ë¦¬ìŠ¤íŠ¸ í•­ëª©
            "table tbody tr",  # í…Œì´ë¸” í–‰
            ".list-item",
            ".law-item",
            ".regulation-item",
            "div.list li",
            "li a",  # ë§í¬ê°€ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ í•­ëª©
        ]
        
        found_items = []
        for selector in selectors:
            items = soup.select(selector)
            if items and len(items) > 0:
                # ë¹ˆ í•­ëª©ì´ë‚˜ í—¤ë” ì œì™¸í•˜ê³  ì‹¤ì œ ë°ì´í„° í•­ëª©ë§Œ í•„í„°ë§
                valid_items = []
                for item in items:
                    # ë§í¬ê°€ ìˆê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í•­ëª©ë§Œ í¬í•¨
                    link = item.select_one("a")
                    text = item.get_text(strip=True)
                    if (link or text) and len(text) > 10:  # ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
                        valid_items.append(item)
                
                if valid_items:
                    found_items = valid_items
                    print(f"  âœ“ ì„ íƒì '{selector}'ë¡œ {len(valid_items)}ê°œ í•­ëª© ë°œê²¬")
                    break
        
        if not found_items:
            # ë””ë²„ê¹…ì„ ìœ„í•´ HTML ì¼ë¶€ ì €ì¥
            self.save_debug_html(soup, filename="debug_bok_list.html")
            print("  âš  ëª©ë¡ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë””ë²„ê·¸ HTML ì €ì¥: output/debug/debug_bok_list.html")
            print("  ğŸ’¡ ë””ë²„ê·¸ HTMLì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.")
            return results
        
        for item in found_items:
            try:
                # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
                title = None
                title_elem = (
                    item.select_one("a") or
                    item.select_one(".title") or
                    item.select_one("td:first-child") or
                    item.select_one(".name") or
                    item.select_one("strong") or
                    item  # ì „ì²´ í•­ëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                )
                
                if title_elem:
                    # ë§í¬ê°€ ìˆìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸
                    if title_elem.name == "a":
                        title = title_elem.get_text(strip=True)
                    else:
                        # ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‹œë„
                        link_in_elem = title_elem.select_one("a")
                        if link_in_elem:
                            title = link_in_elem.get_text(strip=True)
                        else:
                            title = title_elem.get_text(strip=True)
                
                # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                if not title or len(title) < 5:
                    continue
                
                # ëª¨ë“  ê·œì •ì„ ì¶”ì¶œ (í•„í„°ë§ì€ ë‚˜ì¤‘ì— _filter_regulations_by_targetsì—ì„œ ìˆ˜í–‰)
                # print(f"  âœ“ ê·œì • ë°œê²¬: {title}")
                
                # ìƒì„¸ ë§í¬ ì¶”ì¶œ
                detail_link = ""
                link_elem = item.select_one("a[href]")
                if link_elem:
                    href = link_elem.get("href", "")
                    if href:
                        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if href.startswith("/"):
                            detail_link = self.BASE_URL + href
                        elif href.startswith("http"):
                            detail_link = href
                        else:
                            detail_link = urljoin(self.BASE_URL, href)
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ê°œì •ì¼, ë²ˆí˜¸ ë“±)
                regulation_info = {
                    "title": title,
                    "regulation_name": title,
                    "organization": "í•œêµ­ì€í–‰",
                    "detail_link": detail_link,
                    "content": "",
                    "department": "",
                    "file_names": [],
                    "download_links": [],
                    "enactment_date": "",
                    "revision_date": "",
                }
                
                # ê°œì •ì¼ ì¶”ì¶œ ì‹œë„ (ë‹¤ì–‘í•œ íŒ¨í„´)
                date_text = None
                date_elem = (
                    item.select_one(".date") or
                    item.select_one(".revision-date") or
                    item.select_one("td:nth-child(3)") or
                    item.select_one("td:nth-child(2)") or
                    item.select_one("span.date") or
                    item.select_one("em.date")
                )
                
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                else:
                    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD í˜•ì‹)
                    import re
                    full_text = item.get_text()
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', full_text)
                    if date_match:
                        date_text = date_match.group(1)
                
                if date_text:
                    regulation_info["revision_date"] = date_text
                    print(f"    ê°œì •ì¼: {date_text}")
                
                results.append(regulation_info)
                
            except Exception as e:
                print(f"  âš  í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return results
    
    def extract_regulation_detail(self, url: str, regulation_name: str = "") -> Dict:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ê·œì • ë‚´ìš© ì¶”ì¶œ"""
        detail_info = {
            "content": "",  # PDFì—ì„œ ì¶”ì¶œí•œ ë³¸ë¬¸ ë‚´ìš©
            "file_names": [],
            "download_links": [],
            "revision_date": "",
            "enactment_date": "",  # ì œì •ì¼
            "department": "",  # ì†Œê´€ë¶€ì„œ
        }
        
        try:
            soup = self.fetch_page(url, use_selenium=True)
            
            # íŒŒì¼ë§í¬ì™€ íŒŒì¼ëª… ì¶”ì¶œ: #main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(3) > a
            file_selector = "#main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(3) > a"
            file_elem = soup.select_one(file_selector)
            
            if file_elem:
                href = file_elem.get("href", "")
                if href:
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith("/"):
                        file_url = self.BASE_URL + href
                    elif href.startswith("http"):
                        file_url = href
                    else:
                        file_url = urljoin(self.BASE_URL, href)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ: hrefì˜ fileNm íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
                    file_name = None
                    from urllib.parse import urlparse, parse_qs, unquote
                    
                    try:
                        # URL íŒŒì‹±
                        parsed_url = urlparse(href)
                        query_params = parse_qs(parsed_url.query)
                        
                        # fileNm íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                        if 'fileNm' in query_params:
                            file_name = query_params['fileNm'][0]
                            # URL ë””ì½”ë”© (í•œê¸€ ë“±ì´ ì¸ì½”ë”©ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
                            file_name = unquote(file_name)
                        else:
                            # fileNmì´ ì—†ìœ¼ë©´ hrefì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                            if 'fileNm=' in href:
                                file_nm_part = href.split('fileNm=')[1]
                                # & ë˜ëŠ” &amp; ë˜ëŠ” ëê¹Œì§€
                                if '&' in file_nm_part:
                                    file_name = file_nm_part.split('&')[0]
                                elif '&amp;' in file_nm_part:
                                    file_name = file_nm_part.split('&amp;')[0]
                                else:
                                    file_name = file_nm_part
                                file_name = unquote(file_name)
                    except Exception as e:
                        print(f"  âš  íŒŒì¼ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    # íŒŒì¼ëª…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° fallback
                    if not file_name:
                        # span í…ìŠ¤íŠ¸ëŠ” "ì²¨ë¶€íŒŒì¼ ìˆìŠµë‹ˆë‹¤"ì´ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                        # hrefì—ì„œ íŒŒì¼ í™•ì¥ìë¡œ íŒŒì¼ëª… ì¶”ì •
                        if '.pdf' in href.lower():
                            file_name = "íŒŒì¼.pdf"
                        elif '.hwp' in href.lower():
                            file_name = "íŒŒì¼.hwp"
                        else:
                            file_name = "íŒŒì¼"
                    
                    detail_info["download_links"].append(file_url)
                    detail_info["file_names"].append(file_name)
                    print(f"  âœ“ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {file_name}")
                    print(f"    ë§í¬: {file_url}")
                    
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¹„êµ
                    downloaded_file_path = self._download_and_compare_file(file_url, file_name, regulation_name=regulation_name)
                    
                    # PDF íŒŒì¼ì´ë©´ ë‚´ìš© ì¶”ì¶œ
                    if downloaded_file_path and downloaded_file_path.get('file_path'):
                        file_path = downloaded_file_path['file_path']
                        if file_path.lower().endswith('.pdf'):
                            print(f"  PDF ë‚´ìš© ì¶”ì¶œ ì¤‘...")
                            pdf_content = self.file_extractor.extract_pdf_content(file_path)
                            if pdf_content:
                                detail_info["content"] = pdf_content
                                print(f"  âœ“ PDFì—ì„œ {len(pdf_content)}ì ì¶”ì¶œ ì™„ë£Œ")
                                
                                # PDFì—ì„œ ì†Œê´€ë¶€ì„œì™€ ì œì •ì¼ ì¶”ì¶œ
                                extracted_info = self._extract_info_from_pdf_content(pdf_content)
                                if extracted_info.get("department"):
                                    detail_info["department"] = extracted_info["department"]
                                    print(f"  âœ“ ì†Œê´€ë¶€ì„œ: {extracted_info['department']}")
                                if extracted_info.get("enactment_date"):
                                    detail_info["enactment_date"] = extracted_info["enactment_date"]
                                    print(f"  âœ“ ì œì •ì¼: {extracted_info['enactment_date']}")
                            else:
                                print(f"  âš  PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
            else:
                print(f"  âš  íŒŒì¼ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì…€ë ‰í„°: {file_selector})")
            
            # ìµœê·¼ê°œì •ì¼ ì¶”ì¶œ: #main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(1) > a
            date_selector = "#main-container > div.content > div.bdView > div > div > table > tbody > tr:nth-child(1) > td:nth-child(1) > a"
            date_elem = soup.select_one(date_selector)
            
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    detail_info["revision_date"] = date_text
                    print(f"  âœ“ ìµœê·¼ê°œì •ì¼: {date_text}")
            else:
                print(f"  âš  ìµœê·¼ê°œì •ì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì…€ë ‰í„°: {date_selector})")
            
        except Exception as e:
            print(f"  âš  ìƒì„¸ í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return detail_info
    
    def _extract_info_from_pdf_content(self, content: str) -> Dict[str, str]:
        """PDF ë‚´ìš©ì—ì„œ ì†Œê´€ë¶€ì„œì™€ ì œì •ì¼ ì¶”ì¶œ"""
        result = {
            "department": "",
            "enactment_date": "",
        }
        
        if not content:
            return result
        
        # ì œì •ì¼ íŒ¨í„´ ì°¾ê¸° (YYYYë…„ MMì›” DDì¼ ë˜ëŠ” YYYY-MM-DD í˜•ì‹)
        # ì˜ˆ: "2023ë…„ 1ì›” 12ì¼", "2023-01-12", "ì œì •ì¼: 2023.01.12"
        date_patterns = [
            r'ì œì •ì¼[:\s]*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'ì œì •ì¼[:\s]*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
            r'ì œì •ì¼[:\s]*(\d{4})-(\d{1,2})-(\d{1,2})',
            r'ì œì •[:\s]*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'ì œì •[:\s]*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
            r'ì œì •[:\s]*(\d{4})-(\d{1,2})-(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                year, month, day = match.groups()
                # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                result["enactment_date"] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                break
        
        # ì†Œê´€ë¶€ì„œ íŒ¨í„´ ì°¾ê¸° (ë§ˆì§€ë§‰ ê°œì •ì¼ ë¶€ë¶„ì—ì„œ ì¶”ì¶œ)
        # ì˜ˆ: "ê°œì • 2025. 6. 24. êµ­ì¥ê²°ì¬ êµ­ì œì´ê´„íŒ€- 793"
        # ì˜ˆ: "ê°œì •2023.12.20.ì´ì¬ê²°ì¬ ì™¸í™˜ì •ë³´íŒ€-1028"
        # íŒ¨í„´: ê°œì • + ë‚ ì§œ + ê²°ì¬ + íŒ€ëª… + "-" + ìˆ«ì
        department_patterns = [
            r'ê°œì •\s*\d{4}\.?\s*\d{1,2}\.?\s*\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)\s*-',  # ê³µë°± í¬í•¨
            r'ê°œì •\s*\d{4}\.?\s*\d{1,2}\.?\s*\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)-',  # ê³µë°± ì—†ìŒ
            r'ê°œì •\d{4}\.?\d{1,2}\.?\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)\s*-',  # ê³µë°± ì—†ìŒ (ë‚ ì§œ ë¶€ë¶„)
            r'ê°œì •\d{4}\.?\d{1,2}\.?\d{1,2}\.?\s*[ê°€-í£]*ê²°ì¬\s+([ê°€-í£]+íŒ€)-',  # ê³µë°± ì—†ìŒ (ì „ì²´)
        ]
        
        # ëª¨ë“  ê°œì •ì¼ íŒ¨í„´ì„ ì°¾ì•„ì„œ ë§ˆì§€ë§‰ ê²ƒì„ ì‚¬ìš©
        all_matches = []
        for pattern in department_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                team_name = match.group(1).strip()
                if team_name:
                    all_matches.append((match.start(), team_name))
        
        # ë§ˆì§€ë§‰ ê°œì •ì¼ì—ì„œ ì¶”ì¶œí•œ íŒ€ëª… ì‚¬ìš©
        if all_matches:
            # ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë§ˆì§€ë§‰ ê²ƒ ì„ íƒ
            all_matches.sort(key=lambda x: x[0])
            result["department"] = all_matches[-1][1]
        
        return result
    
    def _get_safe_filename(self, filename: str, regulation_name: str = "") -> str:
        """
        íŒŒì¼ëª…ì„ ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê²½ë¡œì— ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ìë§Œ)
        
        Args:
            filename: ì›ë³¸ íŒŒì¼ëª…
            regulation_name: ê·œì •ëª… (íŒŒì¼ëª… ìƒì„±ìš©)
            
        Returns:
            ì•ˆì „í•œ íŒŒì¼ëª…
        """
        import re
        # ê·œì •ëª…ì´ ìˆìœ¼ë©´ ê·œì •ëª… ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
        if regulation_name:
            # ê·œì •ëª…ì—ì„œ ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ
            safe_reg_name = re.sub(r'[^\w\s-]', '', regulation_name)
            safe_reg_name = safe_reg_name.replace(' ', '_')
            # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
            ext = Path(filename).suffix if filename else '.pdf'
            return f"{safe_reg_name}{ext}"
        else:
            # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ
            safe_name = re.sub(r'[^\w\s.-]', '', filename)
            return safe_name.replace(' ', '_')
    
    def _download_and_compare_file(self, file_url: str, file_name: str, regulation_name: str = "") -> Optional[Dict]:
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì´ì „ íŒŒì¼ê³¼ ë¹„êµ
        
        Args:
            file_url: ë‹¤ìš´ë¡œë“œ URL
            file_name: íŒŒì¼ëª…
            regulation_name: ê·œì •ëª… (ì´ì „ íŒŒì¼ ë§¤ì¹­ìš©)
            
        Returns:
            ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_filename = self._get_safe_filename(file_name, regulation_name)
            
            # ìƒˆ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
            new_file_path = os.path.join(self.download_dir, safe_filename)
            
            # ì´ì „ íŒŒì¼ ê²½ë¡œ (ê·œì •ëª… ê¸°ë°˜ìœ¼ë¡œ ì°¾ê¸°)
            previous_file_path = os.path.join(self.previous_dir, safe_filename)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            print(f"  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_name}")
            # FileExtractor.download_fileëŠ” (filepath, actual_filename) íŠœí”Œ ë°˜í™˜
            downloaded_result = self.file_extractor.download_file(
                file_url,
                safe_filename,
                use_selenium=False,  # requestsë¡œ ë‹¤ìš´ë¡œë“œ
                driver=None
            )
            
            # íŠœí”Œ ì–¸íŒ¨í‚¹
            if downloaded_result:
                downloaded_path, actual_filename = downloaded_result
            else:
                downloaded_path, actual_filename = None, None
            
            if not downloaded_path or not os.path.exists(downloaded_path):
                print(f"  âš  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return None
            
            # ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ìƒˆ íŒŒì¼ ê²½ë¡œë¡œ ì´ë™/ë³µì‚¬
            if downloaded_path != new_file_path:
                import shutil
                if os.path.exists(new_file_path):
                    os.remove(new_file_path)  # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
                shutil.move(downloaded_path, new_file_path)
                print(f"  âœ“ íŒŒì¼ ì €ì¥: {new_file_path}")
            
            # ì´ì „ íŒŒì¼ê³¼ ë¹„êµ
            comparison_result = None
            if os.path.exists(previous_file_path):
                print(f"  ì´ì „ íŒŒì¼ê³¼ ë¹„êµ ì¤‘...")
                comparison_result = self.file_comparator.compare_and_report(
                    new_file_path,
                    previous_file_path,
                    save_diff=True
                )
                
                if comparison_result['changed']:
                    print(f"  âœ“ íŒŒì¼ ë³€ê²½ ê°ì§€: {comparison_result['diff_summary']}")
                    if 'diff_file' in comparison_result:
                        print(f"    Diff íŒŒì¼: {comparison_result['diff_file']}")
                else:
                    print(f"  âœ“ íŒŒì¼ ë™ì¼ (ë³€ê²½ ì—†ìŒ)")
                
                # ì´ì „ íŒŒì¼ì„ ìƒˆ íŒŒì¼ë¡œ êµì²´ (ë‹¤ìŒ ë¹„êµë¥¼ ìœ„í•´)
                import shutil
                shutil.copy2(new_file_path, previous_file_path)
                print(f"  âœ“ ì´ì „ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                print(f"  âœ“ ìƒˆ íŒŒì¼ (ì´ì „ íŒŒì¼ ì—†ìŒ)")
                # ì´ì „ íŒŒì¼ ë””ë ‰í† ë¦¬ì— ë³µì‚¬ (ë‹¤ìŒ ë¹„êµë¥¼ ìœ„í•´)
                import shutil
                os.makedirs(self.previous_dir, exist_ok=True)
                shutil.copy2(new_file_path, previous_file_path)
                print(f"  âœ“ ì´ì „ íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ")
            
            return {
                'file_path': new_file_path,
                'previous_file_path': previous_file_path if os.path.exists(previous_file_path) else None,
                'comparison': comparison_result,
            }
            
        except Exception as e:
            print(f"  âš  íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_regulations(self) -> List[Dict]:
        """
        ë²•ê·œì •ë³´ - ê·œì • ìŠ¤í¬ë˜í•‘
        CSV ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ ê° ê·œì •ëª…ì„ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘
        """
        print(f"\n=== í•œêµ­ì€í–‰ ë²•ê·œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
        if not self.target_laws:
            print("âš  CSV ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ëŒ€ìƒ ê·œì •: {len(self.target_laws)}ê°œ")
        for i, target in enumerate(self.target_laws, 1):
            print(f"  {i}. {target['law_name']}")
        print()
        
        results = []
        
        try:
            # ê° ê·œì •ëª…ì„ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë° ì¶”ì¶œ
            for idx, target in enumerate(self.target_laws, 1):
                regulation_name = target["law_name"]
                print(f"\n[{idx}/{len(self.target_laws)}] {regulation_name}")
                
                # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”©
                search_text_encoded = quote(regulation_name)
                search_url = self.SEARCH_URL_TEMPLATE.format(search_text=search_text_encoded)
                
                print(f"  ê²€ìƒ‰ URL: {search_url}")
                
                # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì ‘ê·¼
                soup = self.fetch_page(search_url, use_selenium=True)
                
                # ë””ë²„ê¹… HTML ì €ì¥ (ì²« ë²ˆì§¸ ê²€ìƒ‰ë§Œ)
                if idx == 1:
                    self.save_debug_html(soup, filename="debug_bok_search.html")
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê·œì • ëª©ë¡ ì¶”ì¶œ
                regulation_list = self.extract_regulation_list(soup)
                
                if not regulation_list:
                    print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    # ë¹ˆ í•­ëª©ìœ¼ë¡œ ì¶”ê°€ (ë‚˜ì¤‘ì— save_bok_resultsì—ì„œ ì²˜ë¦¬)
                    empty_item = {
                        "title": regulation_name,
                        "regulation_name": regulation_name,
                        "organization": "í•œêµ­ì€í–‰",
                        "target_name": regulation_name,
                        "target_category": target.get("category", ""),
                        "detail_link": "",
                        "content": "",
                        "department": "",
                        "file_names": [],
                        "download_links": [],
                        "enactment_date": "",
                        "revision_date": "",
                    }
                    results.append(empty_item)
                    continue
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê·œì • ì°¾ê¸°
                matched_regulation = None
                for reg in regulation_list:
                    reg_name = reg.get("regulation_name") or reg.get("title", "")
                    if self._normalize_title(reg_name) == self._normalize_title(regulation_name):
                        matched_regulation = reg
                        break
                
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
                if not matched_regulation and regulation_list:
                    matched_regulation = regulation_list[0]
                    print(f"  âš  ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê·œì •ì„ ì°¾ì§€ ëª»í•´ ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©: {matched_regulation.get('title', '')}")
                
                if matched_regulation:
                    matched_regulation["target_name"] = regulation_name
                    matched_regulation["target_category"] = target.get("category", "")
                    if target.get("law_name"):
                        matched_regulation["regulation_name"] = target["law_name"]
                    
                    # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    detail_link = matched_regulation.get("detail_link", "")
                    if detail_link:
                        print(f"  ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼: {detail_link}")
                        detail_info = self.extract_regulation_detail(detail_link, regulation_name=regulation_name)
                        matched_regulation.update(detail_info)
                    else:
                        print(f"  âš  ìƒì„¸ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    results.append(matched_regulation)
                else:
                    print(f"  âš  ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    # ë¹ˆ í•­ëª©ìœ¼ë¡œ ì¶”ê°€
                    empty_item = {
                        "title": regulation_name,
                        "regulation_name": regulation_name,
                        "organization": "í•œêµ­ì€í–‰",
                        "target_name": regulation_name,
                        "target_category": target.get("category", ""),
                        "detail_link": "",
                        "content": "",
                        "department": "",
                        "file_names": [],
                        "download_links": [],
                        "enactment_date": "",
                        "revision_date": "",
                    }
                    results.append(empty_item)
                
                time.sleep(self.delay)
            
        except Exception as e:
            print(f"âœ— ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        return results
    
    def _filter_regulations_by_targets(self, regulation_list: List[Dict]) -> List[Dict]:
        """CSV ëª©ë¡ì— í¬í•¨ëœ ê·œì •ë§Œ ìˆœì„œëŒ€ë¡œ ë°˜í™˜í•œë‹¤."""
        if not self.target_laws:
            return regulation_list

        normalized_tree: Dict[str, List[Dict]] = {}
        for reg in regulation_list:
            reg_name = reg.get("regulation_name") or reg.get("title", "")
            key = self._normalize_title(reg_name)
            if not key:
                continue
            normalized_tree.setdefault(key, []).append(reg)

        selected_regulations: List[Dict] = []
        missing_targets: List[str] = []

        for target in self.target_laws:
            target_name = target["law_name"]
            key = self._normalize_title(target_name)
            matches = normalized_tree.get(key)
            if matches and len(matches) > 0:
                # ê°™ì€ ì´ë¦„ì˜ ê·œì •ì´ ì—¬ëŸ¬ ê°œ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ì‚¬ìš©
                reg = dict(matches[0])  # ë”•ì…”ë„ˆë¦¬ ë³µì‚¬
                reg["target_name"] = target_name
                reg["target_category"] = target.get("category", "")
                if target.get("law_name"):
                    reg["regulation_name"] = target["law_name"]
                selected_regulations.append(reg)
            else:
                missing_targets.append(target_name)

        if missing_targets:
            print(f"  âš  CSVì— ìˆìœ¼ë‚˜ ëª©ë¡ì—ì„œ ì°¾ì§€ ëª»í•œ ê·œì •: {len(missing_targets)}ê°œ")
            for name in missing_targets[:5]:
                print(f"     - {name}")
            if len(missing_targets) > 5:
                print("     ...")
            print(f"     (ì°¾ì§€ ëª»í•œ í•­ëª©ì€ ê²°ê³¼ì— ë¹ˆ ë‚´ìš©ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤)")

        return selected_regulations


def save_bok_results(records: List[Dict], crawler: Optional[BokScraper] = None):
    """JSON ë° CSVë¡œ í•œêµ­ì€í–‰ ë²•ê·œ ë°ì´í„°ë¥¼ ì €ì¥í•œë‹¤.
    
    Args:
        records: ìŠ¤í¬ë˜í•‘ëœ ë²•ê·œì •ë³´ ë¦¬ìŠ¤íŠ¸
        crawler: BokScraper ì¸ìŠ¤í„´ìŠ¤ (CSVì˜ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©)
    """
    # CSVì˜ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ë„ë¡ ì •ë ¬ (CSV ìˆœì„œ ìœ ì§€)
    if crawler and crawler.target_laws:
        # CSV í•­ëª© ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        records_dict = {}
        for item in records:
            reg_name = item.get("target_name") or item.get("regulation_name") or item.get("title", "")
            if reg_name:
                records_dict[reg_name] = item
        
        # CSV ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ê²°ê³¼ ìƒì„±
        ordered_records = []
        missing_count = 0
        for target in crawler.target_laws:
            target_name = target["law_name"]
            if target_name in records_dict:
                ordered_records.append(records_dict[target_name])
            else:
                # CSVì— ìˆì§€ë§Œ ê²°ê³¼ì— ì—†ëŠ” ê²½ìš° ë¹ˆ í•­ëª© ì¶”ê°€
                missing_count += 1
                empty_item: Dict[str, str] = {
                    "title": target_name,
                    "regulation_name": target_name,
                    "organization": "í•œêµ­ì€í–‰",
                    "target_name": target_name,
                    "target_category": target.get("category", ""),
                    "content": "",  # ë¹ˆ ë³¸ë¬¸
                    "department": "",
                    "file_names": [],
                    "download_links": [],
                    "enactment_date": "",
                    "revision_date": "",
                }
                ordered_records.append(empty_item)
                print(f"ë””ë²„ê¹…: ì°¾ì§€ ëª»í•œ í•­ëª© ì¶”ê°€ - {target_name}")
        
        if missing_count > 0:
            print(f"ë””ë²„ê¹…: ì´ {missing_count}ê°œ í•­ëª©ì„ ë¹ˆ ë³¸ë¬¸ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        
        records = ordered_records
    
    if not records:
        print("ì €ì¥í•  ë²•ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    json_dir = os.path.join("output", "json")
    csv_dir = os.path.join("output", "csv")
    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(csv_dir, exist_ok=True)
    
    # ë²•ê·œ ì •ë³´ ë°ì´í„° ì •ë¦¬ (CSVì™€ ë™ì¼í•œ í•œê¸€ í•„ë“œëª…ìœ¼ë¡œ ì •ë¦¬)
    law_results = []
    for idx, item in enumerate(records, 1):
        # ì—¬ëŸ¬ ì²¨ë¶€íŒŒì¼ì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„
        file_names_str = "; ".join(item.get("file_names", [])) if item.get("file_names") else ""
        download_links_str = "; ".join(item.get("download_links", [])) if item.get("download_links") else ""
        
        law_item = {
            "ë²ˆí˜¸": str(idx),  # ìˆœë²ˆìœ¼ë¡œ ë²ˆí˜¸ ìƒì„±
            "ê·œì •ëª…": item.get("regulation_name", item.get("title", "")),
            "ê¸°ê´€ëª…": item.get("organization", "í•œêµ­ì€í–‰"),
            "ë³¸ë¬¸": (item.get("content", "") or "").replace("\n", " ").replace("\r", " "),
            "ì œì •ì¼": item.get("enactment_date", ""),
            "ìµœê·¼ ê°œì •ì¼": item.get("revision_date", ""),
            "ì†Œê´€ë¶€ì„œ": item.get("department", ""),
            "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬": download_links_str,
            "íŒŒì¼ ì´ë¦„": file_names_str,
        }
        law_results.append(law_item)
    
    # JSON ì €ì¥ (í•œê¸€ í•„ë“œëª…ìœ¼ë¡œ)
    json_path = os.path.join(json_dir, "bok_scraper.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "url": BokScraper.SEARCH_URL_TEMPLATE,
                "total_count": len(law_results),
                "results": law_results,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )
    print(f"\nâœ“ JSON ì €ì¥ ì™„ë£Œ: {json_path}")
    
    # CSV ì €ì¥ (ì •ë¦¬ëœ law_results ì‚¬ìš©)
    csv_headers = [
        "ë²ˆí˜¸",
        "ê·œì •ëª…",
        "ê¸°ê´€ëª…",
        "ë³¸ë¬¸",
        "ì œì •ì¼",
        "ìµœê·¼ ê°œì •ì¼",
        "ì†Œê´€ë¶€ì„œ",
        "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬",
        "íŒŒì¼ ì´ë¦„",
    ]
    csv_path = os.path.join(csv_dir, "bok_scraper.csv")
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=csv_headers)
        writer.writeheader()
        for law_item in law_results:
            writer.writerow(law_item)
    print(f"âœ“ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í•œêµ­ì€í–‰ ë²•ê·œì •ë³´ ìŠ¤í¬ë˜í¼ (CSV ëª©ë¡ ê¸°ë°˜)")
    parser.add_argument("--limit", type=int, default=0, help="ê°€ì ¸ì˜¬ ê°œìˆ˜ ì œí•œ (0=ì „ì²´)")
    parser.add_argument(
        "--csv",
        type=str,
        default=None,
        help="ëŒ€ìƒ ê·œì • ëª©ë¡ CSV ê²½ë¡œ (ê¸°ë³¸: BOK_Scraper/input/list.csv)",
    )
    args = parser.parse_args()
    
    scraper = BokScraper(csv_path=args.csv)
    results = scraper.crawl_regulations()
    
    print(f"\nì´ {len(results)}ê°œì˜ ë²•ê·œì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    save_bok_results(results, crawler=scraper)

